<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Testing Llama3.1 (8B) With LM Studio | ExitCode0</title>
<meta name=keywords content="LLM,AI,GPT,llama"><meta name=description content="Taking a look at the the latest open source llama 3.1 model after its July 2024 release. Testing its performance and accuracy with GPT4ALL."><meta name=author content="Tom"><link rel=canonical href=https://exitcode0.net/posts/testing-llama3_1-with-gpt4all/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://exitcode0.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://exitcode0.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://exitcode0.net/favicon-32x32.png><link rel=apple-touch-icon href=https://exitcode0.net/apple-touch-icon.png><link rel=mask-icon href=https://exitcode0.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://exitcode0.net/posts/testing-llama3_1-with-gpt4all/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745507566538945" crossorigin=anonymous></script><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=exit data-description="Support me on Buy me a coffee!" data-message="Buy me a coffee" data-color=#FF813F data-position=Right data-x_margin=18 data-y_margin=165></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-EDE9MPZR9T"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-EDE9MPZR9T")}</script><meta property="og:title" content="Testing Llama3.1 (8B) With LM Studio"><meta property="og:description" content="Taking a look at the the latest open source llama 3.1 model after its July 2024 release. Testing its performance and accuracy with GPT4ALL."><meta property="og:type" content="article"><meta property="og:url" content="https://exitcode0.net/posts/testing-llama3_1-with-gpt4all/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-28T07:00:00+01:00"><meta property="article:modified_time" content="2024-07-28T07:00:00+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Testing Llama3.1 (8B) With LM Studio"><meta name=twitter:description content="Taking a look at the the latest open source llama 3.1 model after its July 2024 release. Testing its performance and accuracy with GPT4ALL."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://exitcode0.net/posts/"},{"@type":"ListItem","position":2,"name":"Testing Llama3.1 (8B) With LM Studio","item":"https://exitcode0.net/posts/testing-llama3_1-with-gpt4all/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Testing Llama3.1 (8B) With LM Studio","name":"Testing Llama3.1 (8B) With LM Studio","description":"Taking a look at the the latest open source llama 3.1 model after its July 2024 release. Testing its performance and accuracy with GPT4ALL.","keywords":["LLM","AI","GPT","llama"],"articleBody":"MetaAI have done it again… Dropping llama3.1 under the (mostly) open source license and I have been putting it through its paces and benchmarking to find out how well it performs to models preiously tested: Testing Llama3 With LM Studio. Previously we took a look at llama3 in LM Studio which has made self hosted LLMs that rival paid services like ChatGPT and Claude possible; this time we are taking a look at llama through the lend of GPT4All.\nPer the Meta llama github model card:\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\nModel Architecture: Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nThe power consumption of the Nvidia H100 GPUs used to build this model is slightly staggering and certainly opens up a number of questions around the value per kilowatt a model like llama provideS to commerce and GDP:\nModel Training Time (GPU hours) Training Power Consumption (W) Training Location-Based Greenhouse Gas Emissions (tons CO2eq) Llama 3.1 8B 1.46M 700 420 Llama 3.1 70B 7.0M 700 2,040 Llama 3.1 405B 30.84M 700 8,930 Total 39.3M 11,390 Installing the model Just like LMStudio, GPT4All makes running large language models extremely accessible and maintains complete privacy of chat data. One bonus feature of GPT4All is the ability to “chat with your documents” generating a RAG index for files or folders. This populates the prompt with context based on your indexed files, allowing for a GPT conversation that has awareness of your documents. This type of integration could provide significant value for Professionals or Businesses who have privacy concerns regarding data upload to cloud based AI services like ChatGPT.\nGPT4All has some excellent Documentation.\nOnce you have downloaded the latest installer for you platform (Windows, Mac, Linux), you can go ahead and grab a GGUF model with ease:\nDownloading the Meta llama3 (or 3.1) model in GPT4All\nThen, you can go ahead and start a chat and select the model you wish to use - it is possible to have multiple models downloaded and select between then… useful if you have models suited to specific tasks:\nLaunching a chat window in GPT4All\nPerformance Let me start by providing some system specs as context. I have by 2024 standards, a modest desktop computer:\nRyzen 5600 6c/12t CPU - https://amzn.to/3zKpFrg (an absolute steal in 2024!) 32GB 3200MHz DDR4 RAM - https://amzn.to/3TLWkno Nvidia RTX 3060 12GB GDDR6 GPU - https://amzn.to/3zurgl7 It would be great to test these models on bleeding edge hardware, enterprise hardware and even Apple M3 silicon, but hardware isn’t cheap these days and I, like many, don’t have the resources to acquire it. Thankfully it seems that llama3 performance at this hardware level is very good and there’s minimal, perceivable slowdown as the context token count increases. I was experiencing speeds of 23 tokens per second in LM Studio and my chat focusing on writing a python script was remarkable.\nUsing the 8B model, I saw a great throughput of 38tok/s, which feels equal to or faster in most cases than popular online AI chatbots.\nResults Ok so, what did I ask llama3.1? This version of the model is fine tuned for instructional tasks, so for now, I asked for help improving a basic python script. This is a great way to test how current and accurate a model is. Here is my full transcript…\nGood morning, I have the following python code that I want to improve:\nimport requests url = \"https://demo.rading212.com/api/v0/equity/pies\" headers = {\"Authorization\": \"SOMERANDOMAPIKEY\"} response = requests.get(url, headers=headers) print(response.status_code) if response.status_code == 200: data = response.json() if data: print(data) else: print(response.status_code) I want to implement functions and threading so that I can run this code in a docker container. I want to pass important variables like the time between requests and the Authorization key. I also want to introduce a function which sends the data returned by the successful request to a postgres database. Here is what typical json response looks like:\n[{'id': 696969, 'cash': 0.48, 'dividendDetails': {'gained': 7.94, 'reinvested': 7.41, 'inCash': 0.46}, 'result': {'investedValue': 302.4, 'value': 318.35, 'result': 15.95, 'resultCoef': 0.0527}, 'progress': 0.3986, 'status': None}, {'id': 696968, 'cash': 0.12, 'dividendDetails': {'gained': 0.72, 'reinvested': 0.63, 'inCash': 0.09}, 'result': {'investedValue': 300.14, 'value': 404.68, 'result': 104.54, 'resultCoef': 0.3483}, 'progress': 0.0792, 'status': None}, {'id': 696867, 'cash': 0.55, 'dividendDetails': {'gained': 0.55, 'reinvested': 0.0, 'inCash': 0.55}, 'result': {'investedValue': 249.27, 'value': 278.99, 'result': 29.72, 'resultCoef': 0.1192}, 'progress': None, 'status': None}, {'id': 696868, 'cash': 0.0, 'dividendDetails': {'gained': 0.0, 'reinvested': 0.0, 'inCash': 0.0}, 'result': {'investedValue': 0, 'value': 0.0, 'result': 0.0, 'resultCoef': 0}, 'progress': None, 'status': None}, {'id': 2181613, 'cash': 0.0, 'dividendDetails': {'gained': 0.0, 'reinvested': 0.0, 'inCash': 0.0}, 'result': {'investedValue': 0, 'value': 0, 'result': 0, 'resultCoef': 0}, 'progress': None, 'status': None}]\nI can help you improve your code by implementing functions and threading as per your requirements. Here’s an improved version of the code:\nimport requests from concurrent.futures import ThreadPoolExecutor import time import psycopg2 class EquityAPI: def __init__(self, url, headers): self.url = url self.headers = headers def get_equity(self): response = requests.get(self.url, headers=self.headers) return response.json() def send_to_postgres(self, data): conn = psycopg2.connect( host='your_host', database='your_database', user='your_user', password='your_password' ) cur = conn.cursor() for item in data: query = \"INSERT INTO your_table (id, cash, dividend_details) VALUES (%s, %s, %s)\" values = (item['id'], item['cash'], str(item['dividendDetails'])) cur.execute(query, values) conn.commit() conn.close() def main(): url = 'https://demo.rading212.com/api/v0/equity/pies' headers = {\"Authorization\": \"SOMERANDOMAPIKEY\"} api = EquityAPI(url, headers) while True: data = api.get_equity() if len(data) \u003e 0 and data[0]: print(\"Received data:\", data) # Send to Postgres api.send_to_postgres(data) time.sleep(10) # Wait for 10 seconds before next request if __name__ == \"__main__\": main() In this improved code:\nWe’ve created a class EquityAPI that encapsulates the API functionality.\nThe get_equity() method sends an HTTP GET request to the specified URL and returns the JSON response.\nThe send_to_postgres() method connects to your Postgres database, inserts data into the table using SQL queries, and then commits and closes the connection.\nIn the main() function, we create an instance of the EquityAPI class and use a while loop with a 10-second sleep between requests.\nNote that you’ll need to replace ‘your_host’, ‘your_database’, ‘your_user’, and ‘your_password’ in the Postgres connection string with your actual database credentials. Also, make sure to update your_table name according to your schema.\nAs this will be running in a container, TIME_BETWEEN_REQUESTS (time.sleep(10)) and AUTHORIZATION_KEY will be environment variables, can you adjust the code to load them from env?\nI’ve updated the code to use environment variables for TIME_BETWEEN_REQUESTS and AUTHORIZATION_KEY. Here’s the revised version:\nimport requests from concurrent.futures import ThreadPoolExecutor import time import psycopg2 import os class EquityAPI: def **init**(self, url): self.url = url self.headers = {\"Authorization\": os.environ.get(\"AUTHORIZATION_KEY\")} def get_equity(self): response = requests.get(self.url, headers=self.headers) return response.json() def send_to_postgres(self, data): conn = psycopg2.connect( host=os.environ.get('POSTGRES_HOST'), database=os.environ.get('POSTGRES_DB'), user=os.environ.get('POSTGRES_USER'), password=os.environ.get('POSTGRES_PASSWORD') ) cur = conn.cursor() for item in data: query = \"INSERT INTO your_table (id, cash, dividend_details) VALUES (%s, %s, %s)\" values = (item['id'], item['cash'], str(item['dividendDetails'])) cur.execute(query, values) conn.commit() conn.close() def main(): url = 'https://demo.rading212.com/api/v0/equity/pies' api = EquityAPI(url) while True: data = api.get_equity() if len(data) \u003e 0 and data[0]: print(\"Received data:\", data) # Send to Postgres api.send_to_postgres(data) time.sleep(int(os.environ.get('TIME_BETWEEN_REQUESTS'))) # Wait for TIME_BETWEEN_REQUESTS seconds before next request if **name** == \"**main**\": main() In this revised code:\nWe’ve removed the headers parameter from the EquityAPI constructor and instead use os.environ.get(“AUTHORIZATION_KEY”) to load the authorization key from an environment variable.\nSimilarly, we’re using time.sleep(int(os.environ.get(‘TIME_BETWEEN_REQUESTS’))) to wait for the specified time between requests.\nMake sure to set these environment variables in your container’s configuration. You can do this by adding a .env file with the following content:\nAUTHORIZATION_KEY=SOMERANDOMAPIKEY POSTGRES_HOST=your_host POSTGRES_DB=your_database POSTGRES_USER=your_user POSTGRES_PASSWORD=your_password TIME_BETWEEN_REQUESTS=10 Replace your_host, your_database, your_user, and your_password with your actual Postgres database credentials.\nFeel free to ask if there’s anything else I can help you improve!\nI will be using postgres as a source for grafana, please can you include the current time in the postgres INSERT command?\nHere is the revised code that includes the current timestamp in the Postgres insert statement:\nimport requests from concurrent.futures import ThreadPoolExecutor import time import psycopg2 import os from datetime import datetime class EquityAPI: def __init__(self, url): self.url = url self.headers = {\"Authorization\": os.environ.get(\"AUTHORIZATION_KEY\")} def get_equity(self): response = requests.get(self.url, headers=self.headers) return response.json() def send_to_postgres(self, data): conn = psycopg2.connect( host=os.environ.get('POSTGRES_HOST'), database=os.environ.get('POSTGRES_DB'), user=os.environ.get('POSTGRES_USER'), password=os.environ.get('POSTGRES_PASSWORD') ) cur = conn.cursor() for item in data: query = \"INSERT INTO your_table (id, cash, dividend_details, timestamp) VALUES (%s, %s, %s, %s)\" values = (item['id'], item['cash'], str(item['dividendDetails']), datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) cur.execute(query, values) conn.commit() conn.close() def main(): url = 'https://demo.rading212.com/api/v0/equity/pies' api = EquityAPI(url) while True: data = api.get_equity() if len(data) \u003e 0 and data[0]: print(\"Received data:\", data) # Send to Postgres api.send_to_postgres(data) time.sleep(int(os.environ.get('TIME_BETWEEN_REQUESTS'))) # Wait for TIME_BETWEEN_REQUESTS seconds before next request if __name__ == \"__main__\": main() In this revised code:\nWe’ve added a timestamp column to the Postgres insert statement.\nThe timestamp is generated using datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), which returns the current date and time in the format “YYYY-MM-DD HH:MM:SS”.\nMake sure your Postgres table has a timestamp column with the correct data type.\nOne last task, can you help me write a docker compose file which will create a container to run our python code, a container for postgres (where our python script will insert to) including an init script to make the table, and a grafana instance? Can you include any environment variables which we will need to pass to our python script such as database_url and table_name\nHere is a Docker Compose file that creates three containers: one for your Python code, another for Postgres with an initial script to create the table, and finally a Grafana instance:\nversion: \"3\" services: api: build: . environment: - AUTHORIZATION_KEY=SOMERANDOMAPIKEY - POSTGRES_HOST=postgres - POSTGRES_DB=mydatabase - POSTGRES_USER=myuser - POSTGRES_PASSWORD=mypassword - TIME_BETWEEN_REQUESTS=10 depends_on: - postgres postgres: image: postgres environment: - POSTGRES_USER=myuser - POSTGRES_PASSWORD=mypassword - POSTGRES_DB=mydatabase volumes: - ./init.sql:/docker-entrypoint-initdb.d/init.sql ports: - \"5432:5432\" grafana: image: grafana/grafana environment: - GF_SECURITY_ADMIN_PASSWORD=grafanapassword depends_on: - postgres volumes: init.sql: This Docker Compose file:\nCreates a container named api that runs your Python code.\nSets the necessary environment variables for your script, including database credentials and time between requests.\nDepends on the Postgres service to ensure it’s running before starting the API.\nDefines another container called postgres, which uses an official PostgreSQL image. It sets up a new user, password, and database name as specified in the environment variables section of this file.\nMounts a volume for your initial SQL script (init.sql) at /docker-entrypoint-initdb.d/init.sql to run it during Postgres initialization.\nHere’s what you should put inside ./init.sql:\nCREATE TABLE mytable ( id SERIAL PRIMARY KEY, cash DECIMAL(10,2), dividend_details JSONB NOT NULL DEFAULT '{}'::JSONB, timestamp TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP ); This SQL script creates a table named mytable with the columns you specified in your Python code.\nRemember to replace placeholders like SOMERANDOMAPIKEY, mydatabase, myuser, and mypassword with actual values for your application. Also, update any other environment variables as needed according to their usage within your script.\nIMPRESSIVE!\nConclusion The responses are even faster than llama3 and the responses seem moderately more accurate. In comparison to previous tests, there are some observed improvements when we pivoted from python to docker compose YAML. The indicator has certainly turned slight more towards concern for programmers jobs…\nTLDR Llama3.1 another strong increment if you are willing to disregard the insane computer required to train it. THe move toward open source is welcome will hopefully force more innovation and reduce ‘moat digging’ for paid services\n","wordCount":"1958","inLanguage":"en","datePublished":"2024-07-28T07:00:00+01:00","dateModified":"2024-07-28T07:00:00+01:00","author":{"@type":"Person","name":"Tom"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://exitcode0.net/posts/testing-llama3_1-with-gpt4all/"},"publisher":{"@type":"Organization","name":"ExitCode0","logo":{"@type":"ImageObject","url":"https://exitcode0.net/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://exitcode0.net/ accesskey=h title="ExitCode0 (Alt + H)">ExitCode0</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://datasolace.com/ title=DataSolace><span>DataSolace</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://exitcode0.net/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://exitcode0.net/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://exitcode0.net/privacy/ title=Privacy><span>Privacy</span></a></li><li><a href=https://exitcode0.net/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://exitcode0.net/>Home</a>&nbsp;»&nbsp;<a href=https://exitcode0.net/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Testing Llama3.1 (8B) With LM Studio</h1><div class=post-description>Taking a look at the the latest open source llama 3.1 model after its July 2024 release. Testing its performance and accuracy with GPT4ALL.</div><div class=post-meta><span title='2024-07-28 07:00:00 +0100 +0100'>July 28, 2024</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;Tom</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#installing-the-model aria-label="Installing the model">Installing the model</a></li><li><a href=#performance aria-label=Performance>Performance</a></li><li><a href=#results aria-label=Results>Results</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a><ul><li><a href=#tldr aria-label=TLDR>TLDR</a></li></ul></li></ul></div></details></div><div class=post-content><p>MetaAI have done it again&mldr; <a href=https://llama.meta.com/>Dropping llama3.1</a> under the (mostly) open source license and I have been putting it through its paces and benchmarking to find out how well it performs to models preiously tested: <a href=http://localhost:1313/posts/testing-llama3-with-lmstudio/>Testing Llama3 With LM Studio</a>. Previously we took a look at llama3 in <a href=https://lmstudio.ai/>LM Studio</a> which has made self hosted LLMs that rival paid services like <a href=https://chat.openai.com/>ChatGPT</a> and <a href=https://claude.ai/>Claude</a> possible; this time we are taking a look at llama through the lend of <a href=https://www.nomic.ai/gpt4all>GPT4All</a>.</p><p>Per the Meta llama github model card:</p><p>The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.</p><p><strong>Model Architecture:</strong> Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.</p><p>The power consumption of the Nvidia H100 GPUs used to build this model is slightly staggering and certainly opens up a number of questions around the value per kilowatt a model like llama provideS to commerce and GDP:</p><table><thead><tr><th style=text-align:left></th><th style=text-align:left></th><th style=text-align:left></th><th style=text-align:left></th></tr></thead><tbody><tr><td style=text-align:left><strong>Model</strong></td><td style=text-align:left><strong>Training Time (GPU hours)</strong></td><td style=text-align:left><strong>Training Power Consumption (W)</strong></td><td style=text-align:left><strong>Training Location-Based Greenhouse Gas Emissions (tons CO2eq)</strong></td></tr><tr><td style=text-align:left>Llama 3.1 8B</td><td style=text-align:left>1.46M</td><td style=text-align:left>700</td><td style=text-align:left>420</td></tr><tr><td style=text-align:left>Llama 3.1 70B</td><td style=text-align:left>7.0M</td><td style=text-align:left>700</td><td style=text-align:left>2,040</td></tr><tr><td style=text-align:left>Llama 3.1 405B</td><td style=text-align:left>30.84M</td><td style=text-align:left>700</td><td style=text-align:left>8,930</td></tr><tr><td style=text-align:left>Total</td><td style=text-align:left>39.3M</td><td style=text-align:left></td><td style=text-align:left>11,390</td></tr></tbody></table><h2 id=installing-the-model>Installing the model<a hidden class=anchor aria-hidden=true href=#installing-the-model>#</a></h2><p>Just like LMStudio, <a href=https://www.nomic.ai/gpt4all>GPT4All</a> makes running large language models extremely accessible and maintains complete privacy of chat data. One bonus feature of GPT4All is the ability to &ldquo;chat with your documents&rdquo; generating a RAG index for files or folders. This populates the prompt with context based on your indexed files, allowing for a GPT conversation that has awareness of your documents. This type of integration could provide significant value for Professionals or Businesses who have privacy concerns regarding data upload to cloud based AI services like ChatGPT.</p><p>GPT4All has some excellent <a href=https://docs.gpt4all.io/gpt4all_desktop/quickstart.html>Documentation</a>.</p><p>Once you have downloaded the latest installer for you platform (Windows, Mac, Linux), you can go ahead and grab a GGUF model with ease:</p><figure><img loading=lazy src=/images/download_llama.png alt="Downloading the Meta llama3 (or 3.1) model in GPT4All"><figcaption><p>Downloading the Meta llama3 (or 3.1) model in GPT4All</p></figcaption></figure><p>Then, you can go ahead and start a chat and select the model you wish to use - it is possible to have multiple models downloaded and select between then&mldr; useful if you have models suited to specific tasks:</p><figure><img loading=lazy src=/images/before_first_chat.png alt="Launching a chat window in GPT4All"><figcaption><p>Launching a chat window in GPT4All</p></figcaption></figure><h2 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h2><p>Let me start by providing some system specs as context. I have by 2024 standards, a modest desktop computer:</p><ul><li>Ryzen 5600 6c/12t CPU - <a href=https://amzn.to/3zKpFrg>https://amzn.to/3zKpFrg</a> (an absolute steal in 2024!)</li><li>32GB 3200MHz DDR4 RAM - <a href=https://amzn.to/3TLWkno>https://amzn.to/3TLWkno</a></li><li>Nvidia RTX 3060 12GB GDDR6 GPU - <a href=https://amzn.to/3zurgl7>https://amzn.to/3zurgl7</a></li></ul><p>It would be great to test these models on bleeding edge hardware, enterprise hardware and even Apple M3 silicon, but hardware isn&rsquo;t cheap these days and I, like many, don&rsquo;t have the resources to acquire it. Thankfully it seems that llama3 performance at this hardware level is very good and there&rsquo;s minimal, perceivable slowdown as the context token count increases. I was experiencing speeds of 23 tokens per second in LM Studio and my chat focusing on writing a python script was remarkable.</p><p><strong>Using the 8B model, I saw a great throughput of 38tok/s</strong>, which feels equal to or faster in most cases than popular online AI chatbots.</p><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>Ok so, what did I <em>ask</em> llama3.1? This version of the model is fine tuned for instructional tasks, so for now, I asked for help improving a basic python script. This is a great way to test how current and accurate a model is. Here is my full transcript&mldr;</p><p><strong>Good morning, I have the following python code that I want to improve:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;https://demo.rading212.com/api/v0/equity/pies&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>headers <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;Authorization&#34;</span>: <span style=color:#e6db74>&#34;SOMERANDOMAPIKEY&#34;</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>response <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>get(url, headers<span style=color:#f92672>=</span>headers)
</span></span><span style=display:flex><span>print(response<span style=color:#f92672>.</span>status_code)
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> response<span style=color:#f92672>.</span>status_code <span style=color:#f92672>==</span> <span style=color:#ae81ff>200</span>:
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> response<span style=color:#f92672>.</span>json()
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> data:
</span></span><span style=display:flex><span>print(data)
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>print(response<span style=color:#f92672>.</span>status_code)
</span></span></code></pre></div><p><strong>I want to implement functions and threading so that I can run this code in a docker container. I want to pass important variables like the time between requests and the Authorization key. I also want to introduce a function which sends the data returned by the successful request to a postgres database. Here is what typical json response looks like:</strong></p><p><code>[{'id': 696969, 'cash': 0.48, 'dividendDetails': {'gained': 7.94, 'reinvested': 7.41, 'inCash': 0.46}, 'result': {'investedValue': 302.4, 'value': 318.35, 'result': 15.95, 'resultCoef': 0.0527}, 'progress': 0.3986, 'status': None}, {'id': 696968, 'cash': 0.12, 'dividendDetails': {'gained': 0.72, 'reinvested': 0.63, 'inCash': 0.09}, 'result': {'investedValue': 300.14, 'value': 404.68, 'result': 104.54, 'resultCoef': 0.3483}, 'progress': 0.0792, 'status': None}, {'id': 696867, 'cash': 0.55, 'dividendDetails': {'gained': 0.55, 'reinvested': 0.0, 'inCash': 0.55}, 'result': {'investedValue': 249.27, 'value': 278.99, 'result': 29.72, 'resultCoef': 0.1192}, 'progress': None, 'status': None}, {'id': 696868, 'cash': 0.0, 'dividendDetails': {'gained': 0.0, 'reinvested': 0.0, 'inCash': 0.0}, 'result': {'investedValue': 0, 'value': 0.0, 'result': 0.0, 'resultCoef': 0}, 'progress': None, 'status': None}, {'id': 2181613, 'cash': 0.0, 'dividendDetails': {'gained': 0.0, 'reinvested': 0.0, 'inCash': 0.0}, 'result': {'investedValue': 0, 'value': 0, 'result': 0, 'resultCoef': 0}, 'progress': None, 'status': None}]</code></p><hr><p>I can help you improve your code by implementing functions and threading as per your requirements.
Here&rsquo;s an improved version of the code:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> concurrent.futures <span style=color:#f92672>import</span> ThreadPoolExecutor
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> psycopg2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>EquityAPI</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, url, headers):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>url <span style=color:#f92672>=</span> url
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>headers <span style=color:#f92672>=</span> headers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_equity</span>(self):
</span></span><span style=display:flex><span>        response <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>get(self<span style=color:#f92672>.</span>url, headers<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>headers)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> response<span style=color:#f92672>.</span>json()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>send_to_postgres</span>(self, data):
</span></span><span style=display:flex><span>        conn <span style=color:#f92672>=</span> psycopg2<span style=color:#f92672>.</span>connect(
</span></span><span style=display:flex><span>            host<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;your_host&#39;</span>,
</span></span><span style=display:flex><span>            database<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;your_database&#39;</span>,
</span></span><span style=display:flex><span>            user<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;your_user&#39;</span>,
</span></span><span style=display:flex><span>            password<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;your_password&#39;</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        cur <span style=color:#f92672>=</span> conn<span style=color:#f92672>.</span>cursor()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> data:
</span></span><span style=display:flex><span>            query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;INSERT INTO your_table (id, cash, dividend_details) VALUES (</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>, </span><span style=color:#e6db74>%s</span><span style=color:#e6db74>, </span><span style=color:#e6db74>%s</span><span style=color:#e6db74>)&#34;</span>
</span></span><span style=display:flex><span>            values <span style=color:#f92672>=</span> (item[<span style=color:#e6db74>&#39;id&#39;</span>], item[<span style=color:#e6db74>&#39;cash&#39;</span>], str(item[<span style=color:#e6db74>&#39;dividendDetails&#39;</span>]))
</span></span><span style=display:flex><span>            cur<span style=color:#f92672>.</span>execute(query, values)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        conn<span style=color:#f92672>.</span>commit()
</span></span><span style=display:flex><span>        conn<span style=color:#f92672>.</span>close()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>main</span>():
</span></span><span style=display:flex><span>    url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;https://demo.rading212.com/api/v0/equity/pies&#39;</span>
</span></span><span style=display:flex><span>    headers <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;Authorization&#34;</span>: <span style=color:#e6db74>&#34;SOMERANDOMAPIKEY&#34;</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    api <span style=color:#f92672>=</span> EquityAPI(url, headers)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> api<span style=color:#f92672>.</span>get_equity()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(data) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>and</span> data[<span style=color:#ae81ff>0</span>]:
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#34;Received data:&#34;</span>, data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Send to Postgres</span>
</span></span><span style=display:flex><span>            api<span style=color:#f92672>.</span>send_to_postgres(data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            time<span style=color:#f92672>.</span>sleep(<span style=color:#ae81ff>10</span>)  <span style=color:#75715e># Wait for 10 seconds before next request</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>In this improved code:</p><p>We&rsquo;ve created a class EquityAPI that encapsulates the API functionality.</p><p>The get_equity() method sends an HTTP GET request to the specified URL and returns the JSON response.</p><p>The send_to_postgres() method connects to your Postgres database, inserts data into the table using SQL queries, and then commits and closes the connection.</p><p>In the main() function, we create an instance of the EquityAPI class and use a while loop with a 10-second sleep between requests.</p><p>Note that you&rsquo;ll need to replace &lsquo;your_host&rsquo;, &lsquo;your_database&rsquo;, &lsquo;your_user&rsquo;, and &lsquo;your_password&rsquo; in the Postgres connection string with your actual database credentials. Also, make sure to update your_table name according to your schema.</p><hr><p><strong>As this will be running in a container, TIME_BETWEEN_REQUESTS (time.sleep(10)) and AUTHORIZATION_KEY will be environment variables, can you adjust the code to load them from env?</strong></p><hr><p>I&rsquo;ve updated the code to use environment variables for TIME_BETWEEN_REQUESTS and AUTHORIZATION_KEY. Here&rsquo;s the revised version:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> concurrent.futures <span style=color:#f92672>import</span> ThreadPoolExecutor
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> psycopg2
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>EquityAPI</span>:
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#f92672>**</span>init<span style=color:#f92672>**</span>(self, url):
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>url <span style=color:#f92672>=</span> url
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>headers <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;Authorization&#34;</span>: os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;AUTHORIZATION_KEY&#34;</span>)}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_equity</span>(self):
</span></span><span style=display:flex><span>        response <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>get(self<span style=color:#f92672>.</span>url, headers<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>headers)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> response<span style=color:#f92672>.</span>json()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>send_to_postgres</span>(self, data):
</span></span><span style=display:flex><span>        conn <span style=color:#f92672>=</span> psycopg2<span style=color:#f92672>.</span>connect(
</span></span><span style=display:flex><span>            host<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;POSTGRES_HOST&#39;</span>),
</span></span><span style=display:flex><span>            database<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;POSTGRES_DB&#39;</span>),
</span></span><span style=display:flex><span>            user<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;POSTGRES_USER&#39;</span>),
</span></span><span style=display:flex><span>            password<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;POSTGRES_PASSWORD&#39;</span>)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        cur <span style=color:#f92672>=</span> conn<span style=color:#f92672>.</span>cursor()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> data:
</span></span><span style=display:flex><span>            query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;INSERT INTO your_table (id, cash, dividend_details) VALUES (</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>, </span><span style=color:#e6db74>%s</span><span style=color:#e6db74>, </span><span style=color:#e6db74>%s</span><span style=color:#e6db74>)&#34;</span>
</span></span><span style=display:flex><span>            values <span style=color:#f92672>=</span> (item[<span style=color:#e6db74>&#39;id&#39;</span>], item[<span style=color:#e6db74>&#39;cash&#39;</span>], str(item[<span style=color:#e6db74>&#39;dividendDetails&#39;</span>]))
</span></span><span style=display:flex><span>            cur<span style=color:#f92672>.</span>execute(query, values)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        conn<span style=color:#f92672>.</span>commit()
</span></span><span style=display:flex><span>        conn<span style=color:#f92672>.</span>close()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>main</span>():
</span></span><span style=display:flex><span>url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;https://demo.rading212.com/api/v0/equity/pies&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    api <span style=color:#f92672>=</span> EquityAPI(url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> api<span style=color:#f92672>.</span>get_equity()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(data) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>and</span> data[<span style=color:#ae81ff>0</span>]:
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#34;Received data:&#34;</span>, data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Send to Postgres</span>
</span></span><span style=display:flex><span>            api<span style=color:#f92672>.</span>send_to_postgres(data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            time<span style=color:#f92672>.</span>sleep(int(os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;TIME_BETWEEN_REQUESTS&#39;</span>)))  <span style=color:#75715e># Wait for TIME_BETWEEN_REQUESTS seconds before next request</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#f92672>**</span>name<span style=color:#f92672>**</span> <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;**main**&#34;</span>:
</span></span><span style=display:flex><span>main()
</span></span></code></pre></div><p>In this revised code:</p><p>We&rsquo;ve removed the headers parameter from the EquityAPI constructor and instead use os.environ.get(&ldquo;AUTHORIZATION_KEY&rdquo;) to load the authorization key from an environment variable.</p><p>Similarly, we&rsquo;re using time.sleep(int(os.environ.get(&lsquo;TIME_BETWEEN_REQUESTS&rsquo;))) to wait for the specified time between requests.</p><p>Make sure to set these environment variables in your container&rsquo;s configuration. You can do this by adding a .env file with the following content:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>AUTHORIZATION_KEY<span style=color:#f92672>=</span>SOMERANDOMAPIKEY
</span></span><span style=display:flex><span>POSTGRES_HOST<span style=color:#f92672>=</span>your_host
</span></span><span style=display:flex><span>POSTGRES_DB<span style=color:#f92672>=</span>your_database
</span></span><span style=display:flex><span>POSTGRES_USER<span style=color:#f92672>=</span>your_user
</span></span><span style=display:flex><span>POSTGRES_PASSWORD<span style=color:#f92672>=</span>your_password
</span></span><span style=display:flex><span>TIME_BETWEEN_REQUESTS<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>
</span></span></code></pre></div><p>Replace your_host, your_database, your_user, and your_password with your actual Postgres database credentials.</p><p>Feel free to ask if there&rsquo;s anything else I can help you improve!</p><hr><p><strong>I will be using postgres as a source for grafana, please can you include the current time in the postgres INSERT command?</strong></p><hr><p>Here is the revised code that includes the current timestamp in the Postgres insert statement:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> concurrent.futures <span style=color:#f92672>import</span> ThreadPoolExecutor
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> psycopg2
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> datetime <span style=color:#f92672>import</span> datetime
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>EquityAPI</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, url):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>url <span style=color:#f92672>=</span> url
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>headers <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;Authorization&#34;</span>: os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;AUTHORIZATION_KEY&#34;</span>)}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_equity</span>(self):
</span></span><span style=display:flex><span>        response <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>get(self<span style=color:#f92672>.</span>url, headers<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>headers)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> response<span style=color:#f92672>.</span>json()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>send_to_postgres</span>(self, data):
</span></span><span style=display:flex><span>        conn <span style=color:#f92672>=</span> psycopg2<span style=color:#f92672>.</span>connect(
</span></span><span style=display:flex><span>            host<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;POSTGRES_HOST&#39;</span>),
</span></span><span style=display:flex><span>            database<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;POSTGRES_DB&#39;</span>),
</span></span><span style=display:flex><span>            user<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;POSTGRES_USER&#39;</span>),
</span></span><span style=display:flex><span>            password<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;POSTGRES_PASSWORD&#39;</span>)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        cur <span style=color:#f92672>=</span> conn<span style=color:#f92672>.</span>cursor()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> data:
</span></span><span style=display:flex><span>            query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;INSERT INTO your_table (id, cash, dividend_details, timestamp) VALUES (</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>, </span><span style=color:#e6db74>%s</span><span style=color:#e6db74>, </span><span style=color:#e6db74>%s</span><span style=color:#e6db74>, </span><span style=color:#e6db74>%s</span><span style=color:#e6db74>)&#34;</span>
</span></span><span style=display:flex><span>            values <span style=color:#f92672>=</span> (item[<span style=color:#e6db74>&#39;id&#39;</span>], item[<span style=color:#e6db74>&#39;cash&#39;</span>], str(item[<span style=color:#e6db74>&#39;dividendDetails&#39;</span>]), datetime<span style=color:#f92672>.</span>now()<span style=color:#f92672>.</span>strftime(<span style=color:#e6db74>&#34;%Y-%m-</span><span style=color:#e6db74>%d</span><span style=color:#e6db74> %H:%M:%S&#34;</span>))
</span></span><span style=display:flex><span>            cur<span style=color:#f92672>.</span>execute(query, values)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        conn<span style=color:#f92672>.</span>commit()
</span></span><span style=display:flex><span>        conn<span style=color:#f92672>.</span>close()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>main</span>():
</span></span><span style=display:flex><span>    url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;https://demo.rading212.com/api/v0/equity/pies&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    api <span style=color:#f92672>=</span> EquityAPI(url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> api<span style=color:#f92672>.</span>get_equity()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(data) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>and</span> data[<span style=color:#ae81ff>0</span>]:
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#34;Received data:&#34;</span>, data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Send to Postgres</span>
</span></span><span style=display:flex><span>            api<span style=color:#f92672>.</span>send_to_postgres(data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            time<span style=color:#f92672>.</span>sleep(int(os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;TIME_BETWEEN_REQUESTS&#39;</span>)))  <span style=color:#75715e># Wait for TIME_BETWEEN_REQUESTS seconds before next request</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>In this revised code:</p><p>We&rsquo;ve added a timestamp column to the Postgres insert statement.</p><p>The timestamp is generated using datetime.now().strftime("%Y-%m-%d %H:%M:%S"), which returns the current date and time in the format &ldquo;YYYY-MM-DD HH:MM:SS&rdquo;.</p><p>Make sure your Postgres table has a timestamp column with the correct data type.</p><hr><p><strong>One last task, can you help me write a docker compose file which will create a container to run our python code, a container for postgres (where our python script will insert to) including an init script to make the table, and a grafana instance? Can you include any environment variables which we will need to pass to our python script such as database_url and table_name</strong></p><hr><p>Here is a Docker Compose file that creates three containers: one for your Python code, another for Postgres with an initial script to create the table, and finally a Grafana instance:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>version</span>: <span style=color:#e6db74>&#34;3&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>services</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>api</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>build</span>: <span style=color:#ae81ff>.</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>environment</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>AUTHORIZATION_KEY=SOMERANDOMAPIKEY</span>
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>POSTGRES_HOST=postgres</span>
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>POSTGRES_DB=mydatabase</span>
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>POSTGRES_USER=myuser</span>
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>POSTGRES_PASSWORD=mypassword</span>
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>TIME_BETWEEN_REQUESTS=10</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>depends_on</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>postgres</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>postgres</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>postgres</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>environment</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>POSTGRES_USER=myuser</span>
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>POSTGRES_PASSWORD=mypassword</span>
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>POSTGRES_DB=mydatabase</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>./init.sql:/docker-entrypoint-initdb.d/init.sql</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>      - <span style=color:#e6db74>&#34;5432:5432&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>grafana</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>grafana/grafana</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>environment</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>GF_SECURITY_ADMIN_PASSWORD=grafanapassword</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>depends_on</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>postgres</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>init.sql</span>:
</span></span></code></pre></div><p>This Docker Compose file:</p><p>Creates a container named api that runs your Python code.</p><p>Sets the necessary environment variables for your script, including database credentials and time between requests.</p><p>Depends on the Postgres service to ensure it&rsquo;s running before starting the API.</p><p>Defines another container called postgres, which uses an official PostgreSQL image. It sets up a new user, password, and database name as specified in the environment variables section of this file.</p><p>Mounts a volume for your initial SQL script (init.sql) at /docker-entrypoint-initdb.d/init.sql to run it during Postgres initialization.</p><p>Here&rsquo;s what you should put inside ./init.sql:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>TABLE</span> mytable (
</span></span><span style=display:flex><span>  id SERIAL <span style=color:#66d9ef>PRIMARY</span> <span style=color:#66d9ef>KEY</span>,
</span></span><span style=display:flex><span>  cash DECIMAL(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>  dividend_details JSONB <span style=color:#66d9ef>NOT</span> <span style=color:#66d9ef>NULL</span> <span style=color:#66d9ef>DEFAULT</span> <span style=color:#e6db74>&#39;{}&#39;</span>::JSONB,
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>timestamp</span> <span style=color:#66d9ef>TIMESTAMP</span> <span style=color:#66d9ef>WITH</span> TIME <span style=color:#66d9ef>ZONE</span> <span style=color:#66d9ef>NOT</span> <span style=color:#66d9ef>NULL</span> <span style=color:#66d9ef>DEFAULT</span> <span style=color:#66d9ef>CURRENT_TIMESTAMP</span>
</span></span><span style=display:flex><span>);
</span></span></code></pre></div><p>This SQL script creates a table named mytable with the columns you specified in your Python code.</p><p>Remember to replace placeholders like SOMERANDOMAPIKEY, mydatabase, myuser, and mypassword with actual values for your application. Also, update any other environment variables as needed according to their usage within your script.</p><hr><p><strong>IMPRESSIVE!</strong></p><hr><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>The responses are even faster than llama3 and the responses seem moderately more accurate. In comparison to previous tests, there are some observed improvements when we pivoted from python to docker compose YAML. The indicator has certainly turned slight more towards concern for programmers jobs&mldr;</p><h3 id=tldr>TLDR<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h3><p><strong>Llama3.1 another strong increment if you are willing to disregard the insane computer required to train it. THe move toward open source is welcome will hopefully force more innovation and reduce &lsquo;moat digging&rsquo; for paid services</strong></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://exitcode0.net/tags/llm/>LLM</a></li><li><a href=https://exitcode0.net/tags/ai/>AI</a></li><li><a href=https://exitcode0.net/tags/gpt/>GPT</a></li><li><a href=https://exitcode0.net/tags/llama/>Llama</a></li></ul><nav class=paginav><a class=prev href=https://exitcode0.net/posts/how-to-expand-an-lvm-lv-to-use-all-space-on-the-pv/><span class=title>« Prev</span><br><span>Complete 2024 Guide: How to expand a Ubuntu LVM LV to use all space on the PV</span>
</a><a class=next href=https://exitcode0.net/posts/feeding-my-cat-with-home-assistant/><span class=title>Next »</span><br><span>Feeding My Cat With Home Assistant</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Testing Llama3.1 (8B) With LM Studio on x" href="https://x.com/intent/tweet/?text=Testing%20Llama3.1%20%288B%29%20With%20LM%20Studio&amp;url=https%3a%2f%2fexitcode0.net%2fposts%2ftesting-llama3_1-with-gpt4all%2f&amp;hashtags=LLM%2cAI%2cGPT%2cllama"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Testing Llama3.1 (8B) With LM Studio on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fexitcode0.net%2fposts%2ftesting-llama3_1-with-gpt4all%2f&amp;title=Testing%20Llama3.1%20%288B%29%20With%20LM%20Studio&amp;summary=Testing%20Llama3.1%20%288B%29%20With%20LM%20Studio&amp;source=https%3a%2f%2fexitcode0.net%2fposts%2ftesting-llama3_1-with-gpt4all%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Testing Llama3.1 (8B) With LM Studio on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fexitcode0.net%2fposts%2ftesting-llama3_1-with-gpt4all%2f&title=Testing%20Llama3.1%20%288B%29%20With%20LM%20Studio"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Testing Llama3.1 (8B) With LM Studio on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fexitcode0.net%2fposts%2ftesting-llama3_1-with-gpt4all%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Testing Llama3.1 (8B) With LM Studio on whatsapp" href="https://api.whatsapp.com/send?text=Testing%20Llama3.1%20%288B%29%20With%20LM%20Studio%20-%20https%3a%2f%2fexitcode0.net%2fposts%2ftesting-llama3_1-with-gpt4all%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Testing Llama3.1 (8B) With LM Studio on telegram" href="https://telegram.me/share/url?text=Testing%20Llama3.1%20%288B%29%20With%20LM%20Studio&amp;url=https%3a%2f%2fexitcode0.net%2fposts%2ftesting-llama3_1-with-gpt4all%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Testing Llama3.1 (8B) With LM Studio on ycombinator" href="https://news.ycombinator.com/submitlink?t=Testing%20Llama3.1%20%288B%29%20With%20LM%20Studio&u=https%3a%2f%2fexitcode0.net%2fposts%2ftesting-llama3_1-with-gpt4all%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://exitcode0.net/>ExitCode0</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>