<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Generating Artwork With Stable Diffusion - April 2024 Edition | ExitCode0</title>
<meta name=keywords content="stable-diffusion,AI,art"><meta name=description content="A look at the state of artwork generation with stable-diffusion in April 2024. Investigating if (local) stable-diffusion can reliably produce enjoyable artwork."><meta name=author content="Tom"><link rel=canonical href=https://exitcode0.net/posts/generating-artwork-with-stable-diffusion-april-2024/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://exitcode0.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://exitcode0.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://exitcode0.net/favicon-32x32.png><link rel=apple-touch-icon href=https://exitcode0.net/apple-touch-icon.png><link rel=mask-icon href=https://exitcode0.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://exitcode0.net/posts/generating-artwork-with-stable-diffusion-april-2024/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745507566538945" crossorigin=anonymous></script><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=exit data-description="Support me on Buy me a coffee!" data-message="Buy me a coffee" data-color=#FF813F data-position=Right data-x_margin=18 data-y_margin=165></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-EDE9MPZR9T"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-EDE9MPZR9T")}</script><meta property="og:title" content="Generating Artwork With Stable Diffusion - April 2024 Edition"><meta property="og:description" content="A look at the state of artwork generation with stable-diffusion in April 2024. Investigating if (local) stable-diffusion can reliably produce enjoyable artwork."><meta property="og:type" content="article"><meta property="og:url" content="https://exitcode0.net/posts/generating-artwork-with-stable-diffusion-april-2024/"><meta property="og:image" content="https://exitcode0.net/images/stable-diffusion-april-2024.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-16T20:00:00+01:00"><meta property="article:modified_time" content="2024-04-16T20:00:00+01:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://exitcode0.net/images/stable-diffusion-april-2024.png"><meta name=twitter:title content="Generating Artwork With Stable Diffusion - April 2024 Edition"><meta name=twitter:description content="A look at the state of artwork generation with stable-diffusion in April 2024. Investigating if (local) stable-diffusion can reliably produce enjoyable artwork."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://exitcode0.net/posts/"},{"@type":"ListItem","position":2,"name":"Generating Artwork With Stable Diffusion - April 2024 Edition","item":"https://exitcode0.net/posts/generating-artwork-with-stable-diffusion-april-2024/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Generating Artwork With Stable Diffusion - April 2024 Edition","name":"Generating Artwork With Stable Diffusion - April 2024 Edition","description":"A look at the state of artwork generation with stable-diffusion in April 2024. Investigating if (local) stable-diffusion can reliably produce enjoyable artwork.","keywords":["stable-diffusion","AI","art"],"articleBody":"As I write this, my computer’s Nvidia RTX GPU is busy converting watts into fictional images of robots and given the current cost of electricity, I am asking myself if the results it is producing are worth the resources required to produce them. In the article I hope to demonstrate how I am using AUTOMATIC1111/stable-diffusion-webui and Mikubill/sd-webui-controlnet to generate some borderline “art”. If you are looking to skip ahead to certain section, you can jump ahead using the table of contents above.\nPrompt: ‘A robot emerging from a cave’\nDefinitions and Abbreviations: Some common abbreviations and terms that you might encounter in this article or other sites whilst researching this topic:\nSD: stable-diffusion\na111: AUTOMATIC1111\nCFG: Classifier Free Guidance Scale\nSeed: A value that determines the output of random number generator - if you create an image with same parameters and seed as another image, you’ll get the same result\nSampling Steps: How many times to improve the generated image iteratively; higher values take longer; very low values can produce bad results\nCFG scale: how strongly the image should conform to prompt - lower values produce more creative results\nInstalling AUTOMATIC1111’s stable-diffusion-webui (Windows) AUTOMATIC1111’s stable-diffusion-webui is a web interface for Stable Diffusion, implemented using Gradio library. This project makes txt2img and img2img generation possible much like a lot of popular online services such as DALL·E and Midjourney, but instead allowing you to run it locally and for free (minus the cost of computer hardware and power). You should know that Stable Diffusion is in fact the product of Stability AI and AUTOMATIC1111’s stable-diffusion-webui allows us to make use of the open source models that Stability AI and many others have released.\nPre-Requisite Dependencies Before you get started, there are some additional components that you will need to install first. You may chose to download a newer version of python3, such as python 3.11.X; your mileage may vary and you might have some package compatibility issues.\nPython 3.10.6: https://www.python.org/downloads/release/python-3106/\nGit: https://git-scm.com/download/win\nCode from the AUTOMATIC1111/stable-diffusion-webui repository:\ngit clone https://github.com/AUTOMATIC1111/stable-diffusion-webui Once you have complete the above steps. Open a command prompt in your stable-diffusion-webui directory and run the following command:\n./webui-user.bat Addition arguments such as --api can be added if you want to run the tool with additional features enabled. These arguments can only be passed to the program by launching from the command prompt, rather than just running the .bat file from Windows explorer. A full ist of arguments: https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Command-Line-Arguments-and-Settings. The full command that I use to launch (with additional performance arguments) is as follows:\n.\\webui-user.bat --api --no-half --opt-split-attention --xformers Once the .bat completes the local packages and dependency installation, it will launch your default browser to http://127.0.0.1:7860/. Now you are ready to start generating “art” - be sure to keep your command prompt window running; closing this window will stop stable-diffusion-webui.\nAUTOMATIC1111/stable-diffusion-webui running on on our local machine.\nIf you are looking to run this on an Apple Silicon based system (M1/M2/M3), instructions can be found here: https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon\nAdding Models Models can often be referred to as “checkpoints”; they are files that contain a collection of neural network parameters and weights trained using images as inspiration. Models trained on a particular type of subject matter will have a better chance of creating similar images; want anime style images… use a model trained on anime.\nYou can add any additional model files to your stable-diffusion-webui models path:\nFor example: C:\\AI\\stable-diffusion-webui\\models\\Stable-diffusion\nHere are some links to models that I have tested and a highly subjective and unscientific success rating:\nModel Repository URL Personal Success Rating v1-5-pruned-emaonly.safetensors huggingface.co/runwayml 3/5 stars (v2)512-base-ema.ckpt huggingface.co/stabilityai/stable-diffusion-2-base 3/5 stars v2-1_768-ema-pruned.ckpt huggingface.co/stabilityai/stable-diffusion-2-1 3/5 stars robo-diffusion-v1.ckpt huggingface.co/nousr/robo-diffusion 2/5 stars Chilloutmix-Ni-pruned-fp16-fix.safetensors huggingface.co/swl-models/chilloutmix-ni fp16 4/5 stars chilloutmix-Ni-non-ema-fp32.safetensors huggingface.co/swl-models/chilloutmix-ni non-ema-fp32 4/5 stars You should note that it is possible to use the SD Refiner feature to switch models part way through the generation process and define at what stage of generation that should happen. So in the example of the following prompt, we can switch models to something based on a relevant training dataset:\na lonely bipedal robot walking through a valley. best quality. photograph. nuclear. cataclysmic. end of the world Enabling refinement with a model based on a highly specific training dataset.\nThe Switch At setting determines the fraction of sampling steps when the switch to refiner model should happen; 1=never, 0.5=switch in the middle of generation.\nPrompt Techniques Before we continue, I would like to warn you that most certainly from a relative beginner’s perspective, image generation with SD is a bit of a fuzzy logic. There appears to be no certain method to govern the type of image that is produced; you can only tweak guidance where possible using the tools available to you and the model will ultimate decided on the output. These models are trained against a vast number of parameters and are essentially guessing what each pixel should look like based on trends observed in the train data set.\nTherefore I would suggest that you approach this with patience and try to record your methodology and steps taken to reach a visual output that you are happy with. Change only one variable at a time, but understand that the inherent randomness out the image generation could still provide an unsatisfactory result.\nNegative Prompts In the same way that (positive) prompts help define your expected outcomes, negative prompt help to shape the generated image by providing SD with a list ob objects and types to avoid when it is consulting its inference model.\nFor example, you want to generate a portrait, but Stable Diffusion provided duplicates faces or bodies. A negative prompt including the phrase “duplicates” would provide a high-dimension anchor, which the process seek to stray away from.\nPrompt Examples Prompt: a lonely bipedal robot walking through a valley. best quality. photograph. nuclear. cataclysmic. end of the world Negative: human. duplicates. hair Prompt: A hyper realistic avatar of a man riding on a black honda cbr 650r in leather suit,high detail, high quality,8K,photo realism Negative: flying mirrors,low quality Prompt: electronik robot and ofice ,unreal engine, cozy indoor lighting, artstation, detailed, digital painting,cinematic,character design by mark ryden and pixar and hayao miyazaki, unreal 5, daz, hyperrealistic, octane render Negative: ugly hands, disjointed More fantastic prompt examples can be found here: Prompt examples - Stable Diffusion .\nYou can skip ahead to see more results here in the results comparison section.\nProvided that no generation configuration values are changed and a fixed seed is defined (including Hires Fix settings), you will always get the same output from the vision model. Each time to you generate an image, the webui shows your the parameters used; there for you could replicate any image using these parameters. Try this:\nUsing the SD output parameters to recreate identical images.\nOne thing you must understand, if you increase the Batch Count, the seed value still increments (by 1, at default).\nBetter still, if you are using the --api, you could pass parameters in your API GET request. You can read more about the API here: stable-diffusion-webui/wiki/API. There you will find a link to this useful python script for getting started with requests over the API: txt2img and img2img example python script.\nStyles Style are an amendment to a prompt and can have a offer a very strong level of control over the type of image that SD is going to produce for you. Ordinarily, an artist would have their style of work classified as a particular type of work. Therefore a model trained on a wide array of data should have an understanding of what each of these styles might look like and be able to make better predictions for the next generated pixel.\nOne of the best resources I have encountered is this style cheat sheet: supagruen.github.io/StableDiffusion-CheatSheet. Simply pick a style you are interested in, copy the style prompt and append it to your existing prompt in stable-diffusion-webui. The sit splits out the styles with tags and categories to help you pick your perfect style. This has to be one of the most valuable AI image generation resources to date.\nStable Diffusion 1.5 - Cheat Sheet.\nInstalling Mikubill’s sd-webui-controlnet Installing extensions in stable-diffusion-webui is ordinarily a very user friendly, low skill process. There are a large number of community produced plugins that enable a huge amount of features in SD.\nSo let’s install ControlNet:\nOpen “Extensions” tab. Open “Install from URL” tab in the tab. Enter https://github.com/Mikubill/sd-webui-controlnet.git to “URL for extension’s git repository”. Press “Install” button. Wait until you see the message “Installed into stable-diffusion-webui\\extensions\\sd-webui-controlnet”. Go to “Installed” tab, click “Check for updates”, and then click “Apply and restart UI”. Completely restart A1111 webui - end the command-prompt/terminal window and launch again. Installing Mikubill’s sd-webui-controlnet\nShould you need to forcefully remove this or any other extension, you can simply delete the extension named folder from the following path:\n~ \\stable-diffusion-webui\\extensions Controlnet Techniques Canny I have found that sometimes enabling ControlNet can harm the creativity of text2img generation, irrespective of your classifier free guidance (CFG) scale value. Controlnet can be tweaked to respect the prompt with a higher priority, but it can lead to some questionable outputs due to it’s influence. My experimenting with Controlnet functionality is currently incomplete and I hope to cover my findings in a future article. It is best used with very loose control influence and with an image that closely resembles your desired outcome. This is why, it can be great for human faces, when paired with an appropriate model and style.\nUsing Canny to paint myself as a soldier in World War 2 and World War 3.\nOpenPose This ContorlNet module aims to by add extra conditions to the neural network structure in order to control diffusion model outcomes; OpenPose does this with human pose estimation. Needless to say, that works best on humanoid subjects. The module can analyse an image to produce a wireframe for human poses and this is applied to conditions of generation.\nHere is how we can configure openpose in the controlnet plug section of the SD-webui interface:\nControlNet OpenPose using default values.\nThis results in the following wireframe pose detection - not a perfect capture due to a slightly ambiguous input image of a non-human entity:\nControlNet OpenPose generating a pose structure based on an input image.\nUltimately resulting in some very nice output images, which adhere to our prompt, provide creativity and strike the same pose. I believe that the slight issue in the pose is what caused SD to confuse the direction that the subject was facing in some images.\nControlNet OpenPose generating a batch of 4 images based on our wireframe and prompt.\nImage Resolution and Performance You’re available hardware is going to make a large difference in generation time. As a general rule of thumb, graphics cards with a high memory availability offer better performance during inference based task such as Stable Diffusion. Howwever, the Nvidia GPUs, particularly RTX GPUs which saw the introduction of Tensor cores, clearly have the computational advantage:\nTom’s Hardware Stable Diffusion benchmark article\nWhilst it is not entirely necessary to have an RTX graphics card, using one will significantly reduce your generation times. Nvidia is quite literally selling shovels in a gold rush…\nHere are some performance optimization options that can be applied using command-prompt/terminal flags:\nCommandline argument Explanation --opt-sdp-attention May results in faster speeds than using xFormers on some systems but requires more VRAM. (non-deterministic) --opt-sdp-no-mem-attention May results in faster speeds than using xFormers on some systems but requires more VRAM. (deterministic, slightly slower than --opt-sdp-attention and uses more VRAM) --xformers Use xFormers library. Great improvement to memory consumption and speed. Nvidia GPUs only. (deterministic as of 0.0.19 [webui uses 0.0.20 as of 1.4.0]) --force-enable-xformers Enables xFormers regardless of whether the program thinks you can run it or not. Do not report bugs you get running this. --opt-split-attention Cross attention layer optimization significantly reducing memory use for almost no cost (some report improved performance with it). Black magic. On by default for torch.cuda, which includes both NVidia and AMD cards. --disable-opt-split-attention Disables the optimization above. --opt-sub-quad-attention Sub-quadratic attention, a memory efficient Cross Attention layer optimization that can significantly reduce required memory, sometimes at a slight performance cost. Recommended if getting poor performance or failed generations with a hardware/software configuration that xFormers doesn’t work for. On macOS, this will also allow for generation of larger images. --opt-split-attention-v1 Uses an older version of the optimization above that is not as memory hungry (it will use less VRAM, but will be more limiting in the maximum size of pictures you can make). --medvram Makes the Stable Diffusion model consume less VRAM by splitting it into three parts - cond (for transforming text into numerical representation), first_stage (for converting a picture into latent space and back), and unet (for actual denoising of latent space) and making it so that only one is in VRAM at all times, sending others to CPU RAM. Lowers performance, but only by a bit - except if live previews are enabled. --lowvram An even more thorough optimization of the above, splitting unet into many modules, and only one module is kept in VRAM. Devastating for performance. *do-not-batch-cond-uncond Only before 1.6.0: prevents batching of positive and negative prompts during sampling, which essentially lets you run at 0.5 batch size, saving a lot of memory. Decreases performance. Not a command line option, but an optimization implicitly enabled by using --medvram or --lowvram. In 1.6.0, this optimization is not enabled by any command line flags, and is instead enabled by default. It can be disabled in settings, Batch cond/uncond option in Optimizations category. --always-batch-cond-uncond Only before 1.6.0: disables the optimization above. Only makes sense together with --medvram or --lowvram. In 1.6.0, this command line flag does nothing. --opt-channelslast Changes torch memory type for stable diffusion to channels last. Effects not closely studied. --upcast-sampling For Nvidia and AMD cards normally forced to run with --no-half, should improve generation speed. If unlimited power is what you seek and you are running SD on Windows, in Nvidia control panel, 3d parameters, change your power profile to “maximum performance”. You are now trading even more money for faster images.\nSize Matters Unfortunately, size does matter when it comes to performance… the larger the resolution, the more memory resources you are going to require. By default SD targets 512px x 512px. My best advice is to target a small resolution whilst you build an understanding of the models and controls needed to reach your ambitions with SD. This will allow you to quickly make small, iterative changes and have a faster feedback loop, whilst you work out which settings are best for your use case.\nUnfortunately, your available memory resources may be the limit to the maximum output resolution; a work around is to use upscaling.\nHires Fix Uses a two step process to partially create an image at smaller resolution (such as 512px x 512px), upscale, and then improve details in it without changing composition. So let’s say that we want to optimise our image output for max quality Instagram stories, we can set a width of 540px, a height of 960px and an upscale factor of 2; giving a 1080px by 1920px image.\nOne setting that we must consider is Denoising strength - this determines how little respect the algorithm should have for image’s content. At 0, nothing will change, and at 1 you’ll get an unrelated image. With values below 1.0, processing will take less steps than the Sampling Steps slider specifies. I have found that best results are when left at the default value of 0.7.\nAUTOMATIC1111/stable-diffusion-webui with Hires set to scale images by a factor of 2.\nUpscaling It is the year 2024 and if we are being truthful, 512px by 512px images look like an early Fallout game and 1024px by 1024px is also unacceptable in this age of retina displays. Fortunately, we can use the upscaling feature in SD-webui to grace our eyeballs with higher resolution images, after generation. Let’s take a look at how to upscale in SD-webui:\nUpscaling with the Extras tab.\nThe above figure takes out 1920px by 1080px image and upscales it to 3840px by 2160px. The main caveat to upscaling is that unlike Hires Fix, we are not adding detail when we expand the image because no sampling is taking place.\nPower Consumption A quick note on power consumption. Living in a western nation, power costs are not an insignificant expense and as we have learned, running local software like stable-diffusion-webui and LMStudio induces a significant load on a computer for an extended period of time. Here is some back of a napkin maths for an RTX 3060 (12GB) running at stock clock settings (Ryzen 5600, 32GB DDR4, 1x m.2 SSD):\nConsumption in watts during generation: 480 Time take in hours to generate 10 images at 1024 x 1024: 1.2 Electricity rate per kWh: £0.24 £0.14 per 1.2 hours ~£0.01 per image This might not seem like much but I have spent somewhere in the region of 48 hours running generation just in the processing of learning and developing this article. That’s £5.64 at current rates… so consider buying me a coffee?\nResults So if you haven’t already gathered, there are a lot of variables that we can modify to guide the image generation process and produce a result that we are aiming for. Let’s take a look at what sand can do when we teach it to think… here are some of my favourite images so far:\nLonely robots walking away into a cataclysmic valley part 1.\nLonely robots walking away into a cataclysmic valley part 2.\nLonely robots walking away into a cataclysmic valley part 3.\nLonely robots walking away into a cataclysmic valley part 4.\nLonely robots walking away into a cataclysmic valley part 5.\nLonely robots walking away into a cataclysmic valley part 6.\nLonely robots walking away into a cataclysmic valley part 7.\nLonely robots walking away into a cataclysmic valley part 8.\nIf you found this article useful, consider sharing it in your social circles or bookmarking this page as I have lots more stable-diffusion content in the works…\n","wordCount":"3027","inLanguage":"en","image":"https://exitcode0.net/images/stable-diffusion-april-2024.png","datePublished":"2024-04-16T20:00:00+01:00","dateModified":"2024-04-16T20:00:00+01:00","author":{"@type":"Person","name":"Tom"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://exitcode0.net/posts/generating-artwork-with-stable-diffusion-april-2024/"},"publisher":{"@type":"Organization","name":"ExitCode0","logo":{"@type":"ImageObject","url":"https://exitcode0.net/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://exitcode0.net/ accesskey=h title="ExitCode0 (Alt + H)">ExitCode0</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://datasolace.com/ title=DataSolace><span>DataSolace</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://exitcode0.net/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://exitcode0.net/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://exitcode0.net/privacy/ title=Privacy><span>Privacy</span></a></li><li><a href=https://exitcode0.net/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://exitcode0.net/>Home</a>&nbsp;»&nbsp;<a href=https://exitcode0.net/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Generating Artwork With Stable Diffusion - April 2024 Edition</h1><div class=post-description>A look at the state of artwork generation with stable-diffusion in April 2024. Investigating if (local) stable-diffusion can reliably produce enjoyable artwork.</div><div class=post-meta><span title='2024-04-16 20:00:00 +0100 +0100'>April 16, 2024</span>&nbsp;·&nbsp;15 min&nbsp;·&nbsp;Tom</div></header><figure class=entry-cover><img loading=eager src=https://exitcode0.net/images/stable-diffusion-april-2024.png alt="How to generate artwork With Stable Diffusion Webui"></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#definitions-and-abbreviations aria-label="Definitions and Abbreviations:">Definitions and Abbreviations:</a></li></ul><li><a href=#installing-automatic1111s-stable-diffusion-webui-windows aria-label="Installing AUTOMATIC1111&rsquo;s stable-diffusion-webui (Windows)">Installing AUTOMATIC1111&rsquo;s stable-diffusion-webui (Windows)</a><ul><li><a href=#pre-requisite-dependencies aria-label="Pre-Requisite Dependencies">Pre-Requisite Dependencies</a></li><li><a href=#adding-models aria-label="Adding Models">Adding Models</a></li></ul></li><li><a href=#prompt-techniques aria-label="Prompt Techniques">Prompt Techniques</a><ul><li><a href=#negative-prompts aria-label="Negative Prompts">Negative Prompts</a><ul><li><a href=#prompt-examples aria-label="Prompt Examples">Prompt Examples</a></li></ul></li><li><a href=#styles aria-label=Styles>Styles</a></li></ul></li><li><a href=#installing-mikubills-sd-webui-controlnet aria-label="Installing Mikubill&rsquo;s sd-webui-controlnet">Installing Mikubill&rsquo;s sd-webui-controlnet</a></li><li><a href=#controlnet-techniques aria-label="Controlnet Techniques">Controlnet Techniques</a><ul><li><a href=#canny aria-label=Canny>Canny</a></li><li><a href=#openpose aria-label=OpenPose>OpenPose</a></li></ul></li><li><a href=#image-resolution-and-performance aria-label="Image Resolution and Performance">Image Resolution and Performance</a><ul><li><a href=#size-matters aria-label="Size Matters">Size Matters</a><ul><li><a href=#hires-fix aria-label="Hires Fix">Hires Fix</a></li><li><a href=#upscaling aria-label=Upscaling>Upscaling</a></li></ul></li><li><a href=#power-consumption aria-label="Power Consumption">Power Consumption</a></li></ul></li><li><a href=#results aria-label=Results>Results</a></li></ul></div></details></div><div class=post-content><p>As I write this, my computer&rsquo;s Nvidia RTX GPU is busy converting watts into fictional images of robots and given the current cost of electricity, I am asking myself if the results it is producing are worth the resources required to produce them. In the article I hope to demonstrate how I am using AUTOMATIC1111/stable-diffusion-webui and Mikubill/sd-webui-controlnet to generate some borderline <em>&ldquo;art&rdquo;</em>. If you are looking to skip ahead to certain section, you can jump ahead using the table of contents above.</p><figure><img loading=lazy src=/images/stable-diffusion-april-2024-01.jpg alt="Prompt: &lsquo;A robot emerging from a cave&rsquo;"><figcaption><p>Prompt: <em>&lsquo;A robot emerging from a cave&rsquo;</em></p></figcaption></figure><h3 id=definitions-and-abbreviations>Definitions and Abbreviations:<a hidden class=anchor aria-hidden=true href=#definitions-and-abbreviations>#</a></h3><p>Some common abbreviations and terms that you might encounter in this article or other sites whilst researching this topic:</p><ul><li><p><strong>SD</strong>: stable-diffusion</p></li><li><p><strong>a111</strong>: AUTOMATIC1111</p></li><li><p><strong>CFG</strong>: Classifier Free Guidance Scale</p></li><li><p><strong>Seed</strong>: A value that determines the output of random number generator - if you create an image with same parameters and seed as another image, you&rsquo;ll get the same result</p></li><li><p><strong>Sampling Steps</strong>: How many times to improve the generated image iteratively; higher values take longer; very low values can produce bad results</p></li><li><p><strong>CFG scale</strong>: how strongly the image should conform to prompt - lower values produce more creative results</p></li></ul><hr><h2 id=installing-automatic1111s-stable-diffusion-webui-windows>Installing AUTOMATIC1111&rsquo;s stable-diffusion-webui (Windows)<a hidden class=anchor aria-hidden=true href=#installing-automatic1111s-stable-diffusion-webui-windows>#</a></h2><p>AUTOMATIC1111&rsquo;s stable-diffusion-webui is a web interface for Stable Diffusion, implemented using Gradio library. This project makes txt2img and img2img generation possible much like a lot of popular online services such as DALL·E and Midjourney, but instead allowing you to run it locally and for free (minus the cost of computer hardware and power). You should know that Stable Diffusion is in fact the product of Stability AI and AUTOMATIC1111&rsquo;s stable-diffusion-webui allows us to make use of the open source models that Stability AI and many others have released.</p><h3 id=pre-requisite-dependencies>Pre-Requisite Dependencies<a hidden class=anchor aria-hidden=true href=#pre-requisite-dependencies>#</a></h3><p>Before you get started, there are some additional components that you will need to install first. You may chose to download a newer version of python3, such as python 3.11.X; your mileage may vary and you might have some package compatibility issues.</p><ol><li><p>Python 3.10.6: <a href=https://www.python.org/downloads/release/python-3106/>https://www.python.org/downloads/release/python-3106/</a></p></li><li><p>Git: <a href=https://git-scm.com/download/win>https://git-scm.com/download/win</a></p></li><li><p>Code from the AUTOMATIC1111/stable-diffusion-webui repository:</p><pre tabindex=0><code>git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
</code></pre></li></ol><p>Once you have complete the above steps. Open a command prompt in your <code>stable-diffusion-webui</code> directory and run the following command:</p><pre tabindex=0><code>./webui-user.bat
</code></pre><p>Addition arguments such as <code>--api</code> can be added if you want to run the tool with additional features enabled. These arguments can only be passed to the program by launching from the command prompt, rather than just running the .bat file from Windows explorer. A full ist of arguments: <a href=https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Command-Line-Arguments-and-Settings>https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Command-Line-Arguments-and-Settings</a>. The full command that I use to launch (with additional <a href=#image-resolution-and-performance>performance</a> arguments) is as follows:</p><pre tabindex=0><code>.\webui-user.bat --api --no-half --opt-split-attention --xformers
</code></pre><p>Once the .bat completes the local packages and dependency installation, it will launch your default browser to <a href=http://127.0.0.1:7860/>http://127.0.0.1:7860/</a>. Now you are ready to start generating &ldquo;<em>art</em>&rdquo; - be sure to keep your command prompt window running; closing this window will stop stable-diffusion-webui.</p><figure><img loading=lazy src=/images/stable-diffusion-setup.png alt="AUTOMATIC1111/stable-diffusion-webui running on on our local machine."><figcaption><p>AUTOMATIC1111/stable-diffusion-webui running on on our local machine.</p></figcaption></figure><p>If you are looking to run this on an Apple Silicon based system (M1/M2/M3), instructions can be found here: <a href=https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon>https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon</a></p><h3 id=adding-models>Adding Models<a hidden class=anchor aria-hidden=true href=#adding-models>#</a></h3><p>Models can often be referred to as &ldquo;checkpoints&rdquo;; they are files that contain a collection of neural network parameters and weights trained using images as inspiration. Models trained on a particular type of subject matter will have a better chance of creating similar images; want anime style images&mldr; use a model trained on anime.</p><p>You can add any additional model files to your stable-diffusion-webui models path:</p><p>For example: <code>C:\AI\stable-diffusion-webui\models\Stable-diffusion</code></p><p>Here are some links to models that I have tested and a highly subjective and unscientific success rating:</p><table><thead><tr><th style=text-align:center>Model</th><th style=text-align:center>Repository URL</th><th style=text-align:center>Personal Success Rating</th></tr></thead><tbody><tr><td style=text-align:center>v1-5-pruned-emaonly.safetensors</td><td style=text-align:center><a href=https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned-emaonly.safetensors>huggingface.co/runwayml</a></td><td style=text-align:center>3/5 stars</td></tr><tr><td style=text-align:center>(v2)512-base-ema.ckpt</td><td style=text-align:center><a href=https://huggingface.co/stabilityai/stable-diffusion-2-base/tree/main>huggingface.co/stabilityai/stable-diffusion-2-base</a></td><td style=text-align:center>3/5 stars</td></tr><tr><td style=text-align:center>v2-1_768-ema-pruned.ckpt</td><td style=text-align:center><a href=https://huggingface.co/stabilityai/stable-diffusion-2-1/tree/main>huggingface.co/stabilityai/stable-diffusion-2-1</a></td><td style=text-align:center>3/5 stars</td></tr><tr><td style=text-align:center>robo-diffusion-v1.ckpt</td><td style=text-align:center><a href=https://huggingface.co/nousr/robo-diffusion/tree/e257612be1038e933e1a23f6a291591038ba102a/models>huggingface.co/nousr/robo-diffusion</a></td><td style=text-align:center>2/5 stars</td></tr><tr><td style=text-align:center>Chilloutmix-Ni-pruned-fp16-fix.safetensors</td><td style=text-align:center><a href=https://huggingface.co/swl-models/chilloutmix-ni/blob/main/chilloutmix-Ni-ema-fp16.safetensors>huggingface.co/swl-models/chilloutmix-ni fp16</a></td><td style=text-align:center>4/5 stars</td></tr><tr><td style=text-align:center>chilloutmix-Ni-non-ema-fp32.safetensors</td><td style=text-align:center><a href=https://huggingface.co/swl-models/chilloutmix-ni/blob/main/chilloutmix-Ni-non-ema-fp32.safetensors>huggingface.co/swl-models/chilloutmix-ni non-ema-fp32</a></td><td style=text-align:center>4/5 stars</td></tr></tbody></table><p>You should note that it is possible to use the SD Refiner feature to switch models part way through the generation process and define at what stage of generation that should happen. So in the example of the following prompt, we can switch models to something based on a relevant training dataset:</p><pre tabindex=0><code>a lonely bipedal robot walking through a valley. best quality. photograph. nuclear. cataclysmic. end of the world
</code></pre><figure><img loading=lazy src=/images/stable-diffusion-refine.png alt="Enabling refinement with a model based on a highly specific training dataset."><figcaption><p>Enabling refinement with a model based on a highly specific training dataset.</p></figcaption></figure><p>The <strong>Switch At</strong> setting determines the fraction of sampling steps when the switch to refiner model should happen; 1=never, 0.5=switch in the middle of generation.</p><hr><h2 id=prompt-techniques>Prompt Techniques<a hidden class=anchor aria-hidden=true href=#prompt-techniques>#</a></h2><p>Before we continue, I would like to warn you that most certainly from a relative beginner&rsquo;s perspective, image generation with SD is a bit of a fuzzy logic. There appears to be no certain method to govern the type of image that is produced; you can only tweak guidance where possible using the tools available to you and the model will ultimate decided on the output. These models are trained against a vast number of parameters and are essentially guessing what each pixel should look like based on trends observed in the train data set.</p><p>Therefore I would suggest that you approach this with patience and try to record your methodology and steps taken to reach a visual output that you are happy with. Change only one variable at a time, but understand that the inherent randomness out the image generation could still provide an unsatisfactory result.</p><h3 id=negative-prompts>Negative Prompts<a hidden class=anchor aria-hidden=true href=#negative-prompts>#</a></h3><p>In the same way that (positive) prompts help define your expected outcomes, negative prompt help to shape the generated image by providing SD with a list ob objects and types to avoid when it is consulting its inference model.</p><p>For example, you want to generate a portrait, but Stable Diffusion provided duplicates faces or bodies. A negative prompt including the phrase &ldquo;duplicates&rdquo; would provide a high-dimension anchor, which the process seek to stray away from.</p><h4 id=prompt-examples>Prompt Examples<a hidden class=anchor aria-hidden=true href=#prompt-examples>#</a></h4><ul><li><strong>Prompt:</strong> <code>a lonely bipedal robot walking through a valley. best quality. photograph. nuclear. cataclysmic. end of the world</code><ul><li><strong>Negative:</strong> <code>human. duplicates. hair</code></li></ul></li><li><strong>Prompt:</strong> <code>A hyper realistic avatar of a man riding on a black honda cbr 650r in leather suit,high detail, high quality,8K,photo realism</code><ul><li><strong>Negative:</strong> <code>flying mirrors,low quality</code></li></ul></li><li><strong>Prompt:</strong> <code>electronik robot and ofice ,unreal engine, cozy indoor lighting, artstation, detailed, digital painting,cinematic,character design by mark ryden and pixar and hayao miyazaki, unreal 5, daz, hyperrealistic, octane render</code><ul><li><strong>Negative:</strong> <code>ugly hands, disjointed</code></li></ul></li></ul><p>More fantastic prompt examples can be found here: <a href=https://stablediffusion.fr/prompts rel=noopener target=_blank>Prompt examples - Stable Diffusion</a>
.</p><p>You can skip ahead to see more results here in the <a href=#results-comparison>results comparison</a> section.</p><p>Provided that no generation configuration values are changed and a fixed seed is defined (including <a href=#hires-fix>Hires Fix</a> settings), you will always get the same output from the vision model. Each time to you generate an image, the webui shows your the parameters used; there for you could replicate any image using these parameters. Try this:</p><figure><img loading=lazy src=/images/stable-diffusion-controlnet-prompting.png alt="Using the SD output parameters to recreate identical images."><figcaption><p>Using the SD output parameters to recreate identical images.</p></figcaption></figure><p>One thing you must understand, if you increase the Batch Count, the seed value still increments (by 1, at default).</p><p>Better still, if you are using the <code>--api</code>, you could pass parameters in your API GET request. You can read more about the API here: <a href=https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/API>stable-diffusion-webui/wiki/API</a>. There you will find a link to this useful python script for getting started with requests over the API: <a href=https://gist.github.com/w-e-w/0f37c04c18e14e4ee1482df5c4eb9f53>txt2img and img2img example python script</a>.</p><h3 id=styles>Styles<a hidden class=anchor aria-hidden=true href=#styles>#</a></h3><p>Style are an amendment to a prompt and can have a offer a very strong level of control over the type of image that SD is going to produce for you. Ordinarily, an artist would have their style of work classified as a particular type of work. Therefore a model trained on a wide array of data should have an understanding of what each of these styles might look like and be able to make better predictions for the next generated pixel.</p><p>One of the best resources I have encountered is this style cheat sheet: <a href=https://supagruen.github.io/StableDiffusion-CheatSheet>supagruen.github.io/StableDiffusion-CheatSheet</a>. Simply pick a style you are interested in, copy the style prompt and append it to your existing prompt in stable-diffusion-webui. The sit splits out the styles with tags and categories to help you pick your perfect style. This has to be one of the most valuable AI image generation resources to date.</p><figure><img loading=lazy src=/images/stable-diffusion-1_5-cheat-sheet.png alt="Stable Diffusion 1.5 - Cheat Sheet."><figcaption><p>Stable Diffusion 1.5 - Cheat Sheet.</p></figcaption></figure><hr><h2 id=installing-mikubills-sd-webui-controlnet>Installing Mikubill&rsquo;s sd-webui-controlnet<a hidden class=anchor aria-hidden=true href=#installing-mikubills-sd-webui-controlnet>#</a></h2><p>Installing extensions in stable-diffusion-webui is ordinarily a very user friendly, low skill process. There are a large number of community produced plugins that enable a huge amount of features in SD.</p><p>So let&rsquo;s install ControlNet:</p><ol><li>Open &ldquo;Extensions&rdquo; tab.</li><li>Open &ldquo;Install from URL&rdquo; tab in the tab.</li><li>Enter <a href=https://github.com/Mikubill/sd-webui-controlnet.git>https://github.com/Mikubill/sd-webui-controlnet.git</a> to &ldquo;URL for extension&rsquo;s git repository&rdquo;.</li><li>Press &ldquo;Install&rdquo; button.</li><li>Wait until you see the message &ldquo;Installed into stable-diffusion-webui\extensions\sd-webui-controlnet&rdquo;.</li><li>Go to &ldquo;Installed&rdquo; tab, click &ldquo;Check for updates&rdquo;, and then click &ldquo;Apply and restart UI&rdquo;.</li><li>Completely restart A1111 webui - end the command-prompt/terminal window and launch again.</li></ol><figure><img loading=lazy src=/images/stable-diffusion-controlnet-installation.png alt="Installing Mikubill&rsquo;s sd-webui-controlnet"><figcaption><p>Installing Mikubill&rsquo;s sd-webui-controlnet</p></figcaption></figure><p>Should you need to forcefully remove this or any other extension, you can simply delete the extension named folder from the following path:</p><pre tabindex=0><code>~ \stable-diffusion-webui\extensions
</code></pre><hr><h2 id=controlnet-techniques>Controlnet Techniques<a hidden class=anchor aria-hidden=true href=#controlnet-techniques>#</a></h2><h3 id=canny>Canny<a hidden class=anchor aria-hidden=true href=#canny>#</a></h3><p>I have found that sometimes enabling ControlNet can harm the creativity of text2img generation, irrespective of your classifier free guidance (CFG) scale value. Controlnet can be tweaked to respect the prompt with a higher priority, but it can lead to some questionable outputs due to it&rsquo;s influence. My experimenting with Controlnet functionality is currently incomplete and I hope to cover my findings in a future article. It is best used with very loose control influence and with an image that closely resembles your desired outcome. This is why, it can be great for human faces, when paired with an appropriate model and style.</p><figure><img loading=lazy src=/images/AI-war-selfie.jpg alt="Using Canny to paint myself as a soldier in World War 2 and World War 3."><figcaption><p>Using Canny to paint myself as a soldier in World War 2 and World War 3.</p></figcaption></figure><h3 id=openpose>OpenPose<a hidden class=anchor aria-hidden=true href=#openpose>#</a></h3><p>This ContorlNet module aims to by add extra conditions to the neural network structure in order to control diffusion model outcomes; OpenPose does this with human pose estimation. Needless to say, that works best on humanoid subjects. The module can analyse an image to produce a wireframe for human poses and this is applied to conditions of generation.</p><p>Here is how we can configure openpose in the controlnet plug section of the SD-webui interface:</p><figure><img loading=lazy src=/images/stable-diffusion-controlnet-openpose-settings.png alt="ControlNet OpenPose using default values."><figcaption><p>ControlNet OpenPose using default values.</p></figcaption></figure><p>This results in the following wireframe pose detection - not a perfect capture due to a slightly ambiguous input image of a non-human entity:</p><figure><img loading=lazy src=/images/stable-diffusion-controlnet-openpose-wireframe.png alt="ControlNet OpenPose generating a pose structure based on an input image."><figcaption><p>ControlNet OpenPose generating a pose structure based on an input image.</p></figcaption></figure><p>Ultimately resulting in some very nice output images, which adhere to our prompt, provide creativity and strike the same pose. I believe that the slight issue in the pose is what caused SD to confuse the direction that the subject was facing in some images.</p><figure><img loading=lazy src=/images/stable-diffusion-controlnet-openpose-output.png alt="ControlNet OpenPose generating a batch of 4 images based on our wireframe and prompt."><figcaption><p>ControlNet OpenPose generating a batch of 4 images based on our wireframe and prompt.</p></figcaption></figure><hr><h2 id=image-resolution-and-performance>Image Resolution and Performance<a hidden class=anchor aria-hidden=true href=#image-resolution-and-performance>#</a></h2><p>You&rsquo;re available hardware is going to make a large difference in generation time. As a general rule of thumb, graphics cards with a high memory availability offer better performance during inference based task such as Stable Diffusion. Howwever, the Nvidia GPUs, particularly RTX GPUs which saw the introduction of Tensor cores, clearly have the computational advantage:</p><figure><img loading=lazy src=/images/tom-hardware-SD-benchmark.png alt="Tom&rsquo;s Hardware Stable Diffusion benchmark article"><figcaption><p><a href=https://www.tomshardware.com/pc-components/gpus/stable-diffusion-benchmarks>Tom&rsquo;s Hardware Stable Diffusion benchmark article</a></p></figcaption></figure><p>Whilst it is not entirely necessary to have an RTX graphics card, using one will significantly reduce your generation times. Nvidia is quite literally selling shovels in a gold rush&mldr;</p><p>Here are some performance optimization options that can be applied using command-prompt/terminal flags:</p><table><thead><tr><th style=text-align:left>Commandline argument</th><th style=text-align:left>Explanation</th></tr></thead><tbody><tr><td style=text-align:left><code>--opt-sdp-attention</code></td><td style=text-align:left>May results in faster speeds than using xFormers on some systems but requires more VRAM. (non-deterministic)</td></tr><tr><td style=text-align:left><code>--opt-sdp-no-mem-attention</code></td><td style=text-align:left>May results in faster speeds than using xFormers on some systems but requires more VRAM. (deterministic, slightly slower than <code>--opt-sdp-attention</code> and uses more VRAM)</td></tr><tr><td style=text-align:left><code>--xformers</code></td><td style=text-align:left>Use <a href=https://github.com/facebookresearch/xformers>xFormers</a> library. Great improvement to memory consumption and speed. Nvidia GPUs only. (<a href=https://github.com/facebookresearch/xformers/releases/tag/v0.0.19>deterministic as of 0.0.19</a> [webui uses 0.0.20 as of 1.4.0])</td></tr><tr><td style=text-align:left><code>--force-enable-xformers</code></td><td style=text-align:left>Enables xFormers regardless of whether the program thinks you can run it or not. Do not report bugs you get running this.</td></tr><tr><td style=text-align:left><code>--opt-split-attention</code></td><td style=text-align:left>Cross attention layer optimization significantly reducing memory use for almost no cost (some report improved performance with it). Black magic. On by default for <code>torch.cuda</code>, which includes both NVidia and AMD cards.</td></tr><tr><td style=text-align:left><code>--disable-opt-split-attention</code></td><td style=text-align:left>Disables the optimization above.</td></tr><tr><td style=text-align:left><code>--opt-sub-quad-attention</code></td><td style=text-align:left>Sub-quadratic attention, a memory efficient Cross Attention layer optimization that can significantly reduce required memory, sometimes at a slight performance cost. Recommended if getting poor performance or failed generations with a hardware/software configuration that xFormers doesn&rsquo;t work for. On macOS, this will also allow for generation of larger images.</td></tr><tr><td style=text-align:left><code>--opt-split-attention-v1</code></td><td style=text-align:left>Uses an older version of the optimization above that is not as memory hungry (it will use less VRAM, but will be more limiting in the maximum size of pictures you can make).</td></tr><tr><td style=text-align:left><code>--medvram</code></td><td style=text-align:left>Makes the Stable Diffusion model consume less VRAM by splitting it into three parts - cond (for transforming text into numerical representation), first_stage (for converting a picture into latent space and back), and unet (for actual denoising of latent space) and making it so that only one is in VRAM at all times, sending others to CPU RAM. Lowers performance, but only by a bit - except if live previews are enabled.</td></tr><tr><td style=text-align:left><code>--lowvram</code></td><td style=text-align:left>An even more thorough optimization of the above, splitting unet into many modules, and only one module is kept in VRAM. Devastating for performance.</td></tr><tr><td style=text-align:left><code>*do-not-batch-cond-uncond</code></td><td style=text-align:left>Only before 1.6.0: prevents batching of positive and negative prompts during sampling, which essentially lets you run at 0.5 batch size, saving a lot of memory. Decreases performance. Not a command line option, but an optimization implicitly enabled by using <code>--medvram</code> or <code>--lowvram</code>. In 1.6.0, this optimization is not enabled by any command line flags, and is instead enabled by default. It can be disabled in settings, <code>Batch cond/uncond</code> option in <code>Optimizations</code> category.</td></tr><tr><td style=text-align:left><code>--always-batch-cond-uncond</code></td><td style=text-align:left>Only before 1.6.0: disables the optimization above. Only makes sense together with <code>--medvram</code> or <code>--lowvram</code>. In 1.6.0, this command line flag does nothing.</td></tr><tr><td style=text-align:left><code>--opt-channelslast</code></td><td style=text-align:left>Changes torch memory type for stable diffusion to channels last. Effects not closely studied.</td></tr><tr><td style=text-align:left><code>--upcast-sampling</code></td><td style=text-align:left>For Nvidia and AMD cards normally forced to run with <code>--no-half</code>, <a href=https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/8782>should improve generation speed</a>.</td></tr></tbody></table><p>If unlimited power is what you seek and you are running SD on Windows, in Nvidia control panel, 3d parameters, change your power profile to &ldquo;maximum performance&rdquo;. You are now trading even more money for faster images.</p><h3 id=size-matters>Size Matters<a hidden class=anchor aria-hidden=true href=#size-matters>#</a></h3><p>Unfortunately, size does matter when it comes to performance&mldr; the larger the resolution, the more memory resources you are going to require. By default SD targets 512px x 512px. My best advice is to target a small resolution whilst you build an understanding of the models and controls needed to reach your ambitions with SD. This will allow you to quickly make small, iterative changes and have a faster feedback loop, whilst you work out which settings are best for your use case.</p><p>Unfortunately, your available memory resources may be the limit to the maximum output resolution; a work around is to use <a href=#upscaling>upscaling</a>.</p><h4 id=hires-fix>Hires Fix<a hidden class=anchor aria-hidden=true href=#hires-fix>#</a></h4><p>Uses a two step process to partially create an image at smaller resolution (such as 512px x 512px), upscale, and then improve details in it without changing composition. So let&rsquo;s say that we want to optimise our image output for max quality Instagram stories, we can set a width of 540px, a height of 960px and an upscale factor of 2; giving a 1080px by 1920px image.</p><p>One setting that we must consider is <strong>Denoising strength</strong> - this determines how little respect the algorithm should have for image&rsquo;s content. At 0, nothing will change, and at 1 you&rsquo;ll get an unrelated image. With values below 1.0, processing will take less steps than the Sampling Steps slider specifies. I have found that best results are when left at the default value of 0.7.</p><figure><img loading=lazy src=/images/stable-diffusion-hires-settings.png alt="AUTOMATIC1111/stable-diffusion-webui with Hires set to scale images by a factor of 2."><figcaption><p>AUTOMATIC1111/stable-diffusion-webui with Hires set to scale images by a factor of 2.</p></figcaption></figure><h4 id=upscaling>Upscaling<a hidden class=anchor aria-hidden=true href=#upscaling>#</a></h4><p>It is the year 2024 and if we are being truthful, 512px by 512px images look like an early Fallout game and 1024px by 1024px is also unacceptable in this age of retina displays. Fortunately, we can use the upscaling feature in SD-webui to grace our eyeballs with higher resolution images, after generation. Let&rsquo;s take a look at how to upscale in SD-webui:</p><figure><img loading=lazy src=/images/stable-diffusion-extras-upscaling.png alt="Upscaling with the Extras tab."><figcaption><p>Upscaling with the Extras tab.</p></figcaption></figure><p>The above figure takes out 1920px by 1080px image and upscales it to 3840px by 2160px. The main caveat to upscaling is that unlike Hires Fix, we are not adding detail when we expand the image because no sampling is taking place.</p><h3 id=power-consumption>Power Consumption<a hidden class=anchor aria-hidden=true href=#power-consumption>#</a></h3><p>A quick note on power consumption. Living in a western nation, power costs are not an insignificant expense and as we have learned, running local software like stable-diffusion-webui and LMStudio induces a significant load on a computer for an extended period of time. Here is some back of a napkin maths for an RTX 3060 (12GB) running at stock clock settings (Ryzen 5600, 32GB DDR4, 1x m.2 SSD):</p><pre tabindex=0><code>Consumption in watts during generation: 480
Time take in hours to generate 10 images at 1024 x 1024: 1.2
Electricity rate per kWh: £0.24

£0.14 per 1.2 hours
~£0.01 per image
</code></pre><p>This might not seem like much but I have spent somewhere in the region of 48 hours running generation just in the processing of learning and developing this article. That&rsquo;s <strong>£5.64</strong> at current rates&mldr; so consider <a href=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js>buying me a coffee</a>?</p><hr><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>So if you haven&rsquo;t already gathered, there are a lot of variables that we can modify to guide the image generation process and produce a result that we are aiming for. Let&rsquo;s take a look at what sand can do when we teach it to think&mldr; here are some of my favourite images so far:</p><figure><img loading=lazy src=/images/stable-diffusion-april-robot-01.png alt="Lonely robots walking away into a cataclysmic valley part 1."><figcaption><p>Lonely robots walking away into a cataclysmic valley part 1.</p></figcaption></figure><figure><img loading=lazy src=/images/stable-diffusion-april-robot-02.png alt="Lonely robots walking away into a cataclysmic valley part 2."><figcaption><p>Lonely robots walking away into a cataclysmic valley part 2.</p></figcaption></figure><figure><img loading=lazy src=/images/stable-diffusion-april-robot-03.png alt="Lonely robots walking away into a cataclysmic valley part 3."><figcaption><p>Lonely robots walking away into a cataclysmic valley part 3.</p></figcaption></figure><figure><img loading=lazy src=/images/stable-diffusion-april-robot-04.png alt="Lonely robots walking away into a cataclysmic valley part 4."><figcaption><p>Lonely robots walking away into a cataclysmic valley part 4.</p></figcaption></figure><figure><img loading=lazy src=/images/stable-diffusion-april-robot-05.png alt="Lonely robots walking away into a cataclysmic valley part 5."><figcaption><p>Lonely robots walking away into a cataclysmic valley part 5.</p></figcaption></figure><figure><img loading=lazy src=/images/stable-diffusion-april-robot-06.png alt="Lonely robots walking away into a cataclysmic valley part 6."><figcaption><p>Lonely robots walking away into a cataclysmic valley part 6.</p></figcaption></figure><figure><img loading=lazy src=/images/stable-diffusion-april-robot-07.png alt="Lonely robots walking away into a cataclysmic valley part 7."><figcaption><p>Lonely robots walking away into a cataclysmic valley part 7.</p></figcaption></figure><figure><img loading=lazy src=/images/stable-diffusion-april-robot-08.png alt="Lonely robots walking away into a cataclysmic valley part 8."><figcaption><p>Lonely robots walking away into a cataclysmic valley part 8.</p></figcaption></figure><p>If you found this article useful, consider sharing it in your social circles or bookmarking this page as I have lots more stable-diffusion content in the works&mldr;</p><figure><img loading=lazy src=/images/stable-diffusion-april-robot-gif.gif></figure></div><footer class=post-footer><ul class=post-tags><li><a href=https://exitcode0.net/tags/stable-diffusion/>Stable-Diffusion</a></li><li><a href=https://exitcode0.net/tags/ai/>AI</a></li><li><a href=https://exitcode0.net/tags/art/>Art</a></li></ul><nav class=paginav><a class=prev href=https://exitcode0.net/posts/testing-llama3-with-lmstudio/><span class=title>« Prev</span><br><span>Testing Llama3 With LM Studio</span>
</a><a class=next href=https://exitcode0.net/posts/homeassistant-tls-with-tailscale-traefik/><span class=title>Next »</span><br><span>Home Assistant HTTPS Certificates with Tailscale, Traefik and CoreDNS</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Generating Artwork With Stable Diffusion - April 2024 Edition on x" href="https://x.com/intent/tweet/?text=Generating%20Artwork%20With%20Stable%20Diffusion%20-%20April%202024%20Edition&amp;url=https%3a%2f%2fexitcode0.net%2fposts%2fgenerating-artwork-with-stable-diffusion-april-2024%2f&amp;hashtags=stable-diffusion%2cAI%2cart"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Generating Artwork With Stable Diffusion - April 2024 Edition on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fexitcode0.net%2fposts%2fgenerating-artwork-with-stable-diffusion-april-2024%2f&amp;title=Generating%20Artwork%20With%20Stable%20Diffusion%20-%20April%202024%20Edition&amp;summary=Generating%20Artwork%20With%20Stable%20Diffusion%20-%20April%202024%20Edition&amp;source=https%3a%2f%2fexitcode0.net%2fposts%2fgenerating-artwork-with-stable-diffusion-april-2024%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Generating Artwork With Stable Diffusion - April 2024 Edition on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fexitcode0.net%2fposts%2fgenerating-artwork-with-stable-diffusion-april-2024%2f&title=Generating%20Artwork%20With%20Stable%20Diffusion%20-%20April%202024%20Edition"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Generating Artwork With Stable Diffusion - April 2024 Edition on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fexitcode0.net%2fposts%2fgenerating-artwork-with-stable-diffusion-april-2024%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Generating Artwork With Stable Diffusion - April 2024 Edition on whatsapp" href="https://api.whatsapp.com/send?text=Generating%20Artwork%20With%20Stable%20Diffusion%20-%20April%202024%20Edition%20-%20https%3a%2f%2fexitcode0.net%2fposts%2fgenerating-artwork-with-stable-diffusion-april-2024%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Generating Artwork With Stable Diffusion - April 2024 Edition on telegram" href="https://telegram.me/share/url?text=Generating%20Artwork%20With%20Stable%20Diffusion%20-%20April%202024%20Edition&amp;url=https%3a%2f%2fexitcode0.net%2fposts%2fgenerating-artwork-with-stable-diffusion-april-2024%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Generating Artwork With Stable Diffusion - April 2024 Edition on ycombinator" href="https://news.ycombinator.com/submitlink?t=Generating%20Artwork%20With%20Stable%20Diffusion%20-%20April%202024%20Edition&u=https%3a%2f%2fexitcode0.net%2fposts%2fgenerating-artwork-with-stable-diffusion-april-2024%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://exitcode0.net/>ExitCode0</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>