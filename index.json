[{"content":"Introduction to Running PostgreSQL in a Docker Container using Docker Compose In this blog post, we will explore how to run PostgreSQL in a Docker container using Docker Compose. We will also break down and explain the init-user-db.sh script that is executed at startup to initialize the PostgreSQL tables. Running PostgreSQL in a Docker container provides several benefits, including ease of deployment, portability, and isolation. So let\u0026rsquo;s dive in and understand the process!\nThis post uses Docker Compose V2, if you are still using V1, consider upgrading otherwise use docker-compose instead of docker compose.\nWhy Run PostgreSQL in a Docker Container? Running PostgreSQL in a Docker container offers numerous advantages. Here are a few key benefits:\nEasy Deployment: Docker simplifies the deployment process by encapsulating PostgreSQL and its dependencies into a container, making it easy to set up and manage.\nPortability: Docker containers are self-contained and can be run on any system that supports Docker, ensuring consistent behavior across different environments.\nIsolation: Running PostgreSQL in a container provides isolation from the host system, preventing potential conflicts with existing installations or dependencies.\nSetting Up PostgreSQL in a Docker Container with Docker Compose Before we start, ensure that you have Docker and Docker Compose installed on your system. Once both are set up, you can proceed with the following steps:\nCreate a Docker Compose file: Open a text editor and create a file called docker-compose.yml. Copy and paste the following contents into the file: version: \u0026#39;3.3\u0026#39; services: postgres: container_name: my-postgres image: postgres:latest restart: always environment: POSTGRES_USER: ${POSTGRES_USER} POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} POSTGRES_DB: ${POSTGRES_DB} PGDATA: /var/lib/postgresql/data/pgdata ports: - \u0026#34;5432:5432\u0026#34; volumes: - ./data:/var/lib/postgresql/data/pgdata - ./init-user-db.sh:/docker-entrypoint-initdb.d/init-user-db.sh In this Docker Compose configuration:\nThe postgres service is defined with the specified container name (my-postgres), the latest PostgreSQL image (postgres:latest), and the restart policy set to always to ensure that the container restarts automatically if it stops.\nThe environment section sets the PostgreSQL environment variables (POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB) using the provided values from the environment. In this example I have a .env file in the same directory as the docker-compose.yml file, which means that there is no need for sensitive data such as passwords to be stored in our infrastructure code. This .env file could be replaced by git secrets in part of a pipeline deployment.\nThe ports section maps the host machine\u0026rsquo;s port 5432 to the container\u0026rsquo;s port 5432, allowing you to access the PostgreSQL database from your host machine.\nThe volumes section mounts two directories:\n./data:/var/lib/postgresql/data/pgdata: This maps the ./data directory on your host machine to the /var/lib/postgresql/data/pgdata directory inside the container. It allows you to persist the PostgreSQL data files across container restarts. ./init-user-db.sh:/docker-entrypoint-initdb.d/init-user-db.sh: This mounts the init-user-db.sh script from your host machine to the /docker-entrypoint-initdb.d/init-user-db.sh path inside the container. This script will be executed during container startup to initialize the PostgreSQL tables. Start the PostgreSQL container: Open your terminal or command prompt, navigate to the directory containing the docker-compose.yml file, and run the following command: docker compose up -d The -d flag runs the containers in detached mode, allowing\nthem to run in the background.\nBreaking Down the init-user-db.sh Script The init-user-db.sh script is executed at startup to initialize the PostgreSQL tables. Let\u0026rsquo;s examine the script and understand its purpose:\n#!/bin/bash set -e set -u export PGDATABASE=${POSTGRES_DB} export PGUSER=${POSTGRES_USER} export PGPASSWORD=${POSTGRES_PASSWORD} RUN_PSQL=\u0026#34;psql -X --set AUTOCOMMIT=on --set ON_ERROR_STOP=on \u0026#34; ${RUN_PSQL} \u0026lt;\u0026lt;SQL CREATE TABLE public.mytable ( id uuid NOT NULL, sensor_name text NOT NULL, battery double precision, humidity double precision, link_quality double precision, temperature double precision, voltage integer, \u0026#34;time\u0026#34; timestamp with time zone NOT NULL ) TABLESPACE pg_default; ALTER TABLE IF EXISTS public.mytable OWNER to dbadm; SQL Here\u0026rsquo;s a breakdown of the script:\nThe set -e and set -u commands ensure that the script exits immediately if any command fails or encounters an unset variable.\nThe export statements set environment variables to configure the PostgreSQL connection. The variables PGDATABASE, PGUSER, and PGPASSWORD are assigned values based on the corresponding Docker environment variables (derived from the .env file).\nThe RUN_PSQL variable defines the psql command with specific options. -X disables transaction management, --set AUTOCOMMIT=on ensures that each command is executed in its own transaction, and --set ON_ERROR_STOP=on stops the script execution if any error occurs.\nThe ${RUN_PSQL} \u0026lt;\u0026lt;SQL syntax starts a here-document that allows us to provide SQL commands inline.\nThe SQL commands enclosed within the SQL delimiter create a table named mytable in the public schema with the specified columns and data types. The ALTER TABLE statement sets the owner of the table to dbadm if the table already exists.\nWith this updated Docker Compose configuration, you can easily manage and deploy your PostgreSQL container along with the initialization script. Running docker compose up -d will start the containers and execute the init-user-db.sh script to initialize the PostgreSQL tables.\nConclusion In this blog post, we have learned how to run PostgreSQL in a Docker container using Docker Compose. We have seen the advantages of running PostgreSQL in a container and how to set it up with the docker-compose.yml file. Additionally, we have explored the init-user-db.sh script and its role in initializing the PostgreSQL tables.\nBy leveraging Docker and Docker Compose, you can easily deploy and manage PostgreSQL in a portable and isolated environment. This approach brings flexibility and scalability to your PostgreSQL deployments, making it an ideal choice for various applications.\nHopefully this blog post has been helpful in understanding the process of running PostgreSQL in a Docker container using Docker Compose. Stay tuned for more exciting tutorials and guides. Consider buying me a coffee to feed the addiction and help me solve more automation headaches!\n","permalink":"https://exitcode0.net/posts/automating-postgres-deployment/","summary":"Introduction to Running PostgreSQL in a Docker Container using Docker Compose In this blog post, we will explore how to run PostgreSQL in a Docker container using Docker Compose. We will also break down and explain the init-user-db.sh script that is executed at startup to initialize the PostgreSQL tables. Running PostgreSQL in a Docker container provides several benefits, including ease of deployment, portability, and isolation. So let\u0026rsquo;s dive in and understand the process!","title":"Automating Postgres Deployment with Docker Compose and Init Scripts"},{"content":"In previous posts I have been using Docker Compose to deploy the constituent components of a fully local Home Assistant voice assistant. In this blog post, we will guide you through setting up Wyoming Piper using Docker Compose. Piper is a fast, local neural text to speech system originally optimised for the Raspberry Pi 4. It supports many languages, and voice samples: https://rhasspy.github.io/piper-samples.\nWyoming Piper is a speech recognition and natural language understanding system that can be used for voice control in various applications. It uses the Rhasspy framework and provides support for different languages and voices.\nPrerequisites Before you begin, make sure you have Docker and Docker Compose installed on your system. You can find installation instructions for your operating system on the Docker website and Docker Compose website.\nDocker Compose File Create a new file called docker-compose.yml and open it in a text editor. Copy the following content into the file:\nversion: \u0026#34;3\u0026#34; services: wyoming-piper: image: rhasspy/wyoming-piper ports: - \u0026#34;10200:10200\u0026#34; volumes: - \u0026#34;./piper-data:/data\u0026#34; command: [ \u0026#34;--voice\u0026#34;, \u0026#34;en-gb-southern_english_female-low\u0026#34; ] restart: unless-stopped Let\u0026rsquo;s go through the different sections of this Docker Compose file.\nVersion The version section specifies the version of the Docker Compose file format. In this case, we\u0026rsquo;re using version \u0026ldquo;3\u0026rdquo;.\nServices The services section defines the services that make up your application. In our case, we have a single service called wyoming-piper.\nWyoming Piper Service Under the wyoming-piper service, we have the following configurations:\nimage: Specifies the Docker image to use for the service. In this case, we\u0026rsquo;re using the rhasspy/wyoming-piper image. ports: Maps the container\u0026rsquo;s port 10200 to the host\u0026rsquo;s port 10200. This allows us to access Wyoming Piper\u0026rsquo;s web interface from our local machine. volumes: Mounts the ./piper-data directory on the host to the /data directory inside the container. This is used to persist Wyoming Piper\u0026rsquo;s data. command: Specifies the command-line arguments to pass to the container. In this example, we\u0026rsquo;re using the English (GB) Southern English Female (Low) voice sample. restart: Sets the restart policy for the container. In this case, the container will be automatically restarted unless explicitly stopped. Starting Wyoming Piper To start Wyoming Piper, open a terminal or command prompt, navigate to the directory where you saved the docker-compose.yml file, and run the following command:\ndocker-compose up -d The -d flag runs the containers in the background (detached mode).\nWait for Docker Compose to download the necessary Docker images and start the Wyoming Piper container. You can check the progress in the terminal output.\nOnce the container is up and running, you can access Wyoming Piper\u0026rsquo;s web interface by opening a web browser and navigating to http://localhost:10200.\nConclusion In this blog post, we\u0026rsquo;ve walked you through setting up Wyoming Piper using Docker Compose. Docker Compose allows you to manage the different components of Wyoming Piper in a unified and reproducible way. You can customize the configurations in the docker-compose.yml\nAt this point we have all the components needed for a fully local voice assistant stack, deployed with docker compose. It is now possible to follow the remainder of the Home Assistant docuemntation in configuring your assistant: https://www.home-assistant.io/docs/assist/voice_remote_local_assistant.\nRelevant and supporting posts:\nHow to Use a Docker Compose File for Wyoming Whisper Homeassistant Enable MagicDNS and HTTPS Certificates in Tailscale ","permalink":"https://exitcode0.net/posts/wyoming-piper-docker-compose/","summary":"In previous posts I have been using Docker Compose to deploy the constituent components of a fully local Home Assistant voice assistant. In this blog post, we will guide you through setting up Wyoming Piper using Docker Compose. Piper is a fast, local neural text to speech system originally optimised for the Raspberry Pi 4. It supports many languages, and voice samples: https://rhasspy.github.io/piper-samples.\nWyoming Piper is a speech recognition and natural language understanding system that can be used for voice control in various applications.","title":"Setting up Wyoming Piper with Docker Compose"},{"content":"In this blog post, we will go over how to use a Docker Compose file to deploy and configure Wyoming Whisper. Wyoming Whisper is an open-source, lightweight voice assistant designed to run on a Raspberry Pi or other low-powered device. The impetus for this compose defined container is to intergate with a Home Assistant 2023.5 container and ultimate have a fully local voice assistant. Whisper will provide our speech-to-text service and the Wyoming protocol is how it will be integrated with Home Assistant.\nNOTE: For your reference, I am using Home Assistant in a container which is why I have not simply setup the wyoming wishper addons.\nPrerequisites Before we get started, make sure you have Docker and Docker Compose installed on your system. If you need help with installation, please refer to the official Docker documentation.\nStep 1: Create a Docker Compose File Create a file called docker-compose.yml in a directory of your choosing and add the following content to it:\nversion: \u0026#39;3\u0026#39; services: wyoming-whisper: image: rhasspy/wyoming-whisper ports: - \u0026#34;10300:10300\u0026#34; volumes: - ./whisper-data:/data command: [ \u0026#34;--model\u0026#34;, \u0026#34;medium-int8\u0026#34;, \u0026#34;--language\u0026#34;, \u0026#34;en\u0026#34; ] restart: unless-stopped This file defines a single service named wyoming-whisper. Let\u0026rsquo;s go over what each line does:\nversion: '3': specifies the version of Docker Compose file format being used. image: rhasspy/wyoming-whisper: specifies the Docker image to use for the service. In this case, we are using the rhasspy/wyoming-whisper image. ports: - \u0026quot;10300:10300\u0026quot;: specifies the port mapping between the container and host. The 10300:10300 mapping means that the container\u0026rsquo;s port 10300 is mapped to the host\u0026rsquo;s port 10300. volumes: - ./whisper-data:/data: specifies the volume mapping between the container and host. The ./whisper-data:/data mapping means that the ./whisper-data directory on the host is mapped to the /data directory in the container. command: [ \u0026quot;--model\u0026quot;, \u0026quot;medium-int8\u0026quot;, \u0026quot;--language\u0026quot;, \u0026quot;en\u0026quot; ]: specifies the command to run when the container starts. In this case, we are running Wyoming Whisper with the medium-int8 model and English language. restart: unless-stopped: specifies the restart policy for the container. In this case, the container will automatically restart unless it is explicitly stopped by the user. Whispher models You might be trying to run this container on a lightweight device such as a Raspberry Pi, in which case it would be wise to sellect a lightweight model for improved performance. Here are the model options, sorted from least to most accurate (fastest to slowest):\ntiny-int8 (43 MB) tiny (152 MB) base-int8 (80 MB) base (291 MB) small-int8 (255 MB) small (968 MB) medium-int8 (786 MB) medium (3.1 GB) The model file is downloaded on first run of the container.\nStep 2: Start the Container To start the container, navigate to the directory where your docker-compose.yml file is located and run the following command:\ndocker-compose up -d The -d flag runs the container in the background.\nStep 3: Integrate Wyoming Whisper with Home Assistant Now that the server is up and running we can add a new integration in our Home Assistant settings. This can be done under: Settings \u0026gt; Devices \u0026amp; Services \u0026gt; Add Integration.\nConclusion That\u0026rsquo;s it! You now know how to use a Docker Compose file to deploy and configure Wyoming Whisper. Docker Compose is a powerful tool that simplifies the deployment and management of complex applications. If you are looking to prepare your HA install for a fully local voice assistant and you need to setup HTTPS, consider reading the previous post - linked below - on how this can be done with Tailscale MagicDNS and HTTPS certificates.\n","permalink":"https://exitcode0.net/posts/wyoming-whisper-docker-compose/","summary":"In this blog post, we will go over how to use a Docker Compose file to deploy and configure Wyoming Whisper. Wyoming Whisper is an open-source, lightweight voice assistant designed to run on a Raspberry Pi or other low-powered device. The impetus for this compose defined container is to intergate with a Home Assistant 2023.5 container and ultimate have a fully local voice assistant. Whisper will provide our speech-to-text service and the Wyoming protocol is how it will be integrated with Home Assistant.","title":"How to Use a Docker Compose File for Wyoming Whisper"},{"content":"Tailscale is a virtual private network (VPN) service that allows secure remote access to resources across different networks. It offers a feature called MagicDNS that enables you to access your resources using a domain name instead of an IP address. Additionally, you can use HTTPS certificates to encrypt traffic between clients and servers, ensuring secure communication. In this tutorial, we\u0026rsquo;ll explain how to enable MagicDNS and HTTPS certificates in Tailscale and how to add a TLS certificate to Home Assistant using Tailscale. The most practical benifit for using Home Assistant within a Tailscale network is that it removes the requirement for network port forwarding and exposing services publicly, whilst still allowing your devices to access it from outside your local network. This blog post aims to give you the starting stes required to setup MagicDNS and HTTPS certificates in Tailscale, create a certificate on your home server and install that certificate in a Home Assistant docker container.\nWHY?! Why do I want a valid TLS certificate and a HTTPS connection to Home Assistant without a browser warning? Per the 2023.5 relase (https://www.home-assistant.io/blog/2023/05/03/release-20235/) voice control is all the rage using local voice assistants; in order to use the microphone in a modern browser connected to you Home Assistant dashboard, you need to have a valid HTTPS connection!\nEnabling MagicDNS Sign in to your Tailscale admin console and select your network. Click on \u0026ldquo;Nodes\u0026rdquo; and select the node that you want to enable MagicDNS for (You might only have one default node like me). Under \u0026ldquo;DNS\u0026rdquo; click \u0026ldquo;Enable MagicDNS.\u0026rdquo; Once you\u0026rsquo;ve enabled MagicDNS, you can access your devices using the tailnet domain allocated to your network. For example, mypc.exampletail.ts.net, where mypc is the device name.\nEnabling HTTPS Certificates Navigate to the DNS page of the admin console. Under HTTPS Certificates, click Enable HTTPS. Acknowledge that your machine names and your tailnet name will be published on a public ledger. Once you\u0026rsquo;ve enabled HTTPS certificates, you can generate the certificate and private key by running the following command on each machine (linux):\nsudo tailscale cert xxxx.xxxx.ts.net Replace xxxx.xxxx.ts.net with your tailnet domain, found on the DNS page of the admin console.\nThis will download two files: xxxx.xxxx.ts.net.crt and xxxx.xxxx.ts.net.key.\nAdding the TLS Certificate to Home Assistant To add the TLS certificate to Home Assistant, you need to convert the private key to PEM format and copy the certificate files to the Home Assistant container\u0026rsquo;s SSL directory. The following steps are based on a docker compose installation of HA, assuming the .crt and .key files created above are stored in /home/user/ (your file path will likely differ).\nConvert the private key to PEM format using the following command: sudo openssl pkcs8 -topk8 -nocrypt -in /home/user/xxxx.xxxx.ts.net.key -out /home/user/private.pem Replace /home/user/xxxx.xxxx.ts.net.key with the path to the downloaded key file.\nMake a directory for the TLS certificate files in the Home Assistant container\u0026rsquo;s file structure: mkdir /home/user/docker/homeassistant/ssl Add a volume mapping for the newly created ssl folder to your docker-compose.yml file, this will be where Home Assistant stores and locates the TLS certificate files: volumes: - /home/user/docker/homeassistant/ssl:/ssl You might alreaady have other volumes like I do for config and data; we are just adding another in this step.\nCopy the certificate files to the TLS directory: sudo cp /home/user/xxxx.xxxx.ts.net.crt /home/user/docker/homeassistant/ssl/fullchain.pem sudo cp /home/user/private.pem /home/user/docker/homeassistant/ssl/privkey.pem Replace /home/user/xxxx.xxxx.ts.net.crt with the path to the downloaded certificate file.\nUpdate the Home Assistant configuration.yml file to include the SSL certificate and key paths: http: ssl_certificate: /ssl/fullchain.pem ssl_key: /ssl/private.pem Save the file and restart the Home Assistant container:\ndocker compose up -d Now, you should be able to access Home Assistant securely using the MagicDNS domain name: https://mypc.example.ts.net:8123/.\nNOTE: You will need to be accessing this URL from a Tailscale connected device which is configured to use Tailscale DNS!\n","permalink":"https://exitcode0.net/posts/homeassistant-tls-with-tailscale/","summary":"Tailscale is a virtual private network (VPN) service that allows secure remote access to resources across different networks. It offers a feature called MagicDNS that enables you to access your resources using a domain name instead of an IP address. Additionally, you can use HTTPS certificates to encrypt traffic between clients and servers, ensuring secure communication. In this tutorial, we\u0026rsquo;ll explain how to enable MagicDNS and HTTPS certificates in Tailscale and how to add a TLS certificate to Home Assistant using Tailscale.","title":"Homeassistant Enable MagicDNS and HTTPS Certificates in Tailscale"},{"content":"ESXi is a powerful hypervisor that allows for the virtualization of multiple operating systems on a single physical machine. However, there are times when you may need to pass through a physical device, such as an NVMe drive, to a virtual machine. In this blog post, we will explore the steps required to pass through an NVMe drive to an ESXi virtual machine.\nPrerequisites Before proceeding, ensure that you have the following:\nroot access to an ESXi server An NVMe drive that you want to pass through to a virtual machine Knowledge of the datastore and existing virtual machine name that you want to use Step 1: Enable SSH on your ESXi Server The first step is to enable SSH on your ESXi server. This can be done by logging into the ESXi web client and navigating to \u0026ldquo;Manage\u0026rdquo; \u0026gt; \u0026ldquo;Settings\u0026rdquo; \u0026gt; \u0026ldquo;Security \u0026amp; Users\u0026rdquo; \u0026gt; \u0026ldquo;Services.\u0026rdquo; From here, you can enable SSH and start the SSH service.\nStep 2: SSH into Your ESXi Server Once SSH is enabled, you can SSH into your ESXi server by opening a terminal and running the following command:\nssh root@192.168.1.10 Replace 192.168.1.10 with the IP address of your ESXi server.\nStep 3: Create a Storage Directory Next, create a directory where you can store the virtual disk file. This can be done with the following command:\nmkdir /vmfs/volumes/datastore_name/existing_vm_name/storage Replace datastore_name with the name of the datastore that you want to use and existing_vm_name with the name of the existing virtual machine that you want to use.\nStep 4: List Available Disks List the available disks on your ESXi server with the following command:\nls -l /vmfs/devices/disks Make a note of the name of the NVMe disk that you want to pass through to the virtual machine. It should begin with t10.NVMe.\nStep 5: Create the Virtual Disk File Create the virtual disk file with the following command:\nvmkfstools -z /vmfs/devices/disks/t10.NVMe_____YOUR_DISK_NAME_____ \u0026#34;/vmfs/volumes/datastore_name/existing_vm_name/storage/disk_RDM_1.vmdk\u0026#34; Replace YOUR_DISK_NAME with the name of the NVMe disk that you want to pass through (found in step 4), datastore_name with the name of the datastore that you want to use, and existing_vm_name with the name of the existing virtual machine that you want to use.\nStep 6: Edit the Existing VM Settings Back in the ESXi web interface, edit the settings of the existing virtual machine that you want to pass the NVMe drive through to. Navigate to Edit Settings, click Add Other Device and select NVMe Controller.\nStep 7: Add the Virtual Hard Disk Add the virtual hard disk to the virtual machine by selecting \u0026ldquo;Existing Hard Disk\u0026rdquo; and browsing to the virtual disk file that you created in Step 5. Following along with our example, the file would be found at datastore_name/existing_vm_name/storage/disk_RDM_1.vmdk in the Datastore browser.\nStep 8: Configure the Virtual Disk Expand the hard disk options of the newly added disk and configure it with the following settings:\nSet Controller Location to: NVMe Controller 0, NVMe (0:0). Set Disk Compatibility to: Virtual Set Disk Mode to: Independent - Persistent Set Disk Compatibility Mode back to: Physical Cleanup and Conclusion Now you can save and close the VM settings, before powering on an letting the VM boot. In my case, I passed through my old laptop NVMe disk so that I could access and backup some old file. Lastly, don\u0026rsquo;t forget to resecure your ESXi server, by disabling SSH.\n","permalink":"https://exitcode0.net/posts/esxi-nvme-disk-passthrough/","summary":"ESXi is a powerful hypervisor that allows for the virtualization of multiple operating systems on a single physical machine. However, there are times when you may need to pass through a physical device, such as an NVMe drive, to a virtual machine. In this blog post, we will explore the steps required to pass through an NVMe drive to an ESXi virtual machine.\nPrerequisites Before proceeding, ensure that you have the following:","title":"Passing Through an NVMe Drive to an ESXi Virtual Machine"},{"content":"Introduction Uptime-Kuma is an open-source, self-hosted website monitoring tool. It can check the uptime and response time of websites, APIs, and other services at regular intervals and alert you if any of them go down. Uptime-Kuma can be installed on your own server and customized to suit your needs. In this article, we will explore how to set up and run Uptime-Kuma using Docker Compose.\nThe Docker Compose File The following is the Docker Compose file for the Uptime-Kuma service:\nversion: \u0026#39;3.3\u0026#39; services: uptime-kuma: image: louislam/uptime-kuma:latest container_name: uptime-kuma volumes: - ./data:/app/data ports: - 3001:3001 restart: always This Docker Compose file defines a single service named \u0026ldquo;uptime-kuma\u0026rdquo;. The service is built from the \u0026ldquo;louislam/uptime-kuma\u0026rdquo; Docker image, which is the latest version available. The container name is set to \u0026ldquo;uptime-kuma\u0026rdquo; as well.\nThe \u0026ldquo;volumes\u0026rdquo; section maps the \u0026ldquo;/app/data\u0026rdquo; directory inside the container to the \u0026ldquo;./data\u0026rdquo; directory on the host machine. This allows us to persist data across container restarts. Uptime-Kuma stores all its configuration and monitoring data in this directory, so it\u0026rsquo;s important to keep it intact.\nThe \u0026ldquo;ports\u0026rdquo; section maps port 3001 on the host machine to port 3001 in the container. This allows us to access the Uptime-Kuma web interface from our local machine. By default, Uptime-Kuma runs on port 3001, but you can change it if necessary.\nFinally, the \u0026ldquo;restart\u0026rdquo; section ensures that the container will always be restarted if it exits. This is useful for ensuring that the Uptime-Kuma service is always available, especially if it crashes or is shut down unexpectedly.\nRunning Uptime-Kuma with Docker Compose To run Uptime-Kuma with Docker Compose, first, make sure you have Docker and Docker Compose installed on your machine. Then, create a new directory for your Uptime-Kuma project and save the above Docker Compose file as \u0026ldquo;docker-compose.yml\u0026rdquo; in that directory.\nNext, run the following command from the same directory:\ndocker-compose up -d This command will start the Uptime-Kuma container in the background and detach from it. You can then access the Uptime-Kuma web interface by going to http://localhost:3001 in your web browser.\nConclusion In this article, we explored how to set up and run the Uptime-Kuma website monitoring tool using Docker Compose. We looked at the different sections of the Docker Compose file and explained how they work together to create a functional Uptime-Kuma service. By running Uptime-Kuma with Docker Compose, you can easily deploy and manage the tool on your own server, without having to worry about dependencies or configuration.\n","permalink":"https://exitcode0.net/posts/running-uptime-kuma-with-docker-compose/","summary":"Introduction Uptime-Kuma is an open-source, self-hosted website monitoring tool. It can check the uptime and response time of websites, APIs, and other services at regular intervals and alert you if any of them go down. Uptime-Kuma can be installed on your own server and customized to suit your needs. In this article, we will explore how to set up and run Uptime-Kuma using Docker Compose.\nThe Docker Compose File The following is the Docker Compose file for the Uptime-Kuma service:","title":"Running Uptime Kuma With Docker Compose"},{"content":"A new beinging for this blog\u0026hellip; The ExitCode0 blog is currently migrating. Welcome to the site, please hang tight for more content, coming soon. This site should now be in hosted on GitHub pages!\n","permalink":"https://exitcode0.net/posts/welcome/","summary":"A new beinging for this blog\u0026hellip; The ExitCode0 blog is currently migrating. Welcome to the site, please hang tight for more content, coming soon. This site should now be in hosted on GitHub pages!","title":"Welcome [Back]"},{"content":"Windows 11 became available on the 5th October 2021 and for those people with compatible machines, this guide will help you jump the queue and force the Windows 11 upgrade.\nBefore you start…\nTake a backup all all important data onto an external storage device/location. set aside at least 1 hour for the upgrade – maybe longer, depending on your internet connection speed. Make sure that your Windows 10 installation is up to date with no pending updates. Compatibility You might have seen a number of articles and news stories floating around about Windows 11 dropping support for a large number of computers, particularly older systems. The quickest way to test your system’s compatibility with Windows 11 is using the Microsoft WindowsPCHealthCheckSetup tool: https://aka.ms/GetPCHealthCheckApp\nWindows Health Check Tool Once you have the application installed and running, press the ‘Check now‘ button to test Windows 11 compatibility.\nDownloading the Windows 11 installer To get started with the upgrade, first download the installer:\nhttps://www.microsoft.com/en-us/software-download/windows11 Once the installer is running, it will start to download Windows 11 and begin the in-place upgrade. This will not perform a clean install of Windows and retain most of your settings and keep all of your files and applications.\nOnce the installer reaches 100% on stage 3/3, your computer will ask to restart or automatically restart after a 15 minutes countdown. This reboot might take longer than usual as the upgrade is applied. On completion, expect to see a Windows 11 welcome screen.\nAnd finally, your system will now be running Windows 11!\n","permalink":"https://exitcode0.net/posts/how-to-force-the-windows-11-upgrade/","summary":"Windows 11 became available on the 5th October 2021 and for those people with compatible machines, this guide will help you jump the queue and force the Windows 11 upgrade.\nBefore you start…\nTake a backup all all important data onto an external storage device/location. set aside at least 1 hour for the upgrade – maybe longer, depending on your internet connection speed. Make sure that your Windows 10 installation is up to date with no pending updates.","title":"How to force the Windows 11 upgrade"},{"content":" The HS100/HS110 LED can be bright and obnoxious. If you are looking to turn off the LED status lights on the TPLink Kasa smart plugs, then look no further – the solution resides in this post. It does involve some Python interaction, but I promise that it is a gentle passing with the programming language.\nThis solution works for:\nHS100 HS103 HS105 HS110 The only real drawback is that when the LED is disabled, you can not see when the device might have become disconnected from the wireless network – this is normally indicated by a RED or orange LED status.\nThe problem: How do you turn off the LED Indicator in a TPLink HS105 Smart plug? from HomeNetworking\nThe reddit solution: put some tape over it\nThe Reddit users go on to explain that the tape solution was working rather well. I am sure that using tape is quite effective, but if you don’t want the light emitted by the LED, then you might as well not have the energy consumption either.\nThe real solution: Turn off Kasa smartplug LED lights (HS100/HS110) https://github.com/Tombo1001/Kasa-Dark-Mode\n0 forks.\n1 stars.\n0 open issues.\nRecent commits: * bug fixdevice discovery notice bug fix, Tom Cocking\nUpdate README.mdimproved readme, Tom Cocking Initial commitv1.0 code, requirements and a basic readme, Tom Cocking Initial commit, Tom Cocking The project’s README file has a full set of instructions on setup and how to use the python script. The code has two usage modes:\nOne for all – apply LED status on or off for all discovered smart plugs: \u0026gt; python kasa-dark-mode.py -d Plug Alias: Fan Current LED state: True New LED state: False --- Plug Alias: Network Current LED state: True New LED state: False Interactive mode – walk through each discovered smart plug and make choice on the LED status: \u0026gt; python kasa-dark-mode.py -i You have selected interactive mode Searching for smartplugs... --plug found-- Plug Alias: Fan Current LED state: False Do you wish to turn ON \u0026#39;Fan\u0026#39; LED [Y/n]: n --- --plug found-- Plug Alias: Network Current LED state: True Do you wish to turn OFF \u0026#39;Network\u0026#39; LED [Y/n]: New LED state: False --- No tape. No LED.\n","permalink":"https://exitcode0.net/posts/turn-off-tplink-smart-plug-led/","summary":"The HS100/HS110 LED can be bright and obnoxious. If you are looking to turn off the LED status lights on the TPLink Kasa smart plugs, then look no further – the solution resides in this post. It does involve some Python interaction, but I promise that it is a gentle passing with the programming language.\nThis solution works for:\nHS100 HS103 HS105 HS110 The only real drawback is that when the LED is disabled, you can not see when the device might have become disconnected from the wireless network – this is normally indicated by a RED or orange LED status.","title":"Turn off TPLink Smart plug LED"},{"content":"Depending on your reason for using the PowerShell prompt at it, it might be useful to include data such as timestamps in your recorded output, to enable better auditing. This post will walk you through the simple steps needed for customising the PowerShell prompt, including how to show the current date and time.\nWhat exactly do I mean by the PowerShell prompt and what does the typical prompt look like by default? Well let me dial it back to default and show you:\nWindows 10 21H1 default PowerShell prompt – Windows Terminal The default Windows 10 (21H1) PowerShell prompt simply includes PS (PowerShell), followed by the current working directory; in my case: C:\\Users\\admin. In circumstances where we are looking to record our terminal sessions, it could be extremely useful to have the inclusion of time stamps. This is not only useful for auditing and log correlation but for accountability purposes.\nCreating a PowerShell profile Assuming that this is your first time modifying your PowerShell prompt for the current Windows user, first, we must create the file which stores our settings. Once available, this file is referenced each time PowerShell is launched.\nPowerShell supports several profile files. Also, PowerShell host programs can support their own host-specific profiles. For example, the PowerShell console supports the following basic profile files. The profiles are listed in precedence order. The first profile has the highest precedence.\nDescription Path All Users, All Hosts $PSHOME\\Profile.ps1 All Users, Current Host $PSHOME\\Microsoft.PowerShell_profile.ps1 Current User, All Hosts $Home[My ]Documents\\PowerShell\\Profile.ps1 Current user, Current Host $Home[My ]Documents\\PowerShell\\Microsoft.PowerShell_profile.ps1 Let’s create the file, from the PowerShell terminal:\nNew-Item -ItemType File -Path $PROFILE -Force now let’s edit that file (launching the editor from the terminal):\nise $PROFILE In my case, I have my Documents path mapped to a folder which I sync with Nextcloud. Now that we have a profile file created, we can start to store modifications, however, this script is not signed and will likely fail to run unless you modify your execution policy:\nSet-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy Unrestricted PowerShell ‘function prompt’ A good amount of code and inspiration for my selection of prompt modifications were inspired by this great post: https://www.norlunn.net/2019/10/07/powershell-customize-the-prompt, so I would strongly suggest checking that out.\nHistory ID $HistoryId: $MyInvocation.HistoryId Write-Host -Object \u0026#34;$HistoryId`: \u0026#34; -NoNewline -ForegroundColor Cyan Date and Time Write-Host -Object \u0026#34;$(Get-Date) \u0026#34; -NoNewline -ForegroundColor Green Username Write-Host -Object \u0026#34;-$($env:USERNAME)- \u0026#34; -NoNewline -ForegroundColor DarkRed Current path (working directory) $Drive: $pwd.Drive.Name $Pwds: $pwd -split \u0026#34;\\\\\u0026#34; | Where-Object { -Not [String]::IsNullOrEmpty($_) } $PwdPath: if ($Pwds.Count -gt 3) { $ParentFolder: Split-Path -Path (Split-Path -Path $pwd -Parent) -Leaf $CurrentFolder: Split-Path -Path $pwd -Leaf \u0026#34;..\\$ParentFolder\\$CurrentFolder\u0026#34; } elseif ($Pwds.Count -eq 3) { $ParentFolder: Split-Path -Path (Split-Path -Path $pwd -Parent) -Leaf $CurrentFolder: Split-Path -Path $pwd -Leaf \u0026#34;$ParentFolder\\$CurrentFolder\u0026#34; } elseif ($Pwds.Count -eq 2) { Split-Path -Path $pwd -Leaf } else { \u0026#34;\u0026#34; } Write-Host -Object \u0026#34;$Drive`:\\$PwdPath\u0026#34; -NoNewline -ForegroundColor Magenta return \u0026#34;\u0026gt; \u0026#34; My full prompt $PROFILE function prompt { $Success: $? ## History ID $HistoryId: $MyInvocation.HistoryId Write-Host -Object \u0026#34;$HistoryId`: \u0026#34; -NoNewline -ForegroundColor Cyan ## Date Time Write-Host -Object \u0026#34;$(Get-Date) \u0026#34; -NoNewline -ForegroundColor Green ## User Write-Host -Object \u0026#34;-$($env:USERNAME)- \u0026#34; -NoNewline -ForegroundColor DarkRed ## Path $Drive: $pwd.Drive.Name $Pwds: $pwd -split \u0026#34;\\\\\u0026#34; | Where-Object { -Not [String]::IsNullOrEmpty($_) } $PwdPath: if ($Pwds.Count -gt 3) { $ParentFolder: Split-Path -Path (Split-Path -Path $pwd -Parent) -Leaf $CurrentFolder: Split-Path -Path $pwd -Leaf \u0026#34;..\\$ParentFolder\\$CurrentFolder\u0026#34; } elseif ($Pwds.Count -eq 3) { $ParentFolder: Split-Path -Path (Split-Path -Path $pwd -Parent) -Leaf $CurrentFolder: Split-Path -Path $pwd -Leaf \u0026#34;$ParentFolder\\$CurrentFolder\u0026#34; } elseif ($Pwds.Count -eq 2) { Split-Path -Path $pwd -Leaf } else { \u0026#34;\u0026#34; } Write-Host -Object \u0026#34;$Drive`:\\$PwdPath\u0026#34; -NoNewline -ForegroundColor Magenta return \u0026#34;\u0026gt; \u0026#34; } Save your file and load a new PowerShell terminal session to admire your work:\nColourful and functional!\n","permalink":"https://exitcode0.net/posts/customising-the-powershell-prompt-to-show-date-and-time/","summary":"Depending on your reason for using the PowerShell prompt at it, it might be useful to include data such as timestamps in your recorded output, to enable better auditing. This post will walk you through the simple steps needed for customising the PowerShell prompt, including how to show the current date and time.\nWhat exactly do I mean by the PowerShell prompt and what does the typical prompt look like by default?","title":"Customising the PowerShell prompt to show date and time"},{"content":"I own a basic VW Caddy MK3 (2009) which was equipped with a RCD 300 head unit. It did not come with the aux function/connectivity and this post explains how I added Bluetooth connectivity. This was a very simple installation with no requirement for VCDS coding; the Bluetooth adapter replicates a CD changer.\nAt the time of writing this article, I was able to purchase this adapter on Amazon UK for ~£35 – https://amazon.co.uk/gp/product/B0773P8HM5\nInstallation The installation for this Bluetooth adaptation is a relatively simple one and require very few tools. The manufacturer of the adapter also includes a QR code linking to a easy to follow YouTube video:\nInstallation steps: Remove the surround trim with a trim tool or plastic implement – a screwdriver may leave marks on the plastic trim. Remove 4 T20 screws around the RCD 300 head unit, then pull the RCD 300 out of the dash. Clip the Bluetooth adapter’s white block connector into the bottom left of the block connector (when looking at the back of the head unit). Unscrew the small torx screw on the back of the RCD 300 then screw back in with the small grounding connector between the screw head and the back of the head unit. Plug in the optional microphone and route this to an appropriate position in the vehicle. Reverse the removal steps to secure the head unit and trim in place. The 12 pin VAG connector Using the adapter with the RCD300 Once installed, you need to switch the RCD300 to CD mode – press the CD button. Whilst you can still use the unit as a CD player, please remove any CDs before proceeding with these instructions.\nOnce in CD mode, search for available Bluetooth devices on your phone and connect to the available device. Once paired, the RCD 300 should replicate the connection of a CD multi-changer. Tracks on your phone show up as CDs, therefore you can skip tracks with the left and right arrow buttons. These two buttons also control accepting or rejecting incoming calls on the phone. If your vehicle has corresponding steering wheel controls, these will carry out the same functions.\nFrom there onwards, your phone will auto-connect to the Bluetooth device which becomes available after the vehicle ignition is enabled (and the head unit is powered on). I have found that because of the slight delay – 2 seconds – between the device powering on and connecting to the phone, the RCD 300 defaults back to FM/AM, so it is a requirement to press the CD button every time.\nPlaying Spotify over bluetooth from my grubby phone!\n","permalink":"https://exitcode0.net/posts/adding-bluetooth-to-a-vw-rcd-300/","summary":"I own a basic VW Caddy MK3 (2009) which was equipped with a RCD 300 head unit. It did not come with the aux function/connectivity and this post explains how I added Bluetooth connectivity. This was a very simple installation with no requirement for VCDS coding; the Bluetooth adapter replicates a CD changer.\nAt the time of writing this article, I was able to purchase this adapter on Amazon UK for ~£35 – https://amazon.","title":"Adding Bluetooth to a VW RCD 300"},{"content":" This is part 2 from my previous post on RTX 3060 Nicehash overclocking settings. I don’t want to edit the previous article because the content still stands to be accurate for the hash rate I achieved. However, I have since learned even more about the card and managed to improve my Nicehash quick miner hash rate by a further 10%!\nIf you want to see the first/part1 post, you can do so here: RTX 3060 Nicehash mining overclock settings\nAnd if you are looking for my post (and the download) on unlocking the hash rate with the 470.05 driver, that’s here: Unlock RTX 3060 mining hash rate.\nMy previous settings to acheive 44 MH/s used the following sentiment:\nLower the core clock rate Lower the power limit(%) Increasethe memory clock rate But I have since made some overclocking improvements and some additional changes to improve the longevity of the card. So much so that my overall uplift from stock (using the 470.05 driver) is sitting at around 25%! I am now able to reach 48MH/s with the DaggerHashimoto algorithm.\nBehold, 48 MH/s on an RTX 3060 (the card which NVidia does not want you to mine on)\nRTX 3060 overclocking with MSI Afterburner Your millage may vary with your individual card, but my improved settings to reach higher quickminer hash rates are as follows:\nPower limit: 75% Core Clock: -500 MHz – as low as it will go Memory Clock + 1300 MHz Fan Speed: Auto – with a more aggressive fan curve, see below… The MSI Afterburner settings to achieve 48MH/s on a RTX 3060 Keeping the GPU safe A more aggressive fan curve to cool VRAM I had been warned by a kind comment on my last post that whilst my GPU’s core clock temp might be low, the overclocked VRAM temp might be extremely high and limiting the performance of the card. What’s more it would not be conducive to a long living graphics card.\nI use my profiles in MSI Afterburner to switch back to stock settings when I want to play some video games – I am not a dedicated miner. So killing my card prematurely is not something I want to proceed in doing. I have chose to keep my fan on auto, to spare my ears, but applied a more aggressive fan curve to help keep the card temps down.\nMy particular Gigabyte card does not have a dedicated VRRAM temperature sensor – or at least it is not recognized by GPU-Z or HWinfo. Therefore, my only hope is to keep the average temperate of the card under control.\n(If you are going to follow this step, remember to tick the box to ‘Enable user defined software automatic fan control’)\n","permalink":"https://exitcode0.net/posts/more-rtx-3060-nicehash-overclocking/","summary":"This is part 2 from my previous post on RTX 3060 Nicehash overclocking settings. I don’t want to edit the previous article because the content still stands to be accurate for the hash rate I achieved. However, I have since learned even more about the card and managed to improve my Nicehash quick miner hash rate by a further 10%!\nIf you want to see the first/part1 post, you can do so here: RTX 3060 Nicehash mining overclock settings","title":"More RTX 3060 Nicehash overclocking"},{"content":"I recently managed to buy a Gigabyte RTX 3060 without lining the pockets of a scaler, lucky me! I recently discussed how I was able to circumvent the Nvidia crypto mining limitations: Unlock RTX 3060 mining hash rate (that post also includes access to the driver). This doubled the RTX 3060 Nicehash mining rate using the ‘unlocked’ Nvidia beta driver – going from ~22MH/s up to ~40MH/s. Impressive!\nNow I am going to share my RTX 3060 overclock settings, which enabled me to achieve a further 10% uplift in Nicehash performance. But first I must disclaimer that you are responsible for your own hardware and any changes you make are at your own risk. What’s more, modern silicone is a lottery, you might not get these exact mining hash rates, or you might get more; keep an eye on your thermals and protect your card and safety of your system accordingly!\n**Important Note - this post is based on a V1 (non LHR RTX 3060)**\nRTX 3060 mining overclock settings Before the overclock I was seeing between 38 and 41.5 MH/s when Nicehash was running the DaggerHashimoto algorithm (on the 470.05 driver). After some tweaks in MSI Afterburner, I was able to consistently strike 44+MH/s, a nice 10% increase.\nMy settings are mainly based on those recommended for the RTX 3060Ti here: https://miningchamber.com/gpu-mining/rtx-3060-ti-mining-settings. The synopsis is as follows:\nLower the core clock rate Lower the power limit(%) Increase the memory clock rate\n**My RTX 3060 MSI afterburner settings:**\nPower limit: 65% Core Clock: -400 MHz Memory Clock + 800 MHz Fan Speed: Auto **EDIT**: I have since managed even greater hash rates, which I have covered in a new post: More RTX 3060 Nicehash overclocking.\nMy current RTX 3060 Overclock for 47MH/s\nI would suggest that these settings rest on the side of caution and are stretching my card too hard. I am happy that the card is stable with these settings and is in fact running a little cooler at the same or lower fan speed than at stock. What’s more, Nicehash suggests that my total wattage has dropped from 140w to **110w** – lowering the ROI for the card.\nI am using the profile switching function in MSI Afterburner to make it easy to flick back to stock settings and play some video games when I want to. I’d recommend this for anyone else mining on their card to record the outrageous cost of buying it.\nHave you overclocked your RTX 3060? Share your afterburner settings and hashrate in the comments below… Let’s see who won the silicon lottery!\n","permalink":"https://exitcode0.net/posts/rtx-3060-nicehash-mining-overclock-settings/","summary":"I recently managed to buy a Gigabyte RTX 3060 without lining the pockets of a scaler, lucky me! I recently discussed how I was able to circumvent the Nvidia crypto mining limitations: Unlock RTX 3060 mining hash rate (that post also includes access to the driver). This doubled the RTX 3060 Nicehash mining rate using the ‘unlocked’ Nvidia beta driver – going from ~22MH/s up to ~40MH/s. Impressive!\nNow I am going to share my RTX 3060 overclock settings, which enabled me to achieve a further 10% uplift in Nicehash performance.","title":"RTX 3060 Nicehash mining overclock settings"},{"content":"There has been lots of news coverage around the recent mistake made by Nvidia with their RTX 3060 driver allowing uninhibited mining hash rates: a great round-up from The Verge. A developer driver inadvertently included code used for internal development which removes the hash rate limiter on RTX 3060 in some configurations. The driver has been removed, but there are lots of copies in circulation.\nThere are still some limitations, you must have a monitor attached and can only run a single RTX 3060 at a time. However, it is only a matter of time before motivated miners get over the remaining hurdles.\nGeForce RTX 3060 GAMING OC 12G How to unlock RTX 3060 mining hash rate My setup:\nA single RTX 3060 connected to a monitor Running Nicehash on Windows 10 I am just a gamer looking to recoup some of the extortionate price I paid for a graphics card which I spent weeks trying to buy 😊 I use NiceHash, because it is convenient and profitable for me, I am not a dedicated miner. If you want to get started with NiceHash, here is my referral link: https://www.nicehash.com/?refby=b5363bcf-2c11-4482-82d2-d224cd1895ab.\nNiceHash covers the impact that the driver limitation has on the RTX 3060’s hashing rate here: https://www.nicehash.com/blog/post/nvidia-rtx-3060-mining-hashrate. On the Gigabyte OC card that I have, I was seeing ~22MH/s before installing the development driver. Whilst this still profitable, it is close to 50% of what the card is capable of.\nInstalling Nvidia development driver 470.05 I was early on the news as was able to get a copy of the driver from a forum user who had repackaged it – noting that the driver didn’t work for MSI cards. The download was hosted on Mega, which always gives me a reason to worry, so I ran the download through virus total first and it came back clean. But it really is the case that when you start installing drivers from alternative sources, you are putting yourself at considerable risk.\nWhilst I can’t confirm how long this download will remain available, I wish you the best of luck in obtaining it:\nhttps://drive.google.com/file/d/12jIy0zkJlikzfUevEmsQRWj8-IP91K-9/view?usp=sharing\nI would recommend running a PowerShell hash check on the file to make sure that it matches below – this will tell you if it has been co-opted:\nPS C:\\Drivers\u0026gt; **Get-FileHash .\\470.05.zip** Algorithm Hash Path --------- ---- ---- SHA256 C97C25360E76ED1252B18E583B93F6446EB3801136F38E9214B770C1A4053A16 C:\\Drivers\\470.05.zip It installs much like installing any other Nvidia driver, run the setup file and reboot when you are done. After a reboot, open up the Nvidia control panel and check your driver version:\nRTX 3060 running driver 470.05 Now that I am running this beta driver, here is some output from NiceHash running the daggerhashimoto algorithm. I am seeing between 39 and 42 MH/s; this is without any overclocking tweaks to the GPU.\nEffecive hashrate between 39 and 42 MH/s – Driver version 470.05 Big disclaimer… You are responsible for your own hardware and system integrity. This is a very bleeding edge subject, so your mileage may vary considerably. Happy mining!\n","permalink":"https://exitcode0.net/posts/unlock-rtx-3060-mining-hash-rate/","summary":"There has been lots of news coverage around the recent mistake made by Nvidia with their RTX 3060 driver allowing uninhibited mining hash rates: a great round-up from The Verge. A developer driver inadvertently included code used for internal development which removes the hash rate limiter on RTX 3060 in some configurations. The driver has been removed, but there are lots of copies in circulation.\nThere are still some limitations, you must have a monitor attached and can only run a single RTX 3060 at a time.","title":"Unlock RTX 3060 mining hash rate"},{"content":" So Honeygain has finally arrived as a Docker container and this article will give you everything you need to build your own docker-compose YAML file for faster deployments.\nYou can see the docs provided by the Honeygain devs on the matter here:\nhttps://honeygain.zendesk.com/hc/en-us/articles/360018979919-How-to-run-Honeygain-on-Docker-Linux-\nHowever, they do not provide a nice way to deploy time and again from a docker-compose file, scroll down for a template! At the time of writing, you are permitted to run the service on two devices per public IP. Unfortunately, the docker image doesn’t currently support the content delivery feature\nRunning the Honeygain docker image (without docker-compose) If oyu just want to run the container here are the steps to do so, provided that you have a docker environment ready to go:\nPull the Docker image docker pull honeygain/honeygain Open Honeygain Terms of Use. If you agree with our Terms of Use, please continue docker run honeygain/honeygain -tou-get Start the Honeygain Docker container docker run honeygain/honeygain -tou-accept -email ACCOUNT_EMAIL -pass ACCOUNT_PASSWORD -device DEVICE_NAME Replace ACCOUNT_EMAIL with your Honeygain account email\nReplace ACCOUNT_PASSWORD with your Honeygain account password\nReplace DEVICE_NAME with a name that you would like to give to your Docker container. This name will be visible on the Dashboard.\nNOTE: Use different DEVICE_NAME for every container that you create.\nHoneygain docker-compose YAML example Now here is how an example of a docker-compose file for the Honeygain container. Please note that you will still need to change your device name between deployments.\nversion: \u0026#39;3\u0026#39; services: honeygain: container_name: honeygain image: honeygain/honeygain command: -tou-accept -email my.name@email.com -pass mylamepassword -device sweethoney01 restart: unless-stopped Change the email, password and device name to suit! And be careful not to publish your compose file with real credentials to any public code repository.\nAnd here is how to run your copose file in detached mode (fromt he direcotry where you YAML file is stored):\ndocker-compose run -d ","permalink":"https://exitcode0.net/posts/honeygain-docker-compose-setup/","summary":"So Honeygain has finally arrived as a Docker container and this article will give you everything you need to build your own docker-compose YAML file for faster deployments.\nYou can see the docs provided by the Honeygain devs on the matter here:\nhttps://honeygain.zendesk.com/hc/en-us/articles/360018979919-How-to-run-Honeygain-on-Docker-Linux-\nHowever, they do not provide a nice way to deploy time and again from a docker-compose file, scroll down for a template! At the time of writing, you are permitted to run the service on two devices per public IP.","title":"Honeygain docker-compose setup"},{"content":" For the longest time, I have been running VMs on my Unraid server and had the need to passthrough USB devices. I will be the first to admit that I have too many computers and a shrink might classify it as an addition. Coupled with my quest for a clutter-free desk, I have been faced with the issue of swapping USB devices like a keyboard and mouse, from one machine to another Unraid virtual machine (vm).\nSolving the physical component of changing the device from one machine to another is easy. Just acquire a USB switcher/KVM such as this:\nA simple USB switch / KVM There are hundreds of these and they are relatively cheap. However, the real problem is that when you switch the keyboard and mouse into the Unraid server (in an attempt to ‘plug’ them into a VM), they are assigned to the Unraid host, not a VM.\nIn the past I have developed a way to use a Pi with a button attached to send virsh commands to Unraid, connecting the USB device to a particular VM. This worked well, but required me to push 2 buttons; one to physically switch the USB devices, followed by another to run the virsh command from a Pi – only mildly irritating. The main irritation was the limit to the number of times you could ‘attach’ a USB device to a VM before its virtual hub became full. So I tried removing the device before attaching it (all using virsh) and ultimately ran out of skill and/or patients.\nWhat I really needed was a USB controller to be permanently pass through to the VM. Then, whenever a device was plugged into that USB controller, it would be connecting to the VM and never the Unraid host. And so then next issue (which we are about to solve), which USB controller and how to pass it through?\nInateck Superspeed 4 Ports PCI-E to USB 3.0 I found the ‘Inateck Superspeed 4 Ports PCI-E to USB 3.0’ on Amazon and took a bit of an educated guess on whether I would be able to pass it through. The supported card/controllers list is very thin on the ground.\nImportantly, it used the Fresco FL1100 series chipset, which to the best of my reading, can be passed through to a VM. It is also popular with crypto mining for this reason.\nHow to pass through the USB controller Prerequisites:\nA working VM CPU and Bios that supports VT-d (and has it enabled) USB pcie card Find the device identifier With the USB card plugged in, we must first identify it. Sadly, I cannot confirm that this card will show up in its own IOMMU group as it did for me, so your mileage may vary. My card shows up in its own IOMMU group and I can see its ID: 1b73:1100.\nFresco FL1100 USB Controller\nEdt Boot config Now we need to tell Unraid to ignore/backlist the device by its ID, so that it will be available for passthrough. Go to the Main page, and click on your flash device to start editing the Syslinux boot config:\nUnder ‘kernel /bzimage‘ paste the following line, replacing the ID with yours appropriately:\nappend initrd=/bzroot pci-stub.ids=1b73:1100 Save this, then it is time to reboot.\nEdit the VM Now we can edit the VM (with it powered off) and add the pcie USB card:\nSUCCESS!\nYou should now have true USB hot swap in your UNRAID VM. I have had this working reliably for a number of weeks at this point, the passthrough config survives a complete system reboot and UNRAID upgrades.\n","permalink":"https://exitcode0.net/posts/unraid-usb-controller-passthrough-vm-hot-swap-usb/","summary":"For the longest time, I have been running VMs on my Unraid server and had the need to passthrough USB devices. I will be the first to admit that I have too many computers and a shrink might classify it as an addition. Coupled with my quest for a clutter-free desk, I have been faced with the issue of swapping USB devices like a keyboard and mouse, from one machine to another Unraid virtual machine (vm).","title":"Unraid USB Controller Passthrough - VM hot swap USB"},{"content":"Cryptocurrencies have been back on the popular agenda lately, thanks mostly to the surging value of Bitcoin. I have been interested, but not massively invested in cryptocurrency for many years. If I had the courage to buy and hold all those years ago, I might be writing to you from a beach – I assure you, this is not the case! I have been getting into InfluxDB lately and its fantastic ability to store heaps of time series data. Well, using python and Coinbase, I am going to show you how to collect bitcoin (and other crypto assets) price data and store it in InfluxDB… then we are going to make some neat panels in Grafana to help us realise our data. This is going to be a longer write up than most, so here is here are links to each section:\nContents Table: Prerequisites Using the Coinbase API with python Writing to InfluxDB with python Creating Coinbase data panels in Grafana Ways to support this blog Prerequisites Before we can jump into programming, there are a number of boxes we need to check off.\nYou must have a Coinbase account. If not, consider using my referral link to get $10 free when you sign up and buy/sell $100 of crypto: https://www.coinbase.com/join/cockin_u Your programming environment has python3 and pip3 installed – pip3 will be used to install the Coinbase module: pip install coinbase You have InfluxDB and Grafana setup. There are a number of great tutorials out there on how to do this, here are some places to get started: Unraid – https://unraid.net/blog/ultimate-unraid-dashboard InfluxDB getting started guide, including docker setup – https://docs.influxdata.com/influxdb/v2.0/get-started/ Grafana setup guide – https://grafana.com/docs/grafana/latest/installation/ Using the Coinbase API with python Whist you can use the Coinbase API without an account there are rate-limiting restrictions, so if you want more representative, ‘live’ data, you will need to create a Coinbase account and apply for an API key:\nWhere to find API options in your Coinbase account For the purposes of what we are doing in this exercise, there is no need to give the API key anything more than read level access to our account:\nCoinbase API read permissions\nPython and Coinbase You can access the full Coinbase API reference here, but we will be focusing on getting price data: https://developers.coinbase.com/api/v2#prices\nBasic Python Request – BTC Buy Price USD from coinbase.wallet.client import Client client: Client(\u0026lt;api_key\u0026gt;, \u0026lt;api_secret\u0026gt;) price: client.get_buy_price(currency_pair: \u0026#39;BTC-USD\u0026#39;) Basic Response – BTC Buy Price USD { \u0026#34;data\u0026#34;: { \u0026#34;amount\u0026#34;: \u0026#34;1020.25\u0026#34;, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34; } } So now that we have the basics from the Coinbase API reference, let us flesh this out into something more reliable and error-proof. Now is also a great time to make this into a function and call it continuously to gather time-series data.\nImproved Python Request The main function, giving us a continuous loop and skipping sending data when a Coinbase API request fails (checking for BTC and ETH):\nimport requests import sys import os import json from time import sleep from coinbase.wallet.client import Client def main(): timer: 0 timer_stop: 120 portfolio_total: 0 coins: [\u0026#34;BTC\u0026#34;, \u0026#34;ETH\u0026#34;] while True: for coin in coins: reqdvalue: func_req_price(coin, \u0026#39;USD\u0026#39;, \u0026#39;sell\u0026#39;) if reqdvalue != False: func_logPrice(coin, reqdvalue, \u0026#39;USD-sell\u0026#39;) reqdvalue: func_req_price(coin, \u0026#39;USD\u0026#39;, \u0026#39;buy\u0026#39;) if reqdvalue != False: func_logPrice(coin, reqdvalue, \u0026#39;USD-buy\u0026#39;) sleep(5) A function to get the price data for a provided coin and kind (please note that we DON’T actually need an API key for this particular method):\ndef func_req_price(coin, currency, kind): try: price: float(requests.get( f\u0026#39;https://api.coinbase.com/v2/prices/{coin}-{currency}/{kind}\u0026#39; ).json()[\u0026#39;data\u0026#39;][\u0026#39;amount\u0026#39;]) return price except: print(\u0026#34;Error getting price for: {}\u0026#34;.format(coin)) return False This returns a price value or false if the request failed for any reason.\nWe now have three values – coin, kind (USD buy/sell) and price) – which we can throw into InfluxDB…\nIMPORTANT NOTE: If you are requesting data more frequently from Coinbase or you are pulling data for far more than just BTC and ETH, you will run into rate limits and receive error 429 responses for your requests. This is way Coinbase suggest using the API supported method of getting prices:\nfrom coinbase.wallet.client import Client client: Client(\u0026lt;api_key\u0026gt;, \u0026lt;api_secret\u0026gt;) price: client.get_buy_price(currency_pair: \u0026#39;BTC-USD\u0026#39;) 3 – Writing to InfluxDB with python I have covered most of the groundwork for this section already in this post: https://exitcode0.net/writing-to-an-influxdb-server-with-python3/.\nGeneric InfluxDB example from datetime import datetime from influxdb import InfluxDBClient def func_log(unit, kind, value): client: InfluxDBClient(host=\u0026#39;127.0.0.1\u0026#39;, port=8086) client.create_database(\u0026#39;DBname\u0026#39;) client.switch_database(\u0026#39;DBname\u0026#39;) json_body: [ { \u0026#34;measurement\u0026#34;: \u0026#34;MeasureName\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;unit\u0026#34;: unit, \u0026#34;kind\u0026#34;: kind }, \u0026#34;fields\u0026#34;: { \u0026#34;value\u0026#34;: value }, \u0026#34;time\u0026#34;: f\u0026#39;{datetime.utcnow().isoformat()}Z\u0026#39; } ] try: if client.write_points(json_body): success: True else: print(\u0026#34;Error writing to InfluxDB\u0026#34;) except: print(\u0026#34;Error writing to InfluxDB\u0026#34;) #Call the above function: func_log(unit, kind, value) Coinbase specific InfluxDB example def func_logPrice(coin, price, kind): client: InfluxDBClient(host=\u0026#39;127.0.0.1\u0026#39;, port=8086) client.switch_database(\u0026#39;PyDCA\u0026#39;) json_body: [ { \u0026#34;measurement\u0026#34;: \u0026#34;stability\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;coin\u0026#34;: f\u0026#39;{coin}\u0026#39;, # e.g. BTC \u0026#34;kind\u0026#34;: f\u0026#39;{kind}\u0026#39; #e.g. USD-sell }, \u0026#34;fields\u0026#34;: { \u0026#34;price\u0026#34;: price }, \u0026#34;time\u0026#34;: f\u0026#39;{datetime.utcnow().isoformat()}Z\u0026#39; } ] try: if client.write_points(json_body): #do nothing success: True else: print(\u0026#34;ERROR writing to InfluxDB\u0026#34;) except: print(\u0026#34;ERROR writing to InfluxDB\u0026#34;) Combining all this code, we can pass any value that we retrieve from the Coinbase API and store it in InfluxDB with a time stamp. This will be very useful when we start creating panels in Grafana!\n4 – Creating Coinbase data panels in Grafana In my last segment of code, you can see that I specify the PyDCA data group when writing to influxDX, remember to set this accordingly in your code and Grafana panels.\nHere is a basic query to display BTC and ETH, USD sell prices as a time series graph; I have set ETH on the right-hand axis as the two values differ so greatly, bring the graphs into a tighter range:\nA basic Grafana time series query Showing the USD sell price provided by the Coinbase API API data flat spot As you can see from the above figure, there are some gaps in our data, this can be caused by a lack of response from the Coinbase API request, an API limit, or a failure to write to the InfluxDB. In my experience, this is a frequent occurrence for BTC and ETC prices in USD; most probably because it is the most popular API endpoint. I do not see correlating flat spots in the data for GBP. In Grafana we can make smooth the graph out by setting the GROUP BY: fill(none) tag. Grafana draws a neat line between data points where there is NULL data.\n5 – Ways to support this blog As previously mentioned, if you are new to Coinbase, you can sign up with my referral link. If you do so, you will receive $10 and so will I (You'll both get free Bitcoin when a friend buys or sells $100 of crypto – Coinbase).\nhttps://www.coinbase.com/join/cockin_u\nAlternatively, you can fund my coffee misadventures by throwing a coin in the tip jar:\nFREE BONUS: Alternatively, you can share this post or leave a comment below to let the search algorithms know I’m not totally useless.\nSee you on the moon HODLers!\n","permalink":"https://exitcode0.net/posts/collect-bitcoin-price-data-with-python-and-coinbase/","summary":"Cryptocurrencies have been back on the popular agenda lately, thanks mostly to the surging value of Bitcoin. I have been interested, but not massively invested in cryptocurrency for many years. If I had the courage to buy and hold all those years ago, I might be writing to you from a beach – I assure you, this is not the case! I have been getting into InfluxDB lately and its fantastic ability to store heaps of time series data.","title":"Collect bitcoin price data with python and Coinbase"},{"content":"A quick GitHub gist for anyone looking to write to an InfluxDB server with Python3. This is a generic function that accepts three inputs; as an example, I am using temperature data in degrees Celsius.\nTurning DHT11 readings into beautiful graphs in Grafana Once you have your data in influxDB, a great way to visualise it is using Grafana. I hope to bring you more posts in the future about visualising your python data with Grafana.\nhttps://gist.github.com/Tombo1001/1f096ca224f0e61ac66548e2abd36a25\nNow that you have the basics of writing events to influxDB with Python, feel free to leave links in the comments on the wonderful things you were able to graph with Grafana.\nMore Python Guides CALCULATING COMPOUND INTEREST WITH PYTHON 3 HOW TO CALL A BASH COMMAND WITH VARIABLES IN PYTHON MONITOR YOUR PUBLIC IP ADDRESS WITH PYTHON Worth noting that all of the above data could find its way onto a Grafana dashboard… the possibilities are endless!\n","permalink":"https://exitcode0.net/posts/writing-to-an-influxdb-server-with-python3/","summary":"A quick GitHub gist for anyone looking to write to an InfluxDB server with Python3. This is a generic function that accepts three inputs; as an example, I am using temperature data in degrees Celsius.\nTurning DHT11 readings into beautiful graphs in Grafana Once you have your data in influxDB, a great way to visualise it is using Grafana. I hope to bring you more posts in the future about visualising your python data with Grafana.","title":"Writing to an InfluxDB server with Python3"},{"content":"Following on from a previous post on how to setup a VLAN on a Fortigate hardware switch, this post is going to explain how we can link an AP-bridge SSID to a hardware switch and VLAN.\nFor the most part, the only reference material you will need to complete this configuration can be found here: https://docs.fortinet.com/document/fortiap/6.4.0/fortiwifi-and-fortiap-cookbook/252439/configuring-the-fortigate-interface-to-manage-fortiap-units. However, if you have been working with Fortigates and by extension FortiOS for quite some time, you may be wonder where the CAPWAP option vanished to? Fortinet adopted this into the security fabric naming convention. CAPWAP has even been replaced in the CLI by ‘fabric’.\nAP-Bridge with a VLAN The confif that I a looking to build will give me an SSID for a given VLAN; a client which is connected to this SSID wil be given a DHCP address and will be subject to firewall policy for that VLAN. The client will not be required to set their own VLAN tag – traffic will be tagged by the SSID interface.\nV33 is an AP-brige to VLAN 33 on the hardware switch. We are also going to have an SSID on the ‘default VLAN’ aka a typical wireless LAN – sharing the same subnet and multicast zone as the ports in our hardware switch.\nUltimately we will have an SSID which is isolated from all other ‘LAN’ traffic, perfect for wireless CCTVs cameras or IoT device which we don’t trust.\nThe Configuration Hardware Switch and VLAN Much of this configuration was covered in a previous post, see here: https://exitcode0.net/fortigate-add-a-vlan-to-a-hardware-switch.\nAt this point we will have a hardware switch and a VLAN assigned to it – VLAN 33.\nOne important change is to enabled security fabric management (previously CAPWAP) on our hardware switch:\nSecurity Fabric Connection is enabled – previous known as CAPWAP. If you choose to operate in the CLI, here is how you would implement this:\nconfig system interface edit LAN #(hardware switch name) set allow-access fabric set ap-discover enable next end SSID Configuration Setting traffic mode to AP-bridge and Optional VLAN ID to 33, we create an SSID with a suitabe BSSID and security settings as follows:\nan SSID for VLAN 33 In AP-bridge mode, all traffic on SSID V33 will be tunneled to the Fortigate’s security fabric controller and to the VLAN interface there onwards.\nAnd that is it! It really is that simple. It is worth mentioning, that this is also possible with a software switch, but not without significant throughput penalties – all switching passes through the the Fortigate’s CPU in a software switch.\n","permalink":"https://exitcode0.net/posts/fortigate-ap-bridge-with-a-hardware-switch/","summary":"Following on from a previous post on how to setup a VLAN on a Fortigate hardware switch, this post is going to explain how we can link an AP-bridge SSID to a hardware switch and VLAN.\nFor the most part, the only reference material you will need to complete this configuration can be found here: https://docs.fortinet.com/document/fortiap/6.4.0/fortiwifi-and-fortiap-cookbook/252439/configuring-the-fortigate-interface-to-manage-fortiap-units. However, if you have been working with Fortigates and by extension FortiOS for quite some time, you may be wonder where the CAPWAP option vanished to?","title":"Fortigate: AP-Bridge with a hardware switch"},{"content":" In this post, we are going to discuss how to add a VLAN to a hardware (sometimes referred to as physical) switch or interface on a Fortigate. It is worth noting that I actually do my testing on a FortiWifi, so I can assure you that this also applies there too. I will be focusing on the configuration which is relevant to FortiOS v6.0 and above, so your millage vary between versions. However, you need can usually be found over at the Fortinet Handbook: https://docs.fortinet.com/product/fortigate/6.0.\nSwitch mode Vs Interface Mode I’m not going to get too far into the minutia of switch vs interface mode. There are many arguments for both and I believe that the correct answer is very much dependant on your intended implementation. I have my Fortigate 60E configured in interface mode, with most of my LAN ports, 2-7, configured in a hardware switch. This hardware switch is operating in what you may class as the default VLAN. Most of the configuration which we are going to discuss is basic and will apply to a Fortigate in (software) switch mode. I will try my best to point out any obvious configuration differences where they may arise.\nVLANs in NAT mode It is important to mention that I have my Fortigate 60E running in NAT mode. In NAT mode, the FortiGate supports VLAN trunk links with IEEE 802.1Q‑compliant switches or routers. The trunk link transports VLAN-tagged packets between physical subnets or networks. When you add VLAN sub-interfaces to the FortiGate physical interfaces, the VLANs have IDs that match the VLAN IDs of packets on the trunk link. The FortiGate directs packets with VLAN IDs to sub‑interfaces with matching IDs.\nWhereas in Transparent mode, the Fortigate behaves like a layer-2 bridge but it can still provide services such as antivirus scanning, web filtering, spam filtering, and intrusion protection to traffic. There are some limitations in transparent mode because you can’t use SSL VPN, PPTP/L2TP VPN, DHCP server, or easily perform NAT on traffic. The limits in transparent mode apply to IEEE 802.1Q VLAN trunks passing through the device.\nFortigate VLAN Config Example Physical Switch Config I will start by splitting an interface out of the default hardware switch. I am chosing to do this because I want to preseve the rest of the LAN ports for use in my ‘regular’ LAN. Interface LAN1 will be used for VLANs and connected to a managed switch whih supports VLANs.\nTwo hardware switches – one with VLANs The reality of having ‘two hardware switches’ is that the Fortigate created two virtual-switches, nested under one physical-switch. This is easily demonstrated in the subsequent config:\nconfig system physical-switch edit \u0026#34;sw0\u0026#34; set age-val 0 next end config system virtual-switch edit \u0026#34;internal\u0026#34; set physical-switch \u0026#34;sw0\u0026#34; config port edit \u0026#34;internal2\u0026#34; next edit \u0026#34;internal3\u0026#34; next edit \u0026#34;internal4\u0026#34; next edit \u0026#34;internal5\u0026#34; next edit \u0026#34;internal6\u0026#34; next edit \u0026#34;internal7\u0026#34; next end next edit \u0026#34;VTRUNK\u0026#34; set physical-switch \u0026#34;sw0\u0026#34; config port edit \u0026#34;internal1\u0026#34; next end next end VLAN Config Now that we have split out our hardware switches, we can start adding VLANs. Enabling a DHCP server on a VLAN is optional, but I have no alternatives in my home network and most of the devices I plan to isolate in my VLANs can be troublesome to set static addresses on.\nBasic VLAN config on a hardware switch It is worth noting that, I did not enable a DHCP server on my LAN1 interface, so if my device is connected and does not have a valid VLAN tag set, it will be assigned an IP address. I named internal1 one to reflect that it will be used as a trunk port for the traffic of multiple VLANs to be carried over to a managed switch. Here is the CLI config for each of my VLAN interfaces:\nconfig system interface edit \u0026#34;internal\u0026#34; set vdom \u0026#34;root\u0026#34; set ip ---.---.---.--- ---.---.---.--- set allowaccess ping https ssh set type hard-switch set alias \u0026#34;LAN\u0026#34; set stp enable set snmp-index 6 next edit \u0026#34;VTRUNK\u0026#34; set vdom \u0026#34;root\u0026#34; set ip 172.16.30.1 255.255.255.0 set allowaccess ping set type hard-switch set description \u0026#34;VLAN TRUNK\u0026#34; set alias \u0026#34;VLAN TRUNK\u0026#34; set device-identification enable set lldp-transmission enable set role lan set snmp-index 11 next edit \u0026#34;V31-IoT\u0026#34; set vdom \u0026#34;root\u0026#34; set ip 172.16.31.1 255.255.255.0 set allowaccess ping set description \u0026#34;VLAN IoT\u0026#34; set alias \u0026#34;VLAN IoT\u0026#34; set device-identification enable set role lan set snmp-index 10 set interface \u0026#34;VTRUNK\u0026#34; set vlanid 31 next edit \u0026#34;V32-Hue\u0026#34; set vdom \u0026#34;root\u0026#34; set ip 172.16.32.1 255.255.255.0 set allowaccess ping set description \u0026#34;VLAN32 Hue\u0026#34; set alias \u0026#34;VLAN Hue\u0026#34; set device-identification enable set role lan set snmp-index 12 set interface \u0026#34;VTRUNK\u0026#34; set vlanid 32 next end ","permalink":"https://exitcode0.net/posts/fortigate-add-a-vlan-to-a-hardware-switch/","summary":"In this post, we are going to discuss how to add a VLAN to a hardware (sometimes referred to as physical) switch or interface on a Fortigate. It is worth noting that I actually do my testing on a FortiWifi, so I can assure you that this also applies there too. I will be focusing on the configuration which is relevant to FortiOS v6.0 and above, so your millage vary between versions.","title":"Fortigate: Add a VLAN to a hardware switch"},{"content":" I have covered this a number of times in the past and the posts have proved popular and useful to many. So, here is my guide for updating to the latest version of Python 3 (3.9) on Debian 10 Buster.\nTo clarify the purpose of this guide, Debian 10 ships with Python 2 (2.7) and Python 3 (3.7) installed at my time of writing. For those wishing to upgrade from python 3.7.X to 3.8.X or 3.9.X, this is the guide for you. If you are trying to configure python 3.7 as your default interpreter when you call ‘python‘, try this: CHANGING THE DEFAULT PYTHON VERSION IN DEBIAN. This method involves using the ‘update-alternatives‘ command. We will be using a similar method in this guide, however this time we only do so to give 3.9.X a higher priority to 3.7.X, rather than uninstalling older versions.\nThe basic premise is, install the version of python 3 desire, 3.9, then configure Debian to use python 3.7 at a higher priority to python 3.9.\nThis guide is written to target those using Debian 10, but the same principles apply to older versions of Debian and other operating system based on Debian, such as Kali Linux.\nThe Debian 10, Python upgrade process Check your version\nStep 1 is to check your current python version:\npython3 -V or\npython3 --version Download the latest or desired version of python 3\nNext, we need to download the latest version or desired version of python 3 from the python website. In my case, I selected 3.9.1. Once downloaded we need to extract the tar file.\nwget https://www.python.org/ftp/python/3.9.1/Python-3.9.1.tar.xz tar xf Python-3.9.1.tar.xz cd Python-3.9.1 You can find all versions of Python here: https://www.python.org/ftp/python\nPython 3.9.1 downloading…\nMake and Install\nNow that we have the files downloaded and extracted, it is time to compile them.\n./configure make make install ‘make’ commands can take quite some time to run, this is normal when compiling large programs from source…\nNOTE: if you are running a minimal install of Debian 10, you might need to install a C compiler before you can run ‘./configure‘ and the tool for running ‘make‘ commands:\napt-get update -y apt-get upgrade -y apt-get install build-essential -y apt-get install make -y Switch to the new Python version Finally, after compiling the new version of python from source, we can now configure Debian to make it our default version of python3.\nupdate-alternatives --install /usr/bin/python python3 /usr/local/bin/python3.9 10 The integer at the end of this command (10) sets the priority for the python version; the greater the integer, the higher the priority. At this point, we can rerun the previously used version commands and we should see that we now have Python 3.9.1 active.\nFixing and Updating Pip It was at this point that I attempted to install some required addons using pip and discovered that the upgrade to Python 3.9.1 had broken a few things. These were the commands I used to resolve issues with lsb_release and pip:\nln -s /usr/share/pyshared/lsb_release.py /usr/local/lib/python3.9/site-packages/lsb_release.py pip3 install --upgrade pip If you have found this guide useful or it has solved a burning issue for you, please consider throw a coin in the tip jar to help this site stay active:\nhttps://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick\u0026amp;hosted_button_id=BTQD4GN8TTWJN\u0026amp;source=url\nSome useful guides I found along the way: https://jcutrer.com/linux/upgrade-python37-ubuntu1810 – How to upgrade to python 3.7 on ubuntu 18.10. https://tecadmin.net/install-python-3-7-on-ubuntu-linuxmint/ – How to Install Python 3.7 on Ubuntu, Debian and LinuxMint. https://linuxconfig.org/how-to-change-default-python-version-on-debian-9-stretch-linux – How to change default python version on Debian 9 Stretch Linux Other Useful Debian tips: https://exitcode0.net/debian-9-running-a-python-script-at-startup/ – Debian 9 – Running a python script at boot. ","permalink":"https://exitcode0.net/posts/debian-10-how-to-upgrade-python-3-7-to-python-3-9/","summary":"I have covered this a number of times in the past and the posts have proved popular and useful to many. So, here is my guide for updating to the latest version of Python 3 (3.9) on Debian 10 Buster.\nTo clarify the purpose of this guide, Debian 10 ships with Python 2 (2.7) and Python 3 (3.7) installed at my time of writing. For those wishing to upgrade from python 3.","title":"Debian 10 - How to upgrade python 3.7 to python 3.9"},{"content":" Compound stonks only go up! I have a growing interest in finance and analytics, so it felt like a great idea to start creating my own set of financial tools with Python. This article will explain how I created a simple but effective script for calculating compound interest with Python. I wrote this in Python 3 (as all new Python projects should be from now onwards), but the library dependencies are very lightweight, making this something that could easily be re-written from Python 2.\nCalculating Compound Interest This small programming challenge hinges on the mathematics behind compound interest. If you don’t already have a clear understanding of how compound interest is calculated vs ‘simple’ interest, I would recommend starting here:\nhttps://www.mathsisfun.com/money/interest.html\nEssentially we need to take the following logic and create it in Python:\nWith **compounding,**we work out the interest for the first period, add it to the total, and then calculate the interest for the next period, and so on\nsource: https://www.mathsisfun.com/money/interest.html The Code import math def func_invest(input1, input2): output: input1 + input2 return output def func_interest(input1, input2): rate: input2 / 100 interest: input1 * rate output: input1 + interest return output def func_pl_calc(input1, input2, input3): output: (input1 - input2) - input3 return output def func_round(input, decimals=2): multiplier: 10 ** decimals return math.floor(input*multiplier + 0.5) / multiplier def func_main(): currency: \u0026#34;£\u0026#34; #change to suit monthly_invest: 10 #change to suit total_invested: 0 starting_portfolio: 10000 #change to suit total_portfolio: starting_portfolio cur_month: 1 investment_period: 120 #(months) change to suit interest_period: 12 #(months) change to suit interest_percent: 5 #(%) change to suit print(\u0026#34;-------------\\nInterest rate during investment period: {}%\u0026#34;.format(interest_percent)) print(\u0026#34;Month investment amount: {}{}\u0026#34;.format(currency, monthly_invest)) print(\u0026#34;Starting balance: {:.2f}\\n-------------\u0026#34;.format(starting_portfolio)) while cur_month \u0026lt;= investment_period: #investment math total_invested: func_invest(total_invested, monthly_invest) total_portfolio: total_portfolio + monthly_invest #check if interest payment due on balance if cur_month % interest_period == 0: total_portfolio: func_interest(total_portfolio, interest_percent) total_portfolio: func_round(total_portfolio) #increment investment period cur_month: cur_month + 1 print(\u0026#34;Total amount invested at end of period: {}{:.2f}\u0026#34;.format(currency, total_invested)) print(\u0026#34;Interest value during investment period ({:.0f} year[s]): {}{:.2f}\u0026#34;.format(investment_period / 12, currency, func_pl_calc(total_portfolio, total_invested, starting_portfolio))) print(\u0026#34;\\nPortfolio Value at end of period: {}{:.2f}\\n-------------\u0026#34;.format(currency, total_portfolio)) if __name__ == \u0026#39;__main__\u0026#39;: func_main() Also available on GitHub: https://github.com/Tombo1001/Py-Compound/\nSome of my functions are incredibly basic and completely unnecessary, but I find that it helps to remove the mathematics from the main logic of the script. Feel free to consolidate these into the main function.\nThe Outcome With the code above you should get the following output, noting that interest is calculated at the end of each interest period:\nStarting with the current variables defined in the GitHub repository Changing when interest is calculated (before or after the monthly investment), impacts the end portfolio value.\nIt is possible to realise our compounded interest with a graph, showing that our interest gains increase over time as our portfolio value increases:\nsource: https://www.thecalculatorsite.com/finance/calculators/compoundinterestcalculator.php The Next Steps Now that we have some basic code to calculate compound interest, here are some great next steps for this project:\nCreate a Python Django web app and publish the tool for everyone on the internet to use. Integrate variable interest rates during the investment period. Create graphs of the data as seen above using the popular Matplotlib library. Perhaps you will see all over the above in a post here sooner or later!\n","permalink":"https://exitcode0.net/posts/calculating-compound-interest-with-python-3/","summary":"Compound stonks only go up! I have a growing interest in finance and analytics, so it felt like a great idea to start creating my own set of financial tools with Python. This article will explain how I created a simple but effective script for calculating compound interest with Python. I wrote this in Python 3 (as all new Python projects should be from now onwards), but the library dependencies are very lightweight, making this something that could easily be re-written from Python 2.","title":"Calculating compound interest with Python 3"},{"content":"Are you still waiting for your Windows 10 computer to receive the 20H2 update? Here is how to force the windows 10 20H2 update. For many, the update may already available the Windows 10 update settings:\nThe 20H2 update when available in the Windows 10 update settings. Microsoft is staggering the over-the-air update as they usually do with new major releases. However if you are tired of hitting ‘check for updates’ in the windows 10 update settings, there is a way to press the issue:\nWindows 10 update/download site (EN-GB) https://www.microsoft.com/en-gb/software-download/windows10\nVisit the official Windows 10 download site that allows you to download and install the in-place upgrade tool. This will handle the download and installation of the 20H2 update.\nOne tip for ensuring a seamless upgrade is to disable or suspend BitLocker or any other 3rd party full disk encryption system; particularly if you are upgrading from 1809 or earlier.\nWhat’s new in 20H2? Not much if we are honest and this likely why it marks to start of a new update naming convention.\nRefreshing updates from the Start menu to the Taskbar and more: Source: https://blogs.windows.com/windowsexperience/2020/10/20/whats-new-in-the-windows-10-october-2020-update/\nThe new version of Microsoft Edge by default for new installs – installed for those who upgrade. More updates for commercial and education customers: Simpler device management More secure biometric sign-on Stronger app protection ","permalink":"https://exitcode0.net/posts/how-to-force-the-windows-10-20h2-update/","summary":"Are you still waiting for your Windows 10 computer to receive the 20H2 update? Here is how to force the windows 10 20H2 update. For many, the update may already available the Windows 10 update settings:\nThe 20H2 update when available in the Windows 10 update settings. Microsoft is staggering the over-the-air update as they usually do with new major releases. However if you are tired of hitting ‘check for updates’ in the windows 10 update settings, there is a way to press the issue:","title":"How to force the Windows 10 20H2 update"},{"content":"Here is my quick and easy way to raise the privilage level of a Windows Batch script; allowing you to run your code at an administrator level.\nThis is not a new question and has been asked many times on StackOverflow forums. The best answer I was able to find was the following from dbenham:\nhttps://stackoverflow.com/questions/1894967/how-to-request-administrator-access-inside-a-batch-file/10052222#10052222\nThat being said there are many ways to skin this cat, so I came up with my own method, all be it derivative. The main difference in the following code is that you will always get a UAC prompt in my variant of the code, even if you are currently running it as an account that is a member of the local administrators group:\nUAC prompt invoked! Full Code: ::::::::::::::::::::::::::::::::::::::::: :: Automatically check \u0026amp; get admin rights ::::::::::::::::::::::::::::::::::::::::: @echo off CLS ECHO. ECHO ============================= ECHO Running Admin shell ECHO ============================= :checkPrivileges NET FILE 1\u0026gt;NUL 2\u0026gt;NUL if \u0026#39;%errorlevel%\u0026#39; == \u0026#39;0\u0026#39; ( goto gotPrivileges ) else ( goto getPrivileges ) :getPrivileges if \u0026#39;%1\u0026#39;==\u0026#39;ELEV\u0026#39; (shift \u0026amp; goto gotPrivileges) ECHO. ECHO ************************************** ECHO Invoking UAC for Privilege Escalation ECHO ************************************** setlocal DisableDelayedExpansion set \u0026#34;batchPath=%~0\u0026#34; setlocal EnableDelayedExpansion ECHO Set UAC: CreateObject^(\u0026#34;Shell.Application\u0026#34;^) \u0026gt; \u0026#34;%temp%\\OEgetPrivileges.vbs\u0026#34; ECHO UAC.ShellExecute \u0026#34;!batchPath!\u0026#34;, \u0026#34;ELEV\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;runas\u0026#34;, 1 \u0026gt;\u0026gt; \u0026#34;%temp%\\OEgetPrivileges.vbs\u0026#34; \u0026#34;%temp%\\OEgetPrivileges.vbs\u0026#34; exit /B :gotPrivileges :::::::::::::::::::::::::::: ::START :::::::::::::::::::::::::::: setlocal \u0026amp; pushd . ECHO Hello World, I am your admin: whoami pause Find the code on GitHub: https://github.com/Tombo1001/UACprod\n","permalink":"https://exitcode0.net/posts/how-to-invoke-uac-in-a-windows-batch-script/","summary":"Here is my quick and easy way to raise the privilage level of a Windows Batch script; allowing you to run your code at an administrator level.\nThis is not a new question and has been asked many times on StackOverflow forums. The best answer I was able to find was the following from dbenham:\nhttps://stackoverflow.com/questions/1894967/how-to-request-administrator-access-inside-a-batch-file/10052222#10052222\nThat being said there are many ways to skin this cat, so I came up with my own method, all be it derivative.","title":"How to invoke UAC in a Windows Batch script"},{"content":"A quick Google of the above only seemed to give me answers for the inverse, calling a python script from a bash shell and handing it variables. At least, the first 3 results showed this and I’m probably not alone when it comes to scrolling down past the StackOverflow articles.\nSo I am had to go it alone and the way that I figured out how to call a bash command with variables in Python (3), is a total hack… But it works. This is commonly referred to as ‘getting the job done’ code.\nhttps://xkcd.com/979/ Background Here is some quick background info about what I am trying to do, to help you understand some of my variable names and the particular bash commands I am using.\nI am currently playing around/working on a tool to build easy listening playlist videos with ffmpeg. For millions of reasons, but specifically one (I gave up on figuring out the python library), I want to use ffmpeg in bash, but write everything else my script does is python. Therefore, I need a way to call bash commands from python: easy; and pass variables to that bash command: less easy.\nMaxiumum effort photoshop The Code This is all based on using the ‘Subprocess’ library rather than just the classic ‘os.system’ method. Subprocess lets us do some cool things – technical things – so we must first go ahead and import it (install it with pip3 if required):\nImport subprocess Next, I will define my command which I want to send over to bash for execution, and the variables I want to pass to it (not in that order):\nmy_variable_1: \u0026#34;Goodbye\u0026#34; my_variable_2: \u0026#34;Jupiter\u0026#34; command: (\u0026#34;echo {}\u0026#34;.format(my_variable1, my_variable_2) At this point we can choose to print out the command to the python console to check our syntax:\nprint(command) # optional Now we can send our command to the bash shell and await it’s completion.\nprocess: subprocess.call(command, shell=True) Upon execution of the command, the process variable can be used to check the status of our efforts in bash, allowing us to form a condition for proceeding or returning if we are inside a function:\nif process == 0: print(\u0026#34;we did it, exitcode0 (.net)\u0026#34;) #return else: print(\u0026#34;check your command syntax\u0026#34;) exit(1) The key to passing variables revolves around building our command before calling it with subprocess: “command: (“echo {}”.format(my_variable1, my_variable_2)“\nWith the code above we are currently only checking the status code of the command which we have run. However, it is possible to capture the standard output of the command and bring that back into python for further programming; to do this we need to use subprocess.popen:\ncommand: \u0026#34;ls -la /home/foo\u0026#34; process: subprocess.Popen(command, stdout=subprocess.PIPE) output: process.stdout.read() If we were to print ‘output‘ we should hope to see the standard output of ‘ls -la‘ in the directory ‘/home/foo‘. You could take things further in bash by piping the output to grep before reading it back in with python; an alternative to further string processing in Python.\nMore Python snippets: KASA HOME AUTOMATION WITH IFTTT AND PYTHON WEBHOOKS – https://exitcode0.net/kasa-home-automation-with-ifttt-and-python-webhooks/ PYTHON 3 SSH WITH PARAMIKO – https://exitcode0.net/python-3-ssh-with-paramiko/ ","permalink":"https://exitcode0.net/posts/how-to-call-a-bash-command-with-variables-in-python/","summary":"A quick Google of the above only seemed to give me answers for the inverse, calling a python script from a bash shell and handing it variables. At least, the first 3 results showed this and I’m probably not alone when it comes to scrolling down past the StackOverflow articles.\nSo I am had to go it alone and the way that I figured out how to call a bash command with variables in Python (3), is a total hack… But it works.","title":"How To call a bash command with variables in Python"},{"content":"This is my lean and effective way to monitor your public IP address with Python, specifically Python3. The script – code and GitHub link below – runs on a continuous loop, which I covered in a previous post: here.\nNOTE: this code is reliant on the dig system command – so this essentially only work on a unix based system. I was running this on a Debian install.\nimport os import subprocess import re import time print(\u0026#34;start monitor vpn monitor check\u0026#34;) expected_IP: \u0026#34;0.0.0.0\u0026#34; # ENTER YOUR EXPECTED PUBLIC IPv4 ADDRESS HERE current_IP: subprocess.check_output(\u0026#34;dig +short myip.opendns.com @resolver1.opendns.com\u0026#34;, shell=True) try: while True: if expected_IP in str(current_IP): print(\u0026#34;IPs Match - Things are normal\u0026#34;) else: print(\u0026#34;Current IP: \u0026#34; + str(current_IP) + \u0026#34;\\nIP NOT AS EXPECTED!\u0026#34;) #Code to complete actions called here time.sleep(60) # Change this timer to reduce/increase time between checks (seconds) except KeyboardInterrupt: print(\u0026#34;\\nHard Exit Initiated. Goodbye!\u0026#34;) https://github.com/Tombo1001/PyIPmonitor\nAlternative uses There are a number of use cases for this code, mine was checking if my computer is still connected to a VPN. Monitor my public IP address meant that I could tell if the VPN had disconnected.\nChecking my IP matches my VPN endpoint IP The only thing that was changed here from the current repository code is the print statements – the output just matches the context of the code’s use.\nMore python snippets Here are some links to some of my other python code samples:\nMy python code samples – https://exitcode0.net/code-samples/#python Python3 SSH with paramiko – https://exitcode0.net/python-3-ssh-with-paramiko/ ","permalink":"https://exitcode0.net/posts/monitor-your-public-ip-address-with-python/","summary":"This is my lean and effective way to monitor your public IP address with Python, specifically Python3. The script – code and GitHub link below – runs on a continuous loop, which I covered in a previous post: here.\nNOTE: this code is reliant on the dig system command – so this essentially only work on a unix based system. I was running this on a Debian install.\nimport os import subprocess import re import time print(\u0026#34;start monitor vpn monitor check\u0026#34;) expected_IP: \u0026#34;0.","title":"Monitor your public IP address with Python"},{"content":"I recently migrated my ‘physical’ Windows 10 desktop to a ‘virtual’ machine running under UNRAID (libvert). This is a quick guide on how I was able to use my existing installation on a SSD and pass it through to a VM.\nWhy passthrough a drive to a VM? In some cases, it might be beneficial to start from scratch when creating a Windows 10 VM on UNRAID, creating a new virtual disk and having a fresh install. I chose to take an alternative path because I was contempt with my Windows install as it was. I had my Steam/Epic Games library downloaded onto one of the two disks already in the system and a number of applications installed and configured just the way I like them.\nIn short, being able to preserve my disks in their current state would save me hours of unnecessary reconfiguration.\nSo why not create an image of those disks and place them on the UNRAID array or cache? Well, I plan to have multiple VMs on my UNRAID server (other Physical to Virtual conversions), a number of docker containers, applications and uses for the array which will all be IO heavy operations. Giving my Windows VM an independent disk subsystem will ensure that it never encounters Disk IO bottlenecks. For me, this is a desirable feature.\nMy Windows 10 VM, with it’s two physical disks. The UNRAID prerequisites To pass through a drive to a UNRAID VM, there are a number of prerequisites that must be fulfilled. The following plugin will be needed:\nDlandon’s Unassigned Devices – this is a great plugin that allows a drive that is not a part of the array or cache to be used in a number of ways. It makes it possible to mount, share, or pass through an unassigned device (disk).\nMy two Windows disks marked for ‘pass thru’ Once you have enabled ‘pass thru’ for your disk, it is worth taking a note of the disk IDs, these will be needed for the following stages:\nINTEL_SSDSC2BF240A4L_CVDA411205W12403GN WDC_WD5000LPLX-75ZNTT0_WXP1EC54W1JD UNRAID VM Configuration Now we can look to create our Windows 10 VM, using our passed through Disks (Intel SSD boot drive and Western Digital HDD data drive). Here is a brief list of my settings:\nMachine: Q35-4.2 BIOS: OVMF Hyper-V: No Primary Disk: Manual, /dev/disk/by-id/, sata 2nd Disk: Manual, /dev/disk/by-id/, sata Any other devices, CPU cores and Memory allocations. My Windows 10 VM configuration. NOTE: My physical Windows installation was configured with UEFI (The C: drive was a GPT disk). If you were using legacy/BIOS, to boot into windows (Your C: drive is a BMR disk) you will need to set the UNRAID VM’s BIOS to SeaBIOS.\nThe Result A Windows 10 VM with ‘physical’ disks and in my case a physical GPU too! My Windows instance booted up, reconfigured to account for the change in available memory and CPU, and rebooted again once this was complete.\nI did need to mount the virtIO ISO file and install chipset and network drivers – this also makes Windows observe a VM stop command as a shutdown signal, much like VMware tools.\nMore UNRAID Nuggets https://exitcode0.net/convert-an-esxi-vm-to-unraid-kvm-with-qume-img-convert/ – CONVERT AN ESXI VM TO UNRAID KVM WITH QUME-IMG CONVERT ","permalink":"https://exitcode0.net/posts/how-to-pass-through-a-drive-to-a-unraid-vm/","summary":"I recently migrated my ‘physical’ Windows 10 desktop to a ‘virtual’ machine running under UNRAID (libvert). This is a quick guide on how I was able to use my existing installation on a SSD and pass it through to a VM.\nWhy passthrough a drive to a VM? In some cases, it might be beneficial to start from scratch when creating a Windows 10 VM on UNRAID, creating a new virtual disk and having a fresh install.","title":"How to pass through a drive to a UNRAID VM"},{"content":" I recently moved to a new Windows 10 laptop and experience a number of problems with my Bose SoundLink II, Bluetooth headphones. Fortunately, after some playing around with drivers I was able to solve my problem and this post explains how…\nThe Problem Poor sound quality – with the default drivers, the sound quality was poor and there was a considerable amount of audible noise being delivered to the headphones via the Bluetooth interface. The audio was very lo-fi and the frequency range was dramatically reduced. This resulted in a very thin, tinny sound.\nMic feedback loop – the Bose Soundlink headphones have a built-in mic, meaning that you can use it as a mic enabled headset for calls. With the default drivers which Windows was allocating to the headphones, I could hear a constant mic feedback loop whilst there was active audio output from the system. It was possible to hear yourself tapping on your keyboard through your headphones.\nThe Solution Connect your headphones to the laptop via Bluetooth. Open device manager (devmgmt.msc) as an administrator. Open the properties of the Bluetooth Audio device, go to the Driver tab: My headphones are called ‘KeepYourVinyl’ 4. Update Driver and Browse my computer for driver software. Pick from a list of available drivers on my computer.\n5. Find and select ‘Microsoft Bluetooth A2dp Source’ and proceed with the installation.\nDriver selection 6. Your audio device will likely stop working at this point and you will receive a dialog prompting you to reboot to complete the driver setup.\nFull driver details After a reboot, my headphones connected o the laptop and audio was as you would expect! Audio quality is now on par with that when connecting to an iPhone or Android device.\n","permalink":"https://exitcode0.net/posts/fixed-bose-bluetooth-headphones-with-windows-10/","summary":"I recently moved to a new Windows 10 laptop and experience a number of problems with my Bose SoundLink II, Bluetooth headphones. Fortunately, after some playing around with drivers I was able to solve my problem and this post explains how…\nThe Problem Poor sound quality – with the default drivers, the sound quality was poor and there was a considerable amount of audible noise being delivered to the headphones via the Bluetooth interface.","title":"[FIXED] Bose Bluetooth headphones with Windows 10"},{"content":"Are you still waiting for your Windows 10 computer to receive the 2004 update? Eager to play with WSL2 like I was? Here is how to force the windows 10 2004 update.\nMicrosoft is staggering the over-the-air update as they usually do with new major releases. However if you are tired of hitting ‘check for updates’ in the windows 10 update settings, there is a way to press the issue:\nhttps://www.microsoft.com/en-gb/software-download/windows10\nVisit the official Windows 10 download site allows you to download and install the in-place upgrade tool. This will handle the download and installation of the 2004 update.\nOne tip for ensuring a seamless upgrade is to disable or suspend BitLocker or any other 3rd party full disk encryption systems.\nWhat’s new in 2004? Well, there is definitely more to come on this from me, so consider dropping a bookmark or joining my notification list (bell icon bottom left). I am very keen to test WSL2 with Ubuntu and Kali Linux. Watch this space!\n","permalink":"https://exitcode0.net/posts/how-to-force-the-windows-10-2004-update/","summary":"Are you still waiting for your Windows 10 computer to receive the 2004 update? Eager to play with WSL2 like I was? Here is how to force the windows 10 2004 update.\nMicrosoft is staggering the over-the-air update as they usually do with new major releases. However if you are tired of hitting ‘check for updates’ in the windows 10 update settings, there is a way to press the issue:","title":"How to force the Windows 10 2004 update"},{"content":"Error Correcting Code (ECC) RAM is a variation of coputer memory which helps to ilimintate data curruption or ‘bit rot’, but it is not always imediately apparent if your system memory is running in ECC mode; here is a quick guide on how to check if your system memory is running in ECC mode. This guide covers Windows and Linux systems, but please ensure that if you are running either system in a virtual machine configuration that this command is ran on the host machine.\nLinux – Check if ECC Mode is enabled To check if a system’s memory is running in ECC mode on a Linux system, open up the terminal (elevation may be required in some instances) and type the following command:\ndmidecode -t memory or\ndmidecode --type memory Along with other memory statistics and information, your will receive the following item line if ECC is active and the mode in which it is running:\nError Correction Type: Multi-bit ECC Furthermore, the dmidecode tool is very useful for learning more about the other areas of your system from the Linux terminal. You can find a great breakdown of some of its features here: https://www.redhat.com/sysadmin/linux-tools-dmidecode\nWindows – Check if ECC Mode is enabled Open the windows command prompt and enter the following wmic command:\nwmic memphysical get memoryerrorcorrection Using CMD to check if ECC is enabled\nNote: if you are struggling to run WMIC command in your command prompt and receiving an error that ‘wmic is not recognized as an internal or external command‘, consider checking out this post where I demonstrate how to fix that: https://exitcode0.net/adding-wmic-command-to-the-windows-path/\nAs you can see from the example above, my Windows laptop returned a value of 3, but what does this mean?\nValue Meaning 0 (0x0) Reserved 1 (0x1) Other 2 (0x2) Unknown 3 (0x3) None 4 (0x4) Parity 5 (0x5) Single-bit ECC 6 (0x6) Multi-bit ECC 7 (0x7) CRC Unsurprisingly my laptop is not running any kind of ECC memory and returns None.\n","permalink":"https://exitcode0.net/posts/how-to-check-if-ram-is-running-in-ecc-mode/","summary":"Error Correcting Code (ECC) RAM is a variation of coputer memory which helps to ilimintate data curruption or ‘bit rot’, but it is not always imediately apparent if your system memory is running in ECC mode; here is a quick guide on how to check if your system memory is running in ECC mode. This guide covers Windows and Linux systems, but please ensure that if you are running either system in a virtual machine configuration that this command is ran on the host machine.","title":"How to check if RAM is running in ECC mode"},{"content":"wmic is not recognized as an internal or external command – I was quite shocked to find that a command I use on a very regular basis was not working on a fresh installation of Windows 10 1909 on my trust old ThinkPad.\nI don’t want to spend hours trying to find out why this was not correct in my system path, but instead, I fixed it and spent the time sharing how to fix the problem.\nAdding WBEM to the Windows path Adding the WMIC command to the Windows path is a very simple process (administrator privileges required) and is completed as follows:\nFind the Advanced System Settings item in your start menu search:\nView advanced system settings Open the Advanced tab and select the Environment Variables button:\nOn the advanced tab, open Environment Variables Under System variables, highlight the Path variable and select the edit button:\nSelect the Path system variable and Edit… Now we need to added the following line to our Path variable:\nC:\\Windows\\System32\\wbem\\ We can do that as follows, ensuring to include the trailing backslash as this is a folder:\nOnce you have clicked OK, closing all the windows we have opened so far, you need to REBOOT your computer to apply the change.\nOnce you have completed the reboot, open up the command prompt, and test the WMIC command:\nLearn more about this WMIC command from this link.\n","permalink":"https://exitcode0.net/posts/adding-wmic-command-to-the-windows-path/","summary":"wmic is not recognized as an internal or external command – I was quite shocked to find that a command I use on a very regular basis was not working on a fresh installation of Windows 10 1909 on my trust old ThinkPad.\nI don’t want to spend hours trying to find out why this was not correct in my system path, but instead, I fixed it and spent the time sharing how to fix the problem.","title":"Adding WMIC command to the Windows path"},{"content":"The Pi enthusiasts have been waiting for official USB boot support on the Raspberry Pi for what feels like a lifetime, but finally it is on the horizon. In this post I will explain how to make your Raspberry Pi boot from USB.\nWARNING: Although this is official, it is still in beta testing, so rock-solid stability is far from certain. Learn more about this beta release here.\nWhy should I make my Raspberry Pi boot from USB? Firstly let me address why you would want to do this and what makes many people relieved that the feature is on the horizon.\nA more robust, long-lasting boot device. Micro SD cards, particularly high-quality cards, have come a long way and the days of a dead micro SD card seem to be dwindling. However, the small flash cards have a much shorter read/write life expectancy, and are not really designed to be used as a bootable OS media. Being able to use a SATA (over USB) device which was designed for the typical IO expected from an active operating system will ensure a long-lasting Pi setup.\n32GB Sandisk Card Price over time – Camel Camel Camel SATA based storage is cheaper per GB. Prices of micro SD cards have dropped significantly over the years, but SATA based storage is till better value for money at larger capacities, as well as the reliability benefits.\nHow do I make my Raspberry Pi boot from USB? Difficulty: Medium\nThere are a number of steps involved in this process and it will require you to bounce some files between devices that can be done on the Pi or by connecting the two bootable devices to another computer – this method won’t be covered in this guide.\nRequirements: Raspberry Pi https://amzn.to/3eiF2XT Pi 4 Starter kit The latest version of Raspbian Buster https://www.raspberrypi.org/downloads/raspbian/ USB hard drive/flash drive https://amzn.to/2Twk4wm 256GB Sandisk USB drive. Micro SD Card (still required at this point in BETA) https://amzn.to/3ggknoX 32GB SanDisk Micro SD card Burn the Rasbian ISO to your SD card using something like Etcher: https://www.balena.io/etcher/ and plug it into the Pi.\nProcess (as of 23-05-2020): The breakdown of the process:\nChange the content of the EEPROM on the Raspberry Pi4 and set the boot device to USB. Copy files from the boot directory of the SD card to the boot directory of the USB boot device. We must start by updating the Raspbian installation:\nsudo apt update -y \u0026amp;\u0026amp; sudo apt upgrade -y \u0026amp;\u0026amp; sudo rpi-update -y Now that we are up to date, we must reboot before installing rpi-eeprom:\nsudo reboot now sudo apt install rpi-eeprom Now we have to change the path of the Pi’s EEPROM firmware- this is done by moving from the critical channel to beta channel:\nsudo nano /etc/default/rpi-eeprom-update Replace ‘critical‘ with ‘beta‘ followed by ‘ctrl+x, y‘ to save and exit.\nNow we can program the EEPROM as follows:\nsudo rpi-eeprom-update -d -f /lib/firmware/raspberrypi/bootloader/beta/pieeprom-2020-05-15.bin Followed by a reboot:\nsudo reboot now Once the reboot is complete, we can check the bootloader version as follows:\nvcgencmd bootloader_version If we run the follow…\nvcgencmd bootloader_config … we should see ‘BOOTORDER=0xF41. 4‘\n‘.4’ means that USB boot is active; ‘.1’ means that SD boot is active.\nAt this point the Pi is ready, but the USB media is not. All that is left is to prepare the USB bootable media with the necessary files form our SD card installation.\nStart by plugging in the USB getting rasbian flashed to your USB device in the same manner as the SD card – personally I use Etchor: https://www.balena.io/etcher/\nNow we need to copy all ‘.elf’ and ‘.dat’ files from the boot directory of our SD card to the boot directory on the USB. Let’s start by mounting the USB drive.\nsudo mkdir /mnt/usbdisk sudo mount /dev/sd1 /mnt/usbdisk Now that the USB drive is mounted, we can copy the required files:\nsudo cp /boot/*.elf /mnt/usbdisk sudo cp /boot/*.dat /mnt/usbdisk We are done!\nNow we can now power down (sudo shutdown now), remove the SD card, then power back up and check that we have successfully booted from out USB!\nMore Raspberry Pi Antics: [SOLVED] – RUNNING DOCKER ON A RASPBERRY PI 4 – https://exitcode0.net/solved-running-docker-on-a-raspberry-pi-4/ USING A DHT11 SENSOR WITH A RASPBERRY PI – https://exitcode0.net/using-a-dht11-sensor-with-a-raspberry-pi/ CREATING A LOCAL DNS SERVER WITH PI HOLE – https://exitcode0.net/creating-a-local-dns-server-with-pi-hole/ ","permalink":"https://exitcode0.net/posts/how-to-raspberry-pi-boot-from-usb/","summary":"The Pi enthusiasts have been waiting for official USB boot support on the Raspberry Pi for what feels like a lifetime, but finally it is on the horizon. In this post I will explain how to make your Raspberry Pi boot from USB.\nWARNING: Although this is official, it is still in beta testing, so rock-solid stability is far from certain. Learn more about this beta release here.\nWhy should I make my Raspberry Pi boot from USB?","title":"How To: Raspberry Pi boot from USB"},{"content":" I’ve spent enough time building VMs in vSphere to know that the first few minutes between starting the VM and getting VMware Tools installed is agonising. Similar to my ‘Copy Paste With Powershell Sendkeys‘ script, I have another tool written in AutoIT which sends defined keyboard strokes to the system. So this is not exactly ‘copy paste in vSphere’, but it achieves the same goal.\nIf you actually want to enable clipboard sync between host and VM, you can follow this guide here: Enable Copy and Paste Operations Between the Guest Operating System and Remote Console. This is going to need VMware Tools installed on the host though.\nCopy Paste in vSphere with AutoIT Yes this tool is written in AutoIT, apparently my favorite script kiddie language from the 201X years, so Windows only on this one – sorry. Quite a simple bit of code, we prompt for the text we want to put on the ‘clipboard’ announce the waiting period before sending the keystrokes (allowing the user time to place the cursor in the VM) and then fire off the keystrokes.\n#include \u0026lt;MsgBoxConstants.au3\u0026gt; Local $sAnswer: InputBox(\u0026#34;Send Keys\u0026#34;, \u0026#34;Enter the text you want to send:\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, _ - 1, -1, 0, 0) Local $iTimeout: 7 Local $iSecUntil: 10 MsgBox($MB_SYSTEMMODAL, \u0026#34;Send Keys\u0026#34;, \u0026#34;Your keys will be sent in \u0026#34; \u0026amp; $iSecUntil \u0026amp; \u0026#34; seconds or select the OK button to start a 3 second countdown.\u0026#34;, $iTimeout) Sleep(3000) Send($sAnswer) Exit Code Enhancements? This tool has got me out of a number of binds where the clipboard has failed me – usually some form of remote session. I don’t trust myself to type things accurately in high-pressure situations, especially when character obfuscation is in play.\nAt the moment this is a one-shot executable; you run it once and then the process terminates. One handy feature/enhancement would be to have it run perpetually and listen for a hotkey (definable) on the host triggering the main function of the code to be called.\nThat being said, there are lots of other things I am working on right now, and digging back into an AutoIT project from 2016 is not very high on the priorities list. Perhaps one for a rainy day…\nMore AutoIT ramblings: Automatically pause Spotify with the 3CX client – https://exitcode0.net/automatically-pause-spotify-with-the-3cx-client/ ","permalink":"https://exitcode0.net/posts/copy-paste-in-vsphere-before-installing-vmware-tools/","summary":"I’ve spent enough time building VMs in vSphere to know that the first few minutes between starting the VM and getting VMware Tools installed is agonising. Similar to my ‘Copy Paste With Powershell Sendkeys‘ script, I have another tool written in AutoIT which sends defined keyboard strokes to the system. So this is not exactly ‘copy paste in vSphere’, but it achieves the same goal.\nIf you actually want to enable clipboard sync between host and VM, you can follow this guide here: Enable Copy and Paste Operations Between the Guest Operating System and Remote Console.","title":"Copy Paste in vSphere before installing VMware tools"},{"content":"This is a really old script that I just found in my archive, but it still works all the same. The goal was to write a small script that would automatically pause Spotify with the 3CX client when there was an inbound call.\nThe script is written in AutoIT, meaning that it is a Windows-only solution. The Windows 3CX CTI client allows you to run a particular executable when an inbound call is received, i.e. when the phone rings. I wanted to pause the manic euro beats so that I could answer my phone and here the caller and uphold professionalism.\nThe AutoIT Code ; ***************************************************************************** ; Auto Pause Spotify ; Pauses spotify when called ; Author: Tom Cocking - www.tomcocking.com ; ***************************************************************************** #include \u0026lt;MsgBoxConstants.au3\u0026gt; SpotifyPause() Func SpotifyPause() ; Retrieve the handle of the Spotify window using the classname of SpotifyMainWindow. Local $hWnd: WinGetHandle(\u0026#34;[CLASS:SpotifyMainWindow]\u0026#34;) WinActivate($hWnd) Local $sText: WinGetTitle(\u0026#34;[ACTIVE]\u0026#34;) If $sText: \u0026#34;Spotify\u0026#34; Then Exit Else Send(\u0026#34;{SPACE}\u0026#34;) Exit EndIf If @error Then ConsoleWrite(\u0026#34;An error occurred when trying to retrieve the window handle of Spotify\u0026#34;) Exit EndIf EndFunc As you can see, the code is very straight forward and to the point; we are asking 3CX to run the compiled exe file every time there is an inbound call, fast and reliable code is essential. It is also really important that you conclude you executable with an exit statement to prevent stale processes lingering on your system.\nOne subtle thing that this script does is check if the Spotify instance has a song/artist data in the window title (which would signal that music was playing) before trying to activate the window and pause playback. One thing to mention – it would be a lot easier if there was a global hotkey to play/pause music, particularly one that Spotify adhered to. This would make this script 3 lines long (not counting comments).\nEditors Note: I fully intend on uploading this to a public Github repository so that you can clone the repo, rather than just copying/pasting.\nConfigure the 3CX softphone client Once you have compiled your script into an executable file, you can find the setting in the 3CX client to run an exe upon an incoming call. So when you have a call from the boss, you can rest easy knowing that your secret adoration for Taylor Swift remains undisclosed.\nplease recognise my sarcasm\nYou can find more useful AutoIT snippets over on my Code Samples Page.\n","permalink":"https://exitcode0.net/posts/automatically-pause-spotify-with-the-3cx-client/","summary":"This is a really old script that I just found in my archive, but it still works all the same. The goal was to write a small script that would automatically pause Spotify with the 3CX client when there was an inbound call.\nThe script is written in AutoIT, meaning that it is a Windows-only solution. The Windows 3CX CTI client allows you to run a particular executable when an inbound call is received, i.","title":"Automatically pause Spotify with the 3CX client"},{"content":"I have a number of projects which I am currently working on which usually involve a raspberry pi and so other TCP enabled object. One project, in particular, is to control a KVM server with buttons connected to a raspberry pi (follow on social media or RSS for that future post). I have been using Python 3 SSH with Paramiko to send commands KVM server from the pi. Here is how I do that…\nWhat is Paramiko? Paramiko is a Python (2.7, 3.4+) implementation of the SSHv2 protocol, providing both client and server functionality. While it leverages a Python C extension for low level cryptography, Paramiko itself is a pure Python interface around SSH networking concepts. – http://www.paramiko.org/\nTLDR – Paramiko is a python library which lets you do SSH things.\nThe how-to: Python 3 SSH with Paramiko For the purpose of this blog, I will be focusing on using paramiko to make client connections. Let’s include this library and dig into sending some dangerous commands to Linux boxes.\nimport paramiko I have been using a function to complete my SSH command and passing my SSH variables when calling that function. Lets start with that funciton:\ndef func_do_ssh_Stuff(address, usr, pwd, command): try: print(\u0026#34;ssh \u0026#34; + usr + \u0026#34;@\u0026#34; + address + \u0026#34;, running : \u0026#34; + command) client: paramiko.SSHClient() client.load_system_host_keys() # this loads any local ssh keys client.set_missing_host_key_policy(paramiko.AutoAddPolicy()) client.connect(address, username=usr, password=pwd) _, ss_stdout, ss_stderr: client.exec_command(command) r_out, r_err: ss_stdout.readlines(), ss_stderr.read() print(r_err) if len(r_err) \u0026gt; 5: print(r_err) else: print(r_out) client.close() except IOError: print(\u0026#34;.. host \u0026#34; + address + \u0026#34; is not up\u0026#34;) return \u0026#34;host not up\u0026#34;, \u0026#34;host not up\u0026#34; What’s going on here? The function is expecting to be handed a username, password (or none if you plan to use key based authentication (recommended)), host address and the shell command to run once the connection is established.\nParamiko checks for local keys on the system which it can use when connecting to the host, then attempts a connection. Once the connection is established the command we passed to the function will be executed.\nFinally, we print the output or error returned by the execution of that command. This is all encased in a ‘try, catch’ which will inform us if the SSH target is offline.\nCalling the Paramiko SSH function Now that we have our SSH function declared, we can call it:\nfunc_do_ssh_Stuff(\u0026#34;192.168.0.33\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;echo Hello World!\u0026#34;) Note: you will need to change the IP to suit and add a password (“”) if required.\nI would highly recommend using key-based authentication on this one, otherwise, it will require you to hard code SSH passwords into your scripts… not a great idea.\nAnd now that you can SSH from a Pi (or anything else which runs python), the world is your oyster!\nMore wonderful Python things: How To: Python infinite loops with while true – https://exitcode0.net/how-to-python-infinite-loops-with-while-true/ Kasa home automation with IFTTT and python webooks – https://exitcode0.net/kasa-home-automation-with-ifttt-and-python-webooks/ Getting started with Python 3 – a beginner’s cheat sheet – https://exitcode0.net/getting-started-with-python-3-a-beginners-cheat-sheet/ ","permalink":"https://exitcode0.net/posts/python-3-ssh-with-paramiko/","summary":"I have a number of projects which I am currently working on which usually involve a raspberry pi and so other TCP enabled object. One project, in particular, is to control a KVM server with buttons connected to a raspberry pi (follow on social media or RSS for that future post). I have been using Python 3 SSH with Paramiko to send commands KVM server from the pi. Here is how I do that…","title":"Python 3 SSH with Paramiko"},{"content":"Here is a quick guide on how to create an infinite loop in python using a ‘while true’ statement. There are number of reason that you might want to implement this; a great use case would be outputting a fluctuating variable to the terminal such as a temperature reading from a sensor.\nloops that make your brain hurt A ‘while true’ statement allows us to run a sequence of code until a particular condition is met. In order to make that sequence of code run in an infinite loop, we can set the condition to be one that is impossible to reach. Better still, we can simply omit the condition altogether to ensure that the while true loop never ends.\nwhile True: print(\u0026#34;The current time is: %s\u0026#34; % strTimeNow) time.sleep(5) In cases where it would be useful to exit that loop if a given condition is met or exception is reached, we can encase our ‘while true’ statement with a ‘try except’ statement.\ntry: while True: print(\u0026#34;The current time is: %s\u0026#34; % strTimeNow) time.sleep(5) except KeyboardInterrupt: print(\u0026#34;Hard Exit Initiated. Goodbye!\u0026#34;) There are a number of exception that can be handled by a try statement, I have chose to use the KeyboardInterupt (Ctrl+C) clause in this example.\nYou can learn more about exception handling here: https://docs.python.org/3/tutorial/errors.html#handling-exceptions\nMore Python Tips and tricks: More Python code samples and examples – https://exitcode0.net/code-samples/#python Kasa home automation with IFTTT and python webooks – https://exitcode0.net/kasa-home-automation-with-ifttt-and-python-webooks/ Getting started with Python 3 – a beginner’s cheat sheet – https://exitcode0.net/getting-started-with-python-3-a-beginners-cheat-sheet/ ","permalink":"https://exitcode0.net/posts/how-to-python-infinite-loops-with-while-true/","summary":"Here is a quick guide on how to create an infinite loop in python using a ‘while true’ statement. There are number of reason that you might want to implement this; a great use case would be outputting a fluctuating variable to the terminal such as a temperature reading from a sensor.\nloops that make your brain hurt A ‘while true’ statement allows us to run a sequence of code until a particular condition is met.","title":"How To: Python infinite loops with while true"},{"content":" note to self: never use LVM again… This title has far too many acronyms so I will start by clarifying the problem that this post aims to solve. I have a Ubuntu 18.04 server running LVM (Logical Volume Manager). My primary LV (Logical Volume) was ‘full’ but was only using 4GB of a 29GB PV (Physical Volume). So if you are suffering the same issue, here are the commands you need to fix this problem.\ndf -h output: Filesystem 1K-blocks Used Available Use% Mounted on /dev/mapper/i-hate-lvm 5166504 5150120 0 100% / df -h shows the space availble in mounted partitions This is fictitious data, but this is reflective of the fact that the root filesystem is full. This caused me a LOT of headaches and I needed to free up some free space on that partition before I could run any of the upcoming commands to expand the LV. Ways to free up space include:\nsudo apt clean sudo apt autoremove -y I was running snap and use the following command to remove older versions of a snap application:\nsnap remove \u0026#34;$snapname\u0026#34; --revision=\u0026#34;$revision\u0026#34; Finding the available PV space Now that we have a tiny bit of space on ‘/’ we cab start to work on expanding the available space in the LV. First, we need to find out the available space in the PV which is where the LV ‘lives’.\nvgdisplay the value we are looking for specifically is:\nFree PE / Size 2319 / 9.06 GiB As per the example above, we have just over 2GB available. I want to go head and give all of that space to my one and only LV.\nExtending LV with lvextend so finally, the command you were looking for:\nsudo lvextend -l +2319 /dev/i-hate-lvm Adjust the ‘2319’ value and the ‘/dev/’ path to suit your required level of expansion and the LVM volume.\nExtending an LV filesystem with resize2fs Finally, now that the LV is larger, we need to expand the filesystem on the volume to make the newly acquired space available for use. Fortunately this another really simple command:\nresize2fs /dev/mapper/i-hate-lvm Pay attention to the fact that I am running this command against ‘/dev/mapper/‘.\nRun the ‘df -h’ command and again and you should now have ALL OF THE FREE SPACE!\nThis was a massive PITA for me, an inexperienced LVM user. Consider giving this post a bookmark or a share to help out a fellow LVM sufferer in a moment of need.\nMore useful Ubuntu rangling posts: How to upgrade Ubuntu from 18.04 to 20.04 – https://exitcode0.net/how-to-upgrade-ubuntu-from-18-04-to-20-04/ Setup the Nextcloud Snap Package with HTTPS and a separate data path – https://exitcode0.net/setup-the-nextcloud-snap-package-with-https-and-a-separate-data-path/ ","permalink":"https://exitcode0.net/posts/how-to-expand-an-lvm-lv-to-use-all-space-on-the-pv/","summary":"note to self: never use LVM again… This title has far too many acronyms so I will start by clarifying the problem that this post aims to solve. I have a Ubuntu 18.04 server running LVM (Logical Volume Manager). My primary LV (Logical Volume) was ‘full’ but was only using 4GB of a 29GB PV (Physical Volume). So if you are suffering the same issue, here are the commands you need to fix this problem.","title":"How to expand an LVM LV to use all space on the PV"},{"content":"Ubuntu 20.04 LTS full release is almost upon, with a scheduled release date of April 23rd 2020 (2 days as of writing this). It is packed with lots of great new features and upgrades and the Linux community is very excited to get a hold of it. If you are looking to upgrade ubuntu, there is a quick guide down below.\nIf like me you are already running Ubuntu 18.04 LTS then you might be interested to learn how to upgrade your current version of the distro to 20.04 LTS.\nFourtnately it is relatively simple and requires a few terminal commands.\nPrerequisites It is really important to take a backup of any important data and only consider the upgrade in the first place if you are happy that your current software suite is supported in the new version of the OS. If you are not sure about this then I would advise building 20.04 in a VM for testing before committing to the upgrade.\nStarting the upgrade It would be a good idea to make sure that all your current packages in Ubuntu 18.04 are up to date before jumping to 20.04:\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade \u0026amp;\u0026amp; apt-get dist-upgrade follow that up with a reboot:\nsudo reboot now Now we can install the Ubuntu release upgrade tool:\nsudo apt install update-manager-core Now it it time to upgrade to the latest LTS:\nsudo do-release-upgrade The uprade will take quite a bit of time and require some minor intervention for confirmation during the upgrade. Once it is complete, reboot your system and move onto the final stage.\nIt is important to note that if you are trying to upgrade to 20.04 before the final release date, you will get the following message and tip:\nChecking for a new Ubuntu release There is no development version of an LTS available. To upgrade to the latest non-LTS develoment release set Prompt=normal in /etc/update-manager/release-upgrades Changing ‘Prompt’ to normal from LTS will allow your system to jump to developer builds when running the ‘do-release-upgrade’. Once again, if this is a mission-critical system, consider your decision to upgrade to development versions of your OS carefully.\nIf you play fast and loose, you can always ugrade to a developer version:\nsudo do-release-upgrade -d Final step Now it is time to verify the version of Ubuntu you are running:\nlsb_release -a If you want to share your version with the world, consider using neofetch as a way to make your terminal flex that bit more exciting:\nsudo apt install neofetch neofetch Feel free to post your neofect flex in the comments below!\nMore useful Ubuntu tips and tricks Ubuntu 19.10 – How to upgrade python 2.7 to python 3.7 – [http://Ubuntu 19.10 – How to upgrade python 2.7 to python 3.7](http://Ubuntu 19.10 – How to upgrade python 2.7 to python 3.7 \u0026ldquo;http://Ubuntu 19.10 – How to upgrade python 2.7 to python 3.7\u0026rdquo;)\n(this will probably work on 20.04 too!) ","permalink":"https://exitcode0.net/posts/how-to-upgrade-ubuntu-from-18-04-to-20-04/","summary":"Ubuntu 20.04 LTS full release is almost upon, with a scheduled release date of April 23rd 2020 (2 days as of writing this). It is packed with lots of great new features and upgrades and the Linux community is very excited to get a hold of it. If you are looking to upgrade ubuntu, there is a quick guide down below.\nIf like me you are already running Ubuntu 18.04 LTS then you might be interested to learn how to upgrade your current version of the distro to 20.","title":"How to upgrade Ubuntu from 18.04 to 20.04"},{"content":" https://snapcraft.io/nextcloud I have been using the Nextcloud snap package for a long time, but some recent system changes in my colocation facility mean that it is time for a big upgrade. Rather than migrate my existing Nextcloud instance, I have decided to build a new install and here are my goals for the new installation:\nUse lets encrypt to have automatic and valid HTTPS certificates Have the data path for Nextcloud on a HDD and the rest of the host OS on an SSD. Host OS Details For this setup, I am using Ubuntu 18.04 installed on a 30GB SSD backed partition. I then have a 1TB disk attached and mounted to /media.\nStep 1 – How to install Nextcloud Snap Package Snap is simple and effective, this one command gives you a basic install of Nextcloud (assuming that you have your DNS records and firewall rules in place at this point):\nsudo snap install nextcloud Do not visit the new site in your browser yet!\nStep 2 – Change the data directory to use another disk partition You must make sure that your secondary drive is mounted in /media or /mnt – as described in the Nextcloud snap wiki: https://github.com/nextcloud/nextcloud-snap/blob/master/README.md\nWhen you have you drive mounted, you will need to allow the snap instance to access ‘removable media’:\nsudo snap connect nextcloud:removable-media sudo chown -R root:root /media/\u0026lt;my mount point\u0026gt; Now we need to edit the Nextcloud config to match this data directory:\nsudo nano /var/snap/nextcloud/current/nextcloud/config/autoconfig.php Here we must change the following line to reflect our data path:\n// ... \u0026#39;directory\u0026#39; =\u0026gt; \u0026#39;/media/\u0026lt;my mount point\u0026gt;\u0026#39;, // ... Once you have exited and saves your changes to the config file, it is time to restart the service:\nsudo snap restart nextcloud.php-fpm Now it is safe to visit your Nextcloud URL!\nAt this point, you will be visiting a HTTP page and will be asked to create your default admin Nextcloud user. Feel free to do this.\nStep 3 – Enabling Lets Encrypt HTTPS for the Nextcloud Snap Package We now have a function Nextcloud server, but it is not secure. We need to enable HTTPS and generate a certificate with Lets Encrypt. Again, assuming that your DNS records and firewall rules are in order:\nsudo nextcloud.enable-https lets-encrypt Complete the steps by entering your recovery email address and the domain of the Nextcloud server and the tool will generate and apply your certificate. You only need to this once; Lets Encrypt handles auto-renewal.\nAt this point, you can refresh your browser and you will be auto-forward to the HTTPS version of your new Nextcloud sever.\nOther useful Nextcloud tutorials: Backup Nextcloud with RCLONE – https://exitcode0.net/backup-nextcloud-with-rclone/ ","permalink":"https://exitcode0.net/posts/setup-the-nextcloud-snap-package-with-https-and-a-separate-data-path/","summary":"https://snapcraft.io/nextcloud I have been using the Nextcloud snap package for a long time, but some recent system changes in my colocation facility mean that it is time for a big upgrade. Rather than migrate my existing Nextcloud instance, I have decided to build a new install and here are my goals for the new installation:\nUse lets encrypt to have automatic and valid HTTPS certificates Have the data path for Nextcloud on a HDD and the rest of the host OS on an SSD.","title":"Setup the Nextcloud Snap Package with HTTPS and a separate data path"},{"content":"There might be a number of reasons to want to prevent your windows screensavers and lock-screens from engaging and in some cases (no doubt yours if you have hit this article from a search engine) local policy on the machine preventing you from changing these settings.\nPowershell lets us work around this problem and prevent the machine from locking or activating a screensaver.\nPrevent Screensavers and Lock-screens The bulk of the code below has been lifted from this great write up: https://dmitrysotnikov.wordpress.com/2009/06/29/prevent-desktop-lock-or-screensaver-with-powershell/ I just took things one step further by adding an infinite loop. My version of this code will run indefinitely.\nAffectionately named: PScaffeine The following code sends a relatively harmless (for most cases – change where appropriate) key to the system emulating a keypress on a human interface device:\n#powershell caffeine param($minutes: 29) $myshell: New-Object -com \u0026#34;WScript.Shell\u0026#34; Write-Host \u0026#34;Pouring out fresh coffee...\u0026#34; while($true){ for ($i: 0; $i -lt $minutes; $i++) { Start-Sleep -s 60 $myshell.sendkeys(\u0026#39;{NUMLOCK}\u0026#39;) $myshell.sendkeys(\u0026#39;{NUMLOCK}\u0026#39;) } Write-Host \u0026#34;*Takes another sip*\u0026#34; } Write-Host \u0026#34;Caffeine has worn off...\u0026#34; Change the $minutes parameter to 1 minute less than your systems lock/screensaver start initiation time.\nCombine this with a scheduled task if you want this to run on startup or system login.\nMore useful code snippets ‘Copy, Paste’ With Powershell Sendkeys – https://exitcode0.net/copy-paste-with-powershell-sendkeys/ Powershell – Checking the Language Mode – https://exitcode0.net/powershell-checking-the-language-mode/ Modifying Windows shortcuts is Powershell – https://exitcode0.net/modifying-windows-shortcuts-is-powershell/ All other codes samples – https://exitcode0.net/code-samples/ ","permalink":"https://exitcode0.net/posts/prevent-screensavers-and-lock-screens-with-powershell/","summary":"There might be a number of reasons to want to prevent your windows screensavers and lock-screens from engaging and in some cases (no doubt yours if you have hit this article from a search engine) local policy on the machine preventing you from changing these settings.\nPowershell lets us work around this problem and prevent the machine from locking or activating a screensaver.\nPrevent Screensavers and Lock-screens The bulk of the code below has been lifted from this great write up: https://dmitrysotnikov.","title":"Prevent Screensavers and Lock-screens with Powershell"},{"content":"I’ve made previous posts about backing cloud services with RCLONE – Backup Google Photos with Rclone – and this post will detail how to backup a self-hosted cloud service, Nextcloud.\nGet started with Nextcloud There are a lot of benefits to running your own private ‘cloud’ storage system, but for me, my top concern is privacy and Nextcloud gives me the ability to take accountability for my own data. With this in mind, I am conscious of my Nextcloud server going offline or suffering data loss, so I want a way to create 1 or more backups of my datacentre hosted, Nextcloud data on my home NAS.\nMy home NAS is running Linux, UNRAID to be precise. It would be possible to run the Nextcloud client on a windows or linux desktop to sync all of the files to a machine. However, RCLONE allows you to copy the data from Nextcloud over a webdav connection.\nHow to configure RCLONE Nextcloud backups Let’s start by running the RCLONE configuration tool:\nrclone config Then use the following options:\nn) New remote -- name: MyNextcloudBackup -- type: webdav -- url: https://\u0026lt;your nextcloud server url\u0026gt;/remote.php/webdav/ -- vendor: nextcloud -- user: \u0026lt;your nextcloud username\u0026gt; -- pass: \u0026lt;your nextcloud user password\u0026gt; -- bearer_token: Remote config This will encrypt the password so that it is not stored in plaintext.\nNow we can run the copy to the UNRAID server:\nrclone copy -P MyNextcloudBackup:\u0026lt;folder name\u0026gt; /mnt/user/\u0026lt;share and path on your UNRAID server\u0026gt; If you leave blank then it will download all folder in your nextcloud account.\n-P Will show you the progress of the copy job.\nYou can take this one step further and run this on a schedule using the user scripts plugin: https://forums.unraid.net/topic/48286-plugin-ca-user-scripts/\nOther useful posts: Backup Google Photos with Rclone – https://exitcode0.net/backup-google-photos-with-rclone/ Kali Linux – How to upgrade python 2.7 to python 3.7 – https://exitcode0.net/kali-linux-how-to-upgrade-python-2-7-to-python-3-7/ Switching to a Linux laptop – https://exitcode0.net/switching-to-a-linux-laptop/ I’m working hard to bring as much useful content to this blog as I can. You can support this site by sharing pages or posts and I will be forever indebted to those who do!\n","permalink":"https://exitcode0.net/posts/backup-nextcloud-with-rclone/","summary":"I’ve made previous posts about backing cloud services with RCLONE – Backup Google Photos with Rclone – and this post will detail how to backup a self-hosted cloud service, Nextcloud.\nGet started with Nextcloud There are a lot of benefits to running your own private ‘cloud’ storage system, but for me, my top concern is privacy and Nextcloud gives me the ability to take accountability for my own data. With this in mind, I am conscious of my Nextcloud server going offline or suffering data loss, so I want a way to create 1 or more backups of my datacentre hosted, Nextcloud data on my home NAS.","title":"Backup Nextcloud with RCLONE"},{"content":" Here’s a quick guide on how to convert ESXi VMs to and UNRAID KVM virtual machine – it’s a simple process but a minefield to navigate on searches and forums.\nI currently have a windows 10 VM running on an ESXi server (vmdk) which I want to migrate to my UNRAID server which uses KVM to host VMs. The vmdk file is unfortunately thick provisioned, so I have had to do some disk juggling, due to my limited amount of available of space on my array.\nStep 1 – Copy the VMDK files to the UNRAID server There are a number of ways to do this, but I chose to mount an UNRAID share as an NFS datastore in ESXi and copy the VM folder across (note: copy not move). Remember to power off the VM before doing this. If you need help mounting NFS shares, just leave a comment below.\nStep 2 – Convert ESXi vmdk file to an img file using ‘qemu-img convert’ Now that we have a copy of the vmdk files on our UNRAID server, we can use the qemu-img tool to convert the files to a format that UNRAID can use to boot the VM – a .img file. Get console access to your UNRAID server and use the following command format:\nqemu-img convert -p -f vmdk -O raw /mnt/user/\u0026lt;the location of your vmdk file\u0026gt; /mnt/user/\u0026lt;the location of your new file\u0026gt;.img -p shows you the progress of the conversion. The larger the vmdk, the longer it will take -f the input file type -O the output file type /mnt/user/ is where UNRAID stores its shares. My VM was thick provisioned, so my output file was the same size as the vmdk. If your vmdk is thin provisioned, this process will swell the .img file – effectively making it thick provisioned – so make sure that you have enough available disk space for the conversion. I didn’t have this space available so I have to use the unassigned devices plugin and mount a USB hard drive to complete the conversion. Unassigned drives are mounted in /mnt/disks/.\nStep 3 – Create a KVM VM with the converted .img file Now we can create a VM using our newly created .img file; here are the settings I used for my converted Windows VM:\nThe CPU, memory, graphics card and other passthrough devices have no impact on the machine booting up. The VM will likely need to reconfigure itself to use the new hardware setup and you might need to install the virtIO network drivers.\nIf you are having problems, then it is best to keep the VM config simple and add devices as you solve problems.\nPhysical machine to UNRAID VM? Much of the same principals of this guide can be applied to moving a physical windows machine over to an UNRAID, KVM virtual machine. I will be covering this in future posts – using Clonezilla to build a .img file of a physical Windows machine.\n","permalink":"https://exitcode0.net/posts/convert-an-esxi-vm-to-unraid-kvm-with-qume-img-convert/","summary":"Here’s a quick guide on how to convert ESXi VMs to and UNRAID KVM virtual machine – it’s a simple process but a minefield to navigate on searches and forums.\nI currently have a windows 10 VM running on an ESXi server (vmdk) which I want to migrate to my UNRAID server which uses KVM to host VMs. The vmdk file is unfortunately thick provisioned, so I have had to do some disk juggling, due to my limited amount of available of space on my array.","title":"Convert an ESXi VM to UNRAID KVM with qume-img convert"},{"content":"Google photos auto backup is a nice way to make sure that all your photos on your mobile phone are backed up. It takes the pain out of backups by allowing you to automatically upload your photos and videos to Google servers. For those who would also like an offline copy of those photos, it is now easy to automate this. Here is how to backup your Google Photos library to your local computer or NAS with Rclone.\nhttps://rclone.org/ Rclone is a command line program to sync files and directories to and from:\nGoogle Drive / Photos Dropbox Onedrive and many more services Rclone can actually copy data to these cloud services as well, but this particular article focuses on how to take your data back from the cloud – because I like to own my data.\nPrerequisites This is based on the assumption that:\nyou are running Linux, mac OS or have a docker environment have Rclone installed – Install guide: https://rclone.org/install/ Backup Google Photos with Rclone First, we must make a remote endpoint – Google Photos. So jump into Rclone config to do so:\nrclone config No remotes found - make a new one n) New remote s) Set configuration password q) Quit config n/s/q\u0026gt; n name\u0026gt; remote Type of storage to configure. Enter a string value. Press Enter for the default (\u0026#34;\u0026#34;). Choose a number from below, or type in your own value [snip] XX / Google Photos \\ \u0026#34;google photos\u0026#34; [snip] Storage\u0026gt; google photos ** See help for google photos backend at: https://rclone.org/googlephotos/ ** Google Application Client Id Leave blank normally. Enter a string value. Press Enter for the default (\u0026#34;\u0026#34;). client_id\u0026gt; Google Application Client Secret Leave blank normally. Enter a string value. Press Enter for the default (\u0026#34;\u0026#34;). client_secret\u0026gt; Set to make the Google Photos backend read only. If you choose read only then rclone will only request read only access to your photos, otherwise rclone will request full access. Enter a boolean value (true or false). Press Enter for the default (\u0026#34;false\u0026#34;). read_only\u0026gt; Edit advanced config? (y/n) y) Yes n) No y/n\u0026gt; n Remote config Use auto config? * Say Y if not sure * Say N if you are working on a remote or headless machine y) Yes n) No y/n\u0026gt; y If your browser doesn\u0026#39;t open automatically go to the following link: http://127.0.0.1:53682/auth Log in and authorize rclone for access Waiting for code... Got code *** IMPORTANT: All media items uploaded to Google Photos with rclone *** are stored in full resolution at original quality. These uploads *** will count towards storage in your Google Account. -------------------- [remote] type: google photos token: {\u0026#34;access_token\u0026#34;:\u0026#34;XXX\u0026#34;,\u0026#34;token_type\u0026#34;:\u0026#34;Bearer\u0026#34;,\u0026#34;refresh_token\u0026#34;:\u0026#34;XXX\u0026#34;,\u0026#34;expiry\u0026#34;:\u0026#34;2019-06-28T17:38:04.644930156+01:00\u0026#34;} -------------------- y) Yes this is OK e) Edit this remote d) Delete this remote y/e/d\u0026gt; y I was running this config script over a remote SSH session so actually chose not to use auto config. The only difference is the URL you are given sends you to a Google auth page to manually retrieve your session auth code.\nNow that you have a remote endpoint setup, you can reference it in Rclone commands. This command is a simple way to take photos from my Google account and put them into Folders sorted by Year \u0026gt; Month:\nrclone copy GooglePhotos:media/by-month /mnt/user/PhotosBackup adding –progress to the end of that will give you a visual reference to the progress of the copy action.\nNow what? From here you can set this command to run on a schedule using cron if you are using Linux:\ncrontab -e A great resource for building cron tab commands: http://www.cronmaker.com/\n","permalink":"https://exitcode0.net/posts/backup-google-photos-with-rclone/","summary":"Google photos auto backup is a nice way to make sure that all your photos on your mobile phone are backed up. It takes the pain out of backups by allowing you to automatically upload your photos and videos to Google servers. For those who would also like an offline copy of those photos, it is now easy to automate this. Here is how to backup your Google Photos library to your local computer or NAS with Rclone.","title":"Backup Google Photos with Rclone"},{"content":" Rejoice! Kali Linux 2020.1 is here! The first release of 2020 has arrived and this post will contain some updates and my initial thoughts on the fresh release.\nKali Linux 2020.1 features and changes Non-Root by default Kali single installer image Kali NetHunter Rootless Improvements to theme \u0026amp; kali-undercover New tools Full Kali release notes as per their release blog post: https://www.kali.org/releases/kali-linux-2020-1-release/\nGot root? Kali has installed with root as the default user since the dawn of its existence, Backtrack, WHAX and Whoppix, but finally, the devs have decided that it time to bring Kali in line with Debian on this one and have a default user which is no longer root.\nMany might see this as an unnecessary move as the OS is not intended or condoned for use as a daily driver (even 2020.1) – although some people ignore this anyway so not your browser as root is always a good thing.\nWhile we don’t encourage people to run Kali as their day to day operating system, over the last few years more and more users have started to do so (even if they are not using it to do penetration testing full time), including some members of the Kali development team. When people do so, they obviously don’t run as default root user. With this usage over time, there is the obvious conclusion that default root user is no longer necessary and Kali will be better off moving to a more traditional security model. –\nhttps://www.kali.org/news/kali-default-non-root-user/\nThe maintainers have discovered that a large number of tools no longer require root (with the exception of NMAP as an example) to run, so it made sense to align with a more traditional security model.\nFirst Impressions I grabbed the 64bit ISO and spun up a VM – the host is running KVM, but this makes no difference.\nShock horror no more root by default – text-based install method. Pro-tip for first time Kali users… don’t give a name by which you could be identified.\nI like that you can now select the metapackages which you install and the Desktop environment (unless you use the automated installer method). So you can make your installation as fat or lightweight as you need it.\nOnce you’ve completed your install, everything else feels more or less the same. The maintainers are committed to binning as many python2 dependant tools as they can as technically they are no longer supported. We have some new tools, which I excited to test at a given opportunity:\ncloud-enum emailharvester phpggc sherlock splinter. Upgrade from Kali 1904.1 or earlier If you already have an installation and you don’t want to nuke and pave, then it is also possible to upgrade.\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list deb http://http.kali.org/kali kali-rolling main non-free contrib EOF apt update -y \u0026amp;\u0026amp; apt full-upgrade -y Once that is complete, go for a quick reboot then check that you are now running the latest and greatest:\nlsb_release -a Other useful Kali posts: Kali Linux – How to upgrade python 2.7 to python 3.7 https://exitcode0.net/kali-linux-how-to-upgrade-python-2-7-to-python-3-7/ How to make a VirtualBox VM the same resolution as host – https://exitcode0.net/how-to-make-a-virtualbox-vm-the-same-resolution-as-host/ Clipboard and Shared Folders on Kali Linux with VirtualBox – https://exitcode0.net/clipboard-and-shared-folders-on-kali-linux-with-virtualbox/ AWUS036AC (rt8812au) driver setup in Kali Linux – https://exitcode0.net/awus036ac-rt8812au-driver-setup-in-kali-linux/ (pending validation with 2020.1) ","permalink":"https://exitcode0.net/posts/kali-linux-2020-1-no-more-root-by-default/","summary":"Rejoice! Kali Linux 2020.1 is here! The first release of 2020 has arrived and this post will contain some updates and my initial thoughts on the fresh release.\nKali Linux 2020.1 features and changes Non-Root by default Kali single installer image Kali NetHunter Rootless Improvements to theme \u0026amp; kali-undercover New tools Full Kali release notes as per their release blog post: https://www.kali.org/releases/kali-linux-2020-1-release/\nGot root? Kali has installed with root as the default user since the dawn of its existence, Backtrack, WHAX and Whoppix, but finally, the devs have decided that it time to bring Kali in line with Debian on this one and have a default user which is no longer root.","title":"Kali Linux 2020.1 - No more root by default"},{"content":" https://ddclient.net A quote short post on how to secure your DDNS updates with Namecheap, SSL and DDClient. For those of us who use dynamic DNS to work around roaming IP addresses, it is important to make sure that you are updating your DNS records securely with SSL.\nThe default DDclient config has an example configuration file for using Namecheap’s DDNS service, however, it does not use SSL to check for your IP. In theory that connection could be manipulated and a false IP result could be returned – updating your DNS records to a wrong, malicious IP could cause a number of problems.\nNamecheap SSL DDClient Config Example ssl=yes use=web, web=dynamicdns.park-your-domain.com/getip protocol=namecheap server=dynamicdns.park-your-domain.com login=\u0026lt;your domain goes here\u0026gt; password=\u0026lt;your ddns password goes here\u0026gt; \u0026lt;your sub domain goes here\u0026gt; Something worth noting, Namecheap also have not made the effort to put this option to use SSL in their example config:\nhttps://www.namecheap.com/support/knowledgebase/article.aspx/583/11/how-do-i-configure-ddclient\n","permalink":"https://exitcode0.net/posts/secure-ddns-namecheap-ssl-with-ddclient/","summary":"https://ddclient.net A quote short post on how to secure your DDNS updates with Namecheap, SSL and DDClient. For those of us who use dynamic DNS to work around roaming IP addresses, it is important to make sure that you are updating your DNS records securely with SSL.\nThe default DDclient config has an example configuration file for using Namecheap’s DDNS service, however, it does not use SSL to check for your IP.","title":"Secure DDNS - Namecheap SSL With DDClient"},{"content":"Yet another simple problem/resolution. If you are looking to make your VirtualBox VM’s resolution match that of your host making full-screen mode, truly full screen, look no further, here is the answer.\nOnce again we are looking at an issue caused by a default setting. You can set your VM to full-screen mode, but it not likely to rescale to the native resolution of your display.\nThe setting which you need to change requires you to have the VM powered off. Enter the settings for the VM \u0026gt; Display \u0026gt; Screen Tab \u0026gt; Graphics Controller: VBoxSVGA.\nOnce you have changed it to VBoxSVGA, you can boot your VM and it will automatically fill the available resolution within the bounds of the virtual box window. Subsequently, if you switch to full screen mode, the VM will change the resolution to match the host’s display.\nChange the graphics controller to VBoxSVGA\nOther VirtualBox tips and How-To posts Clipboard and Shared Folders on Kali Linux with VirtualBox – https://exitcode0.net/clipboard-and-shared-folders-on-kali-linux-with-virtualbox/\n","permalink":"https://exitcode0.net/posts/how-to-make-a-virtualbox-vm-the-same-resolution-as-host/","summary":"Yet another simple problem/resolution. If you are looking to make your VirtualBox VM’s resolution match that of your host making full-screen mode, truly full screen, look no further, here is the answer.\nOnce again we are looking at an issue caused by a default setting. You can set your VM to full-screen mode, but it not likely to rescale to the native resolution of your display.\nThe setting which you need to change requires you to have the VM powered off.","title":"How to make a VirtualBox VM the same resolution as host"},{"content":" A short and sweet problem/resolution. If you are looking to enable subnet overlapping on a Fortigate so that you can give multiple interfaces an IP in the same subnet, this is the post for you.\nNOTE: This feature can only be enabled in the Fortigate’s CLI. To enable the overlapping feature, enter the following commands:\nconfig system settings set allow-subnet-overlap [enable/disable] end What is subnet overlapping? Subnet overlapping is disabled by default in fortiOS and for good reason; if you misuse subnet overlapping it can cause massive routing issues for your clients and their traffic. Subnet overlapping lets you apply IPs from the same subnet (e.g 192.160.1.X/24) to multiple interfaces that are not in the same virtual/physical switch.\nWhen trying to set an overlapping IP to an interface without enabling overlapping, the FortiGate will give the following error messages, CLI or GUI respectively:\n\u0026#39;Subnets overlap between \u0026#39;port2\u0026#39; and the primary IP of \u0026#39;port2\u0026#39; object set operator error, -54 discard the setting\u0026#39; or\n\u0026#39;IP address is in same subnet as the others.\u0026#39; When might I use subnet overlapping? One of my most common reasons for using subnet overlapping on a Fortigate is to give a HA interface a management IP on the same subnet as the shared (floating) management interface for your Fortigate. This makes it easy to access the web interface or SSH to the CLI of a HA slave if you need to do some troubleshooting.\nSupported FortiOS versions All of the following branches support subnet overlapping, but all have it disabled by default:\nFortiGate v4.0 MR2\nFortiGate v4.0 MR3\nFortiGate v5.0\nFortiGate v5.2\nFortiGate v5.4\nFortiGate v5.6\nFortiGate v6.0\nFortiGate v6.2\nOnline Resources Fortinet KB for Subnet Overlapping – https://kb.fortinet.com/kb/documentLink.do?externalID=FD30014\nOther Fortigate Tips and How-To posts Double NAT port forwarding with a Fortigate – https://exitcode0.net/double-nat-port-forwarding-with-a-fortigate/\n","permalink":"https://exitcode0.net/posts/how-to-enable-subnet-overlapping-on-a-fortigate/","summary":"A short and sweet problem/resolution. If you are looking to enable subnet overlapping on a Fortigate so that you can give multiple interfaces an IP in the same subnet, this is the post for you.\nNOTE: This feature can only be enabled in the Fortigate’s CLI. To enable the overlapping feature, enter the following commands:\nconfig system settings set allow-subnet-overlap [enable/disable] end What is subnet overlapping? Subnet overlapping is disabled by default in fortiOS and for good reason; if you misuse subnet overlapping it can cause massive routing issues for your clients and their traffic.","title":"How to enable subnet overlapping on a Fortigate"},{"content":"1st January 2020 marked the official end of python 2.7 development, including feature support and security fixes.\nPython 2.7 was over 9 years old in development years, making it the longest supported version to date. The code freeze is no in place, with the final release – 2.7.18 – scheduled for an April 2020 release. So yes there will be one more version to come down the tubes but it’s probably best that the new python project you were thinking of starting is written in 3.7 or above.\nMigrating away from Python 2.7 One useful resource for tracking the impending demise of a Python version: https://python-release-cycle.glitch.me/\nYou might be inclined to believe that any version of 3.X is good enough and better than running or developing in 2.7, however, there are a number of versions of 3 which have already reached the end of support – 3.2, 3.3, 3.4 – and others with only a few months on the clock – 3.5 and 3.6.\nFor those working on a Python project written in 2.7, here is the official porting guide: https://docs.python.org/3/howto/pyporting.html\nFor those who just wish to update the operating system’s primary python version used for running scripts and commands, I have composed a number of simple guides to make 3.7 your default version of python, including subsequent fixes for pip:\nChanging the default python version in Debian – https://exitcode0.net/changing-the-default-python-version-in-debian/ Ubuntu 19.10 – How to upgrade python 2.7 to python 3.7 – https://exitcode0.net/ubuntu-19-10-how-to-upgrade-python-2-7-to-python-3-7/ Kali Linux – How to upgrade python 2.7 to python 3.7 – https://exitcode0.net/kali-linux-how-to-upgrade-python-2-7-to-python-3-7/ and if you are looking to move from 3.5 to 3.7:\nDebian 9 – How to upgrade python 3.5 to python 3.7 – https://exitcode0.net/debian-9-how-to-upgrade-python-3-5-to-python-3-7/ The final goodbye for Python 2.7? In conclusion, whilst the new year marks the official sunset for Python 2.7, the road to migration to newer versions has been famously messy and in some sense a failure. Don’t be surprised to encounter 2.7 projects for years to come; consider 2.7 an annoying requirement for so time to come.\n","permalink":"https://exitcode0.net/posts/python-2-7-end-of-life-the-time-to-upgrade-is-upon-us/","summary":"1st January 2020 marked the official end of python 2.7 development, including feature support and security fixes.\nPython 2.7 was over 9 years old in development years, making it the longest supported version to date. The code freeze is no in place, with the final release – 2.7.18 – scheduled for an April 2020 release. So yes there will be one more version to come down the tubes but it’s probably best that the new python project you were thinking of starting is written in 3.","title":"Python 2.7 end-of-life - The time to upgrade is upon us"},{"content":"They say that the long tail page titles are best for SEO, well this one certainly should be up there with the best of them. In this post, I am going to explain how easy it is to control Kasa a HS100 / HS110 with IFTTT and python webhooks. Using this basic code, you expand into building a very powerful home automation system which fits your needs and take your Kasa home automation to the next level.\nKasa HS100 / HS110 Introduction HS100 (UK Version) Kasa is the home automation/ smart device brand offered by TP-Link. Their products office a great deal of functionality out of the box, allowing you to control a device smart plug/bulb from your phone without the need for a controller – such as the Philips hue systems. This is achieved using a cloud-hosted API model; every device communicates to/from the Kasa cloud-hosted backend. Every Kasa device is registered to your account.\nIFTTT Webhooks Introduction If This Then That (IFTTT) – is a free web-based service to create chains of simple conditional statements, called applets. An applet is triggered by changes that occur within other web services such as Gmail, Facebook, Telegram, Instagram, or Pinterest.\nYou must first activate the webhook service, then you can acquire your webhook URL.\nFind you webhook URL https://ifttt.com/maker_webhooks/settings\nBuild a webhook trigger Next, you need to build a trigger. In the case of a HS100/HS110, it would be sensible to build two triggers – on and off triggers. As you will see later in the code, the Event Name value is passed to IFTTT in our webhook URL; make this unique to each webhook trigger.\nKasa home automation with python Finally, it is time to write to some code – at last. This part is arguably less fiddly than the prerequisite setup for this project. As previously mentioned, this code contains my unique IFTTT trigger URL so I will be obfuscating some sensitive data to prevent everyone from turning my Kasa plug on and off.\n#!/usr/bin/python import requests def funcHSTon (): requests.post(\u0026#34;https://maker.ifttt.com/trigger/temp_low/with/key/\u0026lt;your webhook key\u0026gt;\u0026#34;) def funcHSToff (): requests.post(\u0026#34;https://maker.ifttt.com/trigger/temp_high/with/key/\u0026lt;your webhook key\u0026gt;\u0026#34;) #funcHSToff() funcHSTon() I have split both webhook triggers into two functions – funcHSTon \u0026amp; funcHSToff – and I have commented out the function that I don’t want to run during testing.\nRemember to replace ‘’ with your own key!\nBest of luck with the never ending story of home automation with python!\nOther useful posts: Ubuntu 19.10 – How to upgrade python 2.7 to python 3.7 – https://exitcode0.net/ubuntu-19-10-how-to-upgrade-python-2-7-to-python-3-7/ Debian 9 – How to upgrade python 3.5 to python 3.7 – https://exitcode0.net/debian-9-how-to-upgrade-python-3-5-to-python-3-7/ ","permalink":"https://exitcode0.net/posts/kasa-home-automation-with-ifttt-and-python-webhooks/","summary":"They say that the long tail page titles are best for SEO, well this one certainly should be up there with the best of them. In this post, I am going to explain how easy it is to control Kasa a HS100 / HS110 with IFTTT and python webhooks. Using this basic code, you expand into building a very powerful home automation system which fits your needs and take your Kasa home automation to the next level.","title":"Kasa home automation with IFTTT and python webhooks"},{"content":"I am planning a number of articles which focus on using aircrack-ng and hashcast to recover WPA wireless security passwords. However to get into that you need to have a specific wireless device which supports monitor mode and packet injection. I decided on the Alfa AWUS036AC, but some work was required to get the drivers installed.\nThis guide is based on Kali Linux 2019.4 – but the drivers are certified for earlier versions of Kali and the kernel that 2019.4 uses. See updates below for getting this working on Kali 2020.4.\nHardware – AWUS036AC My choice of hardware for WPA password recovery was the Alfa Networks AWUS036AC – https://amzn.to/34LlqXY\nFull manufacturer specifications – https://www.alfa.com.tw/products_detail/3.htm\nIncreased Wireless Signal Penetration With unmatched Wi-Fi signal strength and coverage. AWUS036AC not only has maximum WiFi range, it helps to penetrate walls, and eliminate Wi-Fi dead spots in your living space easily.\nIf you are conductin a wireless assessment, this is great news because you can conduct your testing from the comfort of your desk chair.\nChipset Realtek RTL8812AU WiFi Standards IEEE 802.11ac/a/b/g/n WiFi Frequency Dual Band 2.4GHz or 5GHz Antenna Connector RP-SMA female x 2 Antenna Type 2.4G/5GHz Dual-Band 5dBi dipole antenna Wireless Performance 802.11a: up to 54Mbps 802.11b: up to 11Mbps 802.11g: up to 54Mbps 802.11n: up to 300Mbps 802.11ac: up to 867Mbps Wireless Security 64/128 bit WEP,WPA/WPA2,WPA-PSK/WPA2-PSK,WPS Interface USB 3.0 OS Requirement Windows XP, Vista, 7, 8/8.1 and Windows 10 32/64bit, macOS 10.5 to 10.14 or later Linux AWUS036AC (rtl8812au) Driver Installation Assuming that you are loged into a terminal session on your kali linux machine as root, the following commands are required to download and install the drivers from source:\nClone the aircrack-ng git repository:\ngit clone https://github.com/aircrack-ng/rtl8812au Enter the newly downloaded directory:\ncd rtl8812au Build and install the source files:\nmake make install Reboot the Kali instance to complete:\nreboot now There are a number of other git repositories where you can obtain drivers for this usb wireless adaptor. My findings were that the aircrack-ng repo was the only place which supplied drivers supported in the current kernel version for Kali 2019.4 – kernel 5.3.9\nVerify your device To ensure that your device is available and ready to be used in Kali, you can run the following command to confirm that the OS can recognise the adapter:\niwconfig Update: 11/01/2021 As you can see from the Github issue reports, https://github.com/aircrack-ng/rtl8812au/issues, there are a lot of active issues with these drivers.\nAs of 11/01/2021, I was able to get this chipset working on Kali with the following driver install method:\napt-get update apt install realtek-rtl88xxau-dkms And just to verify, here is my curent Kali version:\nroot@kali:~# lsb_release -a No LSB modules are available. Distributor ID: Kali Description: Kali GNU/Linux Rolling Release: 2020.4 Codename: kali-rolling …and here is proof of function with airodump-ng:\nSome yummy WEP networks in there for my troubles! It is worth noting that iwconfig shows the interface wlan0 in monitor mode; a mon0 interface is not created like most online tutorials demonstrate: https://www.computerweekly.com/tip/Step-by-step-aircrack-tutorial-for-Wi-Fi-penetration-testing.\nOther useful articles: https://exitcode0.net/kali-linux-how-to-upgrade-python-2-7-to-python-3-7/ https://exitcode0.net/clipboard-and-shared-folders-on-kali-linux-with-virtualbox/ ","permalink":"https://exitcode0.net/posts/awus036ac-rt8812au-driver-setup-in-kali-linux/","summary":"I am planning a number of articles which focus on using aircrack-ng and hashcast to recover WPA wireless security passwords. However to get into that you need to have a specific wireless device which supports monitor mode and packet injection. I decided on the Alfa AWUS036AC, but some work was required to get the drivers installed.\nThis guide is based on Kali Linux 2019.4 – but the drivers are certified for earlier versions of Kali and the kernel that 2019.","title":"AWUS036AC (rtl8812au) driver setup in Kali Linux"},{"content":"I have covered changing the default version of python in Debian, however for those looking to Google for a quick fix on Kali, I hope that this reaches you well.\nThis was tested on a completely fresh install of Kali Linux with no other alterations made prior.\nThe basic premise is to configure Kali to use python 3.7 at a higher priority to python 2.7 or any other version installed on the system.\nCheck your python version Step 1 is to check your current python version:\npython -V or\npython --version Kali default output:\nPython 2.7.17\nSet your Python Default Now it is time configure the priority for the versions of python that we have installed, 2.7 and 3.5/7. You can list all of the available alternatives installed by running:\nls /usr/bin/python* To set your version priorities, with 3.7 being the high priority:\nupdate-alternatives --install /usr/bin/python python /usr/bin/python2.7 1 update-alternatives --install /usr/bin/python python /usr/bin/python3.7 2 We have just set 3.7 (2) to have a priority great than 2.7 (1). Now when we list the python priorities we see see 3.7 is higher that 2.7:\nupdate-alternatives --config python This is also a great way to easily switch those priorities around once they have been set.\nCheck you default version, again… python -V Now this command should return the default which you configured above.\nOther Useful Python tips: https://exitcode0.net/debian-9-how-to-upgrade-python-3-5-to-python-3-7/ – Debian 9 – How to upgrade python 3.5 to python 3.7 https://exitcode0.net/changing-the-default-python-version-in-debian/ – Changing the default python version in Debian If you have found this guide useful or it has solved a burning issue for you, please consider throw a coin in the tip jar to help this site stay active:\nhttps://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick\u0026amp;hosted_button_id=BTQD4GN8TTWJN\u0026amp;source=url\n","permalink":"https://exitcode0.net/posts/kali-linux-how-to-upgrade-python-2-7-to-python-3-7/","summary":"I have covered changing the default version of python in Debian, however for those looking to Google for a quick fix on Kali, I hope that this reaches you well.\nThis was tested on a completely fresh install of Kali Linux with no other alterations made prior.\nThe basic premise is to configure Kali to use python 3.7 at a higher priority to python 2.7 or any other version installed on the system.","title":"Kali Linux - How to upgrade python 2.7 to python 3.7"},{"content":"Much to my surprise, I found that a fresh install of Ubuntu 19.10 was set to use python 2.7 as the default python instance.\nI have covered changing the default version of python in Debian, however for those looking to Google for a quick fix on Ubuntu 19.10, I hope that this reaches you well.\nThis was tested on a completely fresh install of Ubuntu 19.10 with no other alterations made prior.\nThe basic premise is to configure Ubuntu to use python 3.7 at a higher priority to python 2.7 or any other version installed on the system.\nCheck your python version Step 1 is to check your current python version:\npython -V or\npython --version Ubuntu 19.10 default output:\nPython 2.7.17rc1\nSet your Python Default Now it is time configure the priority for the versions of python that we have installed, 2.7 and 3.5/7. You can list all of the available alternatives installed by running:\nls /usr/bin/python* Ubuntu 19.10 default output:\nv2.7 and 3.7 are available To set your version priorities, with 3.7 being the high priority:\nupdate-alternatives --install /usr/bin/python python /usr/bin/python2.7 1 update-alternatives --install /usr/bin/python python /usr/bin/python3.7 2 We have just set 3.7 (2) to have a priority great than 2.7 (1). Now when we list the python priorities we see see 3.7 is higher that 2.7:\nupdate-alternatives --config python This is also a great way to easily switch those priorities around once they have been set.\nCheck you default version, again… python -V Now this command should return the default which you configured above.\nOther Useful Python tips: https://exitcode0.net/debian-9-how-to-upgrade-python-3-5-to-python-3-7/ – Debian 9 – How to upgrade python 3.5 to python 3.7 https://exitcode0.net/changing-the-default-python-version-in-debian/ – Changing the default python version in Debian If you have found this guide useful or it has solved a burning issue for you, please consider throw a coin in the tip jar to help this site stay active:\nhttps://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick\u0026amp;hosted_button_id=BTQD4GN8TTWJN\u0026amp;source=url\n","permalink":"https://exitcode0.net/posts/ubuntu-19-10-how-to-upgrade-python-2-7-to-python-3-7/","summary":"Much to my surprise, I found that a fresh install of Ubuntu 19.10 was set to use python 2.7 as the default python instance.\nI have covered changing the default version of python in Debian, however for those looking to Google for a quick fix on Ubuntu 19.10, I hope that this reaches you well.\nThis was tested on a completely fresh install of Ubuntu 19.10 with no other alterations made prior.","title":"Ubuntu 19.10 - How to upgrade python 2.7 to python 3.7"},{"content":" Python.Org I have been getting started with python 3 – I want to make this my primary scripting language. One way I like to assist myself whilst I learn the rope is to maintain a crib sheet filled with all the trivial things I would otherwise forget.\nPython 3 Cheat Sheet #Define Variables programming_languages: \u0026#34;Python\u0026#34;, \u0026#34;VB\u0026#34;, \u0026#34;C++\u0026#34;, \u0026#34;C#\u0026#34; #Print Variables print(programming_languages) print(\u0026#39;--------------------\u0026#39;) #Basic for loop + variables for language in programming_languages: print(language) print(\u0026#39;--------------------\u0026#39;) #Basic Function def FuncExample(): i: 1 for language in programming_languages: #Concatinate Strings and Integers in print statements print(\u0026#34;Language \u0026#34; + str(i) + \u0026#34;:\u0026#34; + language) #Increment Integer i += 1 FuncExample() print(\u0026#39;--------------------\u0026#39;) #Functions with Variables #1 - Strings def FuncVarExample1(fname, lname): #Print with CRLF print(\u0026#34;First Name: \u0026#34; + fname + \u0026#34;\\r\\n\u0026#34; + \u0026#34;Last Name: \u0026#34; + lname) FuncVarExample1(\u0026#34;Joe\u0026#34;,\u0026#34;Bloggs\u0026#34;) print(\u0026#39;--------------------\u0026#39;) #Functions with Variables #2 - Integers + Returning Values def FuncVarExample2(x, y): #Basic integer maths return x+y #Concatenating Strings and Integers print(\u0026#34;33 + 42: \u0026#34; + str(FuncVarExample2(33,42))) print(\u0026#39;--------------------\u0026#39;) You can also find more code snippets here: https://exitcode0.net/code-samples/\n","permalink":"https://exitcode0.net/posts/getting-started-with-python-3-a-beginners-cheat-sheet/","summary":"Python.Org I have been getting started with python 3 – I want to make this my primary scripting language. One way I like to assist myself whilst I learn the rope is to maintain a crib sheet filled with all the trivial things I would otherwise forget.\nPython 3 Cheat Sheet #Define Variables programming_languages: \u0026#34;Python\u0026#34;, \u0026#34;VB\u0026#34;, \u0026#34;C++\u0026#34;, \u0026#34;C#\u0026#34; #Print Variables print(programming_languages) print(\u0026#39;--------------------\u0026#39;) #Basic for loop + variables for language in programming_languages: print(language) print(\u0026#39;--------------------\u0026#39;) #Basic Function def FuncExample(): i: 1 for language in programming_languages: #Concatinate Strings and Integers in print statements print(\u0026#34;Language \u0026#34; + str(i) + \u0026#34;:\u0026#34; + language) #Increment Integer i += 1 FuncExample() print(\u0026#39;--------------------\u0026#39;) #Functions with Variables #1 - Strings def FuncVarExample1(fname, lname): #Print with CRLF print(\u0026#34;First Name: \u0026#34; + fname + \u0026#34;\\r\\n\u0026#34; + \u0026#34;Last Name: \u0026#34; + lname) FuncVarExample1(\u0026#34;Joe\u0026#34;,\u0026#34;Bloggs\u0026#34;) print(\u0026#39;--------------------\u0026#39;) #Functions with Variables #2 - Integers + Returning Values def FuncVarExample2(x, y): #Basic integer maths return x+y #Concatenating Strings and Integers print(\u0026#34;33 + 42: \u0026#34; + str(FuncVarExample2(33,42))) print(\u0026#39;--------------------\u0026#39;) You can also find more code snippets here: https://exitcode0.","title":"Getting started with Python 3 - a beginner's cheat sheet"},{"content":"I spent more time that care to admit trying to setup a shared folder between my windows host and Kali VirtualBox VM. So hopefully the Google algos pick this one up and save you the time trying to find the right packages to fix this a clipboard sync.\napt-get update apt-get install -y virtualbox-guest-x11 Go for a quick reboot once the above commands are complete and you should have clipboard sync (text) and shared folders should mount successfully.\nMore Linux Tidbits: Changing the default python version in Debian https://exitcode0.net/changing-the-default-python-version-in-debian/ Debian 9 – Running a python script at boot https://exitcode0.net/debian-9-how-to-upgrade-python-3-5-to-python-3-7/ Changing the default python version in Debian https://exitcode0.net/changing-the-default-python-version-in-debian/ ","permalink":"https://exitcode0.net/posts/clipboard-and-shared-folders-on-kali-linux-with-virtualbox/","summary":"I spent more time that care to admit trying to setup a shared folder between my windows host and Kali VirtualBox VM. So hopefully the Google algos pick this one up and save you the time trying to find the right packages to fix this a clipboard sync.\napt-get update apt-get install -y virtualbox-guest-x11 Go for a quick reboot once the above commands are complete and you should have clipboard sync (text) and shared folders should mount successfully.","title":"Clipboard and Shared Folders on Kali Linux with VirtualBox"},{"content":"The Raspberry Pi 4 has now been released offering up to 4GB of RAM! All of the horsepower required for an excellent lower power, docker host.\nHowever, there are currently issues undergoing work which prevent docker from running on the only Rasbian image currently available for the Pi 4 – ‘Rasbian Buster‘. Details of these issues can been found here on the GitHub thread – https://github.com/docker/for-linux/issues/709\nCurrent Working Solution Fear not, for there is a simple way to fool your docker installation and successfully getting it to run on the Pi 4.\nFirst, let make sure that your raspbian install is up to date:\nsudo apt-get update sudo apt-get upgrade Now let’s install docker:\ncurl -SL get.docker.com | sed \u0026#39;s/9)/10)/\u0026#39; | sh Once complete the installer will advise that you should add the Pi user to the docker group if you wish to run docker commands from that account:\nusermod -aG docker pi And you’re done! Docker will now be running on your Raspberry Pi 4.\nIf this posted helped you, consider throwing a penny in the tip jar to support this site.\n","permalink":"https://exitcode0.net/posts/solved-running-docker-on-a-raspberry-pi-4/","summary":"The Raspberry Pi 4 has now been released offering up to 4GB of RAM! All of the horsepower required for an excellent lower power, docker host.\nHowever, there are currently issues undergoing work which prevent docker from running on the only Rasbian image currently available for the Pi 4 – ‘Rasbian Buster‘. Details of these issues can been found here on the GitHub thread – https://github.com/docker/for-linux/issues/709\nCurrent Working Solution Fear not, for there is a simple way to fool your docker installation and successfully getting it to run on the Pi 4.","title":"[SOLVED] - Running Docker on a Raspberry Pi 4"},{"content":"In a previous post, I explained how to upgrade from Python3.5 to Python3.7, however is is still the case in most fresh Debian installs that the default python version is 2.7. This post is going to show you the simple steps you need to take when changing the default python version in Debian.\nThis guide is based on a Debian 9 installation, but work for multiple releases.\nCheck you default version First we need to check our current default version of python. This is the version of python that you Debian OS will try to execute python scripts with unless otherwise specified.\npython -V or\npython --version My fresh install of Debian 9 told me that my default was: 2.7.17. Old and unwanted.\nInstall Python 3 This is optional and only applies for those who don’t already have python 3 installed:\napt install python3 Set your Python Default Now it is time configure the priority for the versions of python that we have installed, 2.7 and 3.5/7. You can list all of the available alternatives installed by running:\nls /usr/bin/python* To set your version priorities, with 3.5 being the high priority:\nupdate-alternatives --install /usr/bin/python python /usr/bin/python2.7 1 update-alternatives --install /usr/bin/python python /usr/bin/python3.5 2 We have just set 3.5 to have a priority great than 2.7. Now when we list the python priorities we see see 3.5 is higher that 2.7:\nupdate-alternatives --config python This is also a great way to easily switch those priorities around once they have been set.\nCheck you default version, again… python -V Now this command should return the default which you configured above.\nIf you have found this guide useful or it has solved a burning issue for you, please consider throw a coin in the tip jar to help this site stay active:\nhttps://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick\u0026amp;hosted_button_id=BTQD4GN8TTWJN\u0026amp;source=url\n","permalink":"https://exitcode0.net/posts/changing-the-default-python-version-in-debian/","summary":"In a previous post, I explained how to upgrade from Python3.5 to Python3.7, however is is still the case in most fresh Debian installs that the default python version is 2.7. This post is going to show you the simple steps you need to take when changing the default python version in Debian.\nThis guide is based on a Debian 9 installation, but work for multiple releases.\nCheck you default version First we need to check our current default version of python.","title":"Changing the default python version in Debian"},{"content":"This post is going to share how to use DHT11 sensors with a the raspberry pi , using the GPIO pins. Furthermore the particular sensor that I am going to be discussing requires zero soldering and is purely plug and play.\nThe Sensor:\nThe sensor that we will be using is the DHT11 – the exact variant is the 3 pin version. You can find the exact version that I used here\nhttps://amzn.to/2XkclSq\nThis pack of 5 sensors also comes with the cables you will need to hook up the DHT11 to the GPIO pinout of the raspberry pi.\nThe PIN Layout\nThe sensor pin are a positive power pin, a neutral pin and a data pin. Depending on the version of your raspberry pi, the GPIO layout can very. The following diagram shows the GPIO layout for the raspberry pi 3 – the board I used:\nThe pins to connect the DHT11 to in this example would be 2, 6, and 7. That being said, it would be possible to use any of the other available data pins (green), if this was the only device attached via the GPIO pins.\n","permalink":"https://exitcode0.net/posts/using-a-dht11-sensor-with-a-raspberry-pi/","summary":"This post is going to share how to use DHT11 sensors with a the raspberry pi , using the GPIO pins. Furthermore the particular sensor that I am going to be discussing requires zero soldering and is purely plug and play.\nThe Sensor:\nThe sensor that we will be using is the DHT11 – the exact variant is the 3 pin version. You can find the exact version that I used here","title":"Using a DHT11 sensor with a raspberry pi"},{"content":"If you are unfortunate enough to have to deal with double NAT on your gateway then you might know the troubles surrounding portforwarding or VIPs. Here is a quick how to guide for setting up a port forward on a Forgate where double NAT is inplace.\nCase Study – Plex port forward\nPlex is a great tool for managing your personal media collection and it gets even better when you enable a port forward to let you access this collection from anywhere in the world. Whilst Plex ahve made a number of changes to allow you to reach your contect via a relay server, the best way to access your content from outside your LAN is by using a port forward.\nDouble NAT means that there is a device runing NAT service in front of your NAT enabled default gateway – this can make portforwards difficult.\nI started by setting the ‘WAN IP’ of my Fortigate to a DMZ IP on the border NAT device – this will prevent any port foltering or firewall restrictions on traffic destined for the Fortigate.\nNext when creating your VIP, use the following config:\nNOTE: I am currently using WAN2 as the primary WAN conenction on my Fortigate.\n","permalink":"https://exitcode0.net/posts/double-nat-port-forwarding-with-a-fortigate/","summary":"If you are unfortunate enough to have to deal with double NAT on your gateway then you might know the troubles surrounding portforwarding or VIPs. Here is a quick how to guide for setting up a port forward on a Forgate where double NAT is inplace.\nCase Study – Plex port forward\nPlex is a great tool for managing your personal media collection and it gets even better when you enable a port forward to let you access this collection from anywhere in the world.","title":"Double NAT port forwarding with a Fortigate"},{"content":"There are a number of ways that you can run a python or bash script at system start but I am about to show you possibly the easiest way, using crontab.\nFirst you need to make that your script is executable and can run unattended. Then you need to login as the user your wish to execute the script as and edit the crontab file:\ncrontab -e If this is your first time editing the crontab file, you might be asked which editor you wish to use – I chose nano.\nOnce in the crontab file add a new line at the bottom of the file; it should look something like this\n@reboot /the/path/to/your/script/./your_script.py [any other parameters] \u0026gt; /path/of/your/log.txt Alternatively, if you don’t wish to record any output from your script you can send the output to /dev/null. Now it’s time to reboot your Debian instance and your script will be live.\n","permalink":"https://exitcode0.net/posts/debian-9-running-a-python-script-at-startup/","summary":"There are a number of ways that you can run a python or bash script at system start but I am about to show you possibly the easiest way, using crontab.\nFirst you need to make that your script is executable and can run unattended. Then you need to login as the user your wish to execute the script as and edit the crontab file:\ncrontab -e If this is your first time editing the crontab file, you might be asked which editor you wish to use – I chose nano.","title":"Debian 9 - Running a python script at boot"},{"content":" I recently spent 30 minutes figuring out how to upgrade to python 3.7 and subsequently pip version on a fresh install of Debian 9. I ran into a number of issues doing this so I though that I would put this quick post together to make this a little bit less complicated for anyone else trying to this.\nThis post relates specifically to python version 3.7.3. Although the generic commands should also apply to earlier versions, but your milage may vary.\nThe basic premise is, upgrade your version of python 3 to your desired version – 3.7.3 in this instance – then configure Debian to use python 3.7 at a higher priority to python 3.5.\nCheck your version\nStep 1 is to check your current python version:\npython3 -V or\npython3 --version Download the latest or desired version of python 3\nNext we need to download the latest version or desired version of python 3 from the python website. In my case I selected 3.7.3. Once downloaded we need to extract the tar file.\nwget https://www.python.org/ftp/python/3.7.3/Python-3.7.3.tar.xz tar xf Python-3.7.3.tar.xz cd ./Python-3.7.3 Make and Install\nNow that we have the files downloaded and extracted, it is time to compile them.\n./configure make make install Switch to the new version\nAfter compile the new version of python from source, we can now configure Debian to make it our default version of python3.\nupdate-alternatives --install /usr/bin/python python /usr/local/bin/python3.7 10 The integer at the end of this command (10) sets the priority for the python version; the greater the integer, the higher the priority. At this point we can rerun the previously used version commands and we should see that we now have 3.7.3 active.\nFixing and Updating Pip\nIt was at this point that I attempted to install some required addons using pip and discovered that the upgrade to python 3.7.3 had broken a few things. These were the commands I used to resolved issues with lsb_release and pip:\nln -s /usr/share/pyshared/lsb_release.py /usr/local/lib/python3.7/site-packages/lsb_release.py pip3 install --upgrade pip If you have found this guide useful or it has solved a burning issue for you, please consider throw a coin in the tip jar to help this site stay active:\nhttps://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick\u0026amp;hosted_button_id=BTQD4GN8TTWJN\u0026amp;source=url\nSome useful guides I found along the way: https://jcutrer.com/linux/upgrade-python37-ubuntu1810 – How to upgrade to python 3.7 on ubuntu 18.10. https://tecadmin.net/install-python-3-7-on-ubuntu-linuxmint/ – How to Install Python 3.7 on Ubuntu, Debian and LinuxMint. https://linuxconfig.org/how-to-change-default-python-version-on-debian-9-stretch-linux – How to change default python version on Debian 9 Stretch Linux Other Useful Debian tips: https://exitcode0.net/debian-9-running-a-python-script-at-startup/ – Debian 9 – Running a python script at boot. ","permalink":"https://exitcode0.net/posts/debian-9-how-to-upgrade-python-3-5-to-python-3-7/","summary":"I recently spent 30 minutes figuring out how to upgrade to python 3.7 and subsequently pip version on a fresh install of Debian 9. I ran into a number of issues doing this so I though that I would put this quick post together to make this a little bit less complicated for anyone else trying to this.\nThis post relates specifically to python version 3.7.3. Although the generic commands should also apply to earlier versions, but your milage may vary.","title":"Debian 9 - How to upgrade python 3.5 to python 3.7"},{"content":"This is a local DNS server for local DNS requests.\nThis post is going to explain the why and how I created a local DNS server in my home network environment. I used the PiHole project to make a network wide advert blocking a reality.\nhttps://pi-hole.net/\nI have previously created a DNS server using Bind, running on CentOS 7. Thankfully digital ocean came to my rescue with some of the config in this home lab project, so be sure to check out their guide if you are looking for slightly higher level DNS setup:\nhttps://www.digitalocean.com/community/tutorials/how-to-configure-bind-as-a-private-network-dns-server-on-centos-7\nPiHole Setup\nStep 1 – Server Installation\nEvery great Linux project starts with a tedious operating system install. I started this project by setting up a Debian 9 VM on one of my local hypervisors. However as the name suggest, PiHole is very comfortable running on a Raspberry Pi single single board computer.\nMy VM specs: 1 vCPU, 512MB RAM, 15GB Storage\nStep 1 – Server Installation\nEvery great Linux project starts with a tedious operating system install. I started this project by setting up a Debian 9 VM on one of my local hypervisors. However as the name suggest, PiHole is very comfortable running on a Raspberry Pi single single board computer.\nMy VM specs: 1 vCPU, 512MB RAM, 15GB Storage\nStep 2 – Update all the things\nBe sure to update and upgrade the package repositories in your OS once the install is complete\nsudo apt-get update\nsudo apt-get upgrade\nYou might also want to take this opportunity to install the nano text editor as well as a couple of other handy utilities\nStep 3 – Set a static IP\nNow that our CentOS box is configured how you like it, its is time to make sure that it has a static IP. If you are going to be distributing this DNS server to your network clients, then it going to be important that this server does not get different DHCP lease addresses\nsudo nano /etc/network/interfaces\nChange the file to reflect the following config, changing the network setting to match your environment:\nauto lo\niface lo inet loopback\niface eth0 inet static\naddress 192.168.0.100\nnetmask 255.255.255.0\nnetwork 192.168.0.0\nbroadcast 192.168.0.255\ngateway 192.168.0.1\nStep 4 – Install PiHole\nThis final step pretty much rounds up the PiHole install process. The following command runs a guided installer which lets you chose your settings but offer a very functional system when the defaults are selected\ncurl -sSL https://install.pi-hole.net |bash\n","permalink":"https://exitcode0.net/posts/creating-a-local-dns-server-with-pi-hole/","summary":"This is a local DNS server for local DNS requests.\nThis post is going to explain the why and how I created a local DNS server in my home network environment. I used the PiHole project to make a network wide advert blocking a reality.\nhttps://pi-hole.net/\nI have previously created a DNS server using Bind, running on CentOS 7. Thankfully digital ocean came to my rescue with some of the config in this home lab project, so be sure to check out their guide if you are looking for slightly higher level DNS setup:","title":"Creating a local DNS server with Pi Hole"},{"content":" I once faced a rather tedious task that involved recursively modifying a number of shortcut paths stored across a convoluted folder structure. There was approximately 100 shortcuts which needed part of their path modifying.\nThe answer: Create a****PowerShell script.\nThe following code utilises regex to check for the existence of a string and modify with the define replacement. It is easily possible to use Read-Host to make this a little bit more interactive, but the purposes of my use-case it was just as simple to modify these variables before running the script.\n$oldPrefix: “\\OLDPLACE”\n#$oldPrefix: “\\NEWPLACE”\n$newPrefix: “\\NEWPLACE”\n#$newPrefix: “\\OLDPLACE”\n$searchPath: $PSScriptRoot\n$totalchanges: 0\n$dryRun: $TRUE\n$shell: new-object -com wscript.shell\nWrite-Host “Welcome to the script”\n$dryRuninput: Read-Host “Type True for dry run, type False for real run: ”\nif ($dryRuninput -match “[true]{4}”) {\n$dryRun: $TRUE\n} elseif ($dryRuninput -match “[false]{5}”) {\n$dryRun: $FALSE\n} else {\nWrite-Host “No Valid input detected”\n}\nif ( $dryRun ) {\nwrite-host “Executing dry run” -foregroundcolor green -backgroundcolor black\n} else {\nwrite-host “Executing real run” -foregroundcolor red -backgroundcolor black\n}\ndir $searchPath -filter *.lnk -recurse | foreach {\n$totalchanges ++\n$lnk: $shell.createShortcut( $_.fullname )\n$oldPath= $lnk.targetPath\n$lnkRegex: [regex]::escape( $oldPrefix )\nif ( $oldPath -match $lnkRegex ) {\n$newPath: $oldPath -replace $lnkRegex, $newPrefix\nwrite-host “Found: ” + $_.fullname -foregroundcolor yellow -backgroundcolor black\nwrite-host ” Replace: ” + $oldPath\nwrite-host ” With: ” + $newPath\nif ( !$dryRun ) {\n$lnk.targetPath: $newPath\n$lnk.Save()\n}\n}\n}\nwrite-host “Total Number of links found: ” $totalchanges\n","permalink":"https://exitcode0.net/posts/modifying-windows-shortcuts-is-powershell/","summary":"I once faced a rather tedious task that involved recursively modifying a number of shortcut paths stored across a convoluted folder structure. There was approximately 100 shortcuts which needed part of their path modifying.\nThe answer: Create a****PowerShell script.\nThe following code utilises regex to check for the existence of a string and modify with the define replacement. It is easily possible to use Read-Host to make this a little bit more interactive, but the purposes of my use-case it was just as simple to modify these variables before running the script.","title":"Modifying Windows shortcuts is Powershell"},{"content":"For security purposes, it is possible to control the language mode in a given Powershell session. These language modes can constrict which modules can be loaded during the life of a powershell session.\nLearn mode about Powershell langeuage modes: About Language Modes – Microsoft\nDetect the Current Language Mode $sLangMode: $ExecutionContext.SessionState.LanguageMode\nIf ($sLangMode -ne “FullLanguage”){\nWrite-Host ” !! Unable to run scrit – Powershell Using Wrong Language Mode !! ”\n}\nElse{\n#RUN THE MAIN FUNCTION\n}\nTry putting this simple statement at the start of your powershell scripts to avoid any unhandled exceptions caused by constrictive language modes.\n","permalink":"https://exitcode0.net/posts/powershell-checking-the-language-mode/","summary":"For security purposes, it is possible to control the language mode in a given Powershell session. These language modes can constrict which modules can be loaded during the life of a powershell session.\nLearn mode about Powershell langeuage modes: About Language Modes – Microsoft\nDetect the Current Language Mode $sLangMode: $ExecutionContext.SessionState.LanguageMode\nIf ($sLangMode -ne “FullLanguage”){\nWrite-Host ” !! Unable to run scrit – Powershell Using Wrong Language Mode !! ”","title":"Powershell - Checking the Language Mode"},{"content":"This is really quick nugget of Powershell for anyone who is struggling to copy and paste into a particular window or dialog box.\nPerhaps it is a case of a website which prevents text form being sent to a field from the clipboard, or in my case, a windows UAC prompt. If you are following password best practices, your passwords should be long, complex and contain zero dictionary words. Furthermore, you should have a different password for every site and service. So, when a user interface doesn’t allow you to paste a password, life gets that little bit more difficult.\nPowershell to the rescue!\nfunction FuncMain {\nadd-type -AssemblyName System.Windows.Forms\nWrite-Host “”\nWrite-Host ” — Welcome to the UAC Clipboard Utility — ”\nWrite-Host “”\n$sUsername: Read-Host “Enter Username”\n$sPassword: Read-Host “Enter Password”\nWrite-Host “User name and password ready…”\nWrite-Host “”\nWrite-Host “!!! PLACE THE CURSOR IN THE UAC USERNAME FIELD NOW !!!”\nWrite-Host “”\nStart-Sleep -Seconds 1\n$i: 5\nWrite-Host “Begin Countdown:”\nwhile( $i -gt 0){\nwrite-host $i\n$i —\nStart-Sleep -Seconds 1\n}\nTry {\n#Sends the entered username followed by tab and then the password.\nSystem.Windows.Forms.SendKeys::SendWait($sUsername + “{TAB}”)\nStart-Sleep -Seconds 1\n}\ncatch {\nWrite-Host “There was an error sending username / password!”\n}\nWrite-Host “”\nWrite-Host “…Paste of Credentials Complete”\nRead-Host “Press Enter to Clear Console and close the script”\nClear-Host\n}\n$sLangMode: $ExecutionContext.SessionState.LanguageMode\nIf ($sLangMode -ne “FullLanguage”){\nWrite-Host “”\nWrite-Host ” !! Unable to run scrit – Powershell Using Wrong Language Mode !! ”\nWrite-Host “”\n}\nElse{\n#RUN THE MAIN FUNCTION\nFuncMain\n}\nModifications: If this doesn’t quite fit your use-case, you can easily modify this code by commenting out the read-host line which asks for a username, as well as commenting out the line where the username keys are sent – This converts the script into a password only tool.\nChanging the value of $i changes the countdown timer.\nThanks again to Powershell scripts for making our lives a little bit easier.\n","permalink":"https://exitcode0.net/posts/copy-paste-with-powershell-sendkeys/","summary":"This is really quick nugget of Powershell for anyone who is struggling to copy and paste into a particular window or dialog box.\nPerhaps it is a case of a website which prevents text form being sent to a field from the clipboard, or in my case, a windows UAC prompt. If you are following password best practices, your passwords should be long, complex and contain zero dictionary words. Furthermore, you should have a different password for every site and service.","title":"'Copy, Paste' With Powershell Sendkeys"}]