[{"content":"MetaAI have done it again\u0026hellip; Dropping llama3.1 under the (mostly) open source license and I have been putting it through its paces and benchmarking to find out how well it performs to models preiously tested: Testing Llama3 With LM Studio. Previously we took a look at llama3 in LM Studio which has made self hosted LLMs that rival paid services like ChatGPT and Claude possible; this time we are taking a look at llama through the lend of GPT4All.\nPer the Meta llama github model card:\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\nModel Architecture: Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nThe power consumption of the Nvidia H100 GPUs used to build this model is slightly staggering and certainly opens up a number of questions around the value per kilowatt a model like llama provideS to commerce and GDP:\nModel Training Time (GPU hours) Training Power Consumption (W) Training Location-Based Greenhouse Gas Emissions (tons CO2eq) Llama 3.1 8B 1.46M 700 420 Llama 3.1 70B 7.0M 700 2,040 Llama 3.1 405B 30.84M 700 8,930 Total 39.3M 11,390 Installing the model Just like LMStudio, GPT4All makes running large language models extremely accessible and maintains complete privacy of chat data. One bonus feature of GPT4All is the ability to \u0026ldquo;chat with your documents\u0026rdquo; generating a RAG index for files or folders. This populates the prompt with context based on your indexed files, allowing for a GPT conversation that has awareness of your documents. This type of integration could provide significant value for Professionals or Businesses who have privacy concerns regarding data upload to cloud based AI services like ChatGPT.\nGPT4All has some excellent Documentation.\nOnce you have downloaded the latest installer for you platform (Windows, Mac, Linux), you can go ahead and grab a GGUF model with ease:\nDownloading the Meta llama3 (or 3.1) model in GPT4All\nThen, you can go ahead and start a chat and select the model you wish to use - it is possible to have multiple models downloaded and select between then\u0026hellip; useful if you have models suited to specific tasks:\nLaunching a chat window in GPT4All\nPerformance Let me start by providing some system specs as context. I have by 2024 standards, a modest desktop computer:\nRyzen 5600 6c/12t CPU 32GB 3200MHz DDR4 RAM Nvidia RTX 3060 12GB GDDR6 GPU It would be great to test these models on bleeding edge hardware, enterprise hardware and even Apple M3 silicon, but hardware isn\u0026rsquo;t cheap these days and I, like many, don\u0026rsquo;t have the resources to acquire it. Thankfully it seems that llama3 performance at this hardware level is very good and there\u0026rsquo;s minimal, perceivable slowdown as the context token count increases. I was experiencing speeds of 23 tokens per second in LM Studio and my chat focusing on writing a python script was remarkable.\nUsing the 8B model, I saw a great throughput of 38tok/s, which feels equal to or faster in most cases than popular online AI chatbots.\nResults Ok so, what did I ask llama3.1? This version of the model is fine tuned for instructional tasks, so for now, I asked for help improving a basic python script. This is a great way to test how current and accurate a model is. Here is my full transcript\u0026hellip;\nGood morning, I have the following python code that I want to improve:\nimport requests url = \u0026#34;https://demo.rading212.com/api/v0/equity/pies\u0026#34; headers = {\u0026#34;Authorization\u0026#34;: \u0026#34;SOMERANDOMAPIKEY\u0026#34;} response = requests.get(url, headers=headers) print(response.status_code) if response.status_code == 200: data = response.json() if data: print(data) else: print(response.status_code) I want to implement functions and threading so that I can run this code in a docker container. I want to pass important variables like the time between requests and the Authorization key. I also want to introduce a function which sends the data returned by the successful request to a postgres database. Here is what typical json response looks like:\n[{'id': 696969, 'cash': 0.48, 'dividendDetails': {'gained': 7.94, 'reinvested': 7.41, 'inCash': 0.46}, 'result': {'investedValue': 302.4, 'value': 318.35, 'result': 15.95, 'resultCoef': 0.0527}, 'progress': 0.3986, 'status': None}, {'id': 696968, 'cash': 0.12, 'dividendDetails': {'gained': 0.72, 'reinvested': 0.63, 'inCash': 0.09}, 'result': {'investedValue': 300.14, 'value': 404.68, 'result': 104.54, 'resultCoef': 0.3483}, 'progress': 0.0792, 'status': None}, {'id': 696867, 'cash': 0.55, 'dividendDetails': {'gained': 0.55, 'reinvested': 0.0, 'inCash': 0.55}, 'result': {'investedValue': 249.27, 'value': 278.99, 'result': 29.72, 'resultCoef': 0.1192}, 'progress': None, 'status': None}, {'id': 696868, 'cash': 0.0, 'dividendDetails': {'gained': 0.0, 'reinvested': 0.0, 'inCash': 0.0}, 'result': {'investedValue': 0, 'value': 0.0, 'result': 0.0, 'resultCoef': 0}, 'progress': None, 'status': None}, {'id': 2181613, 'cash': 0.0, 'dividendDetails': {'gained': 0.0, 'reinvested': 0.0, 'inCash': 0.0}, 'result': {'investedValue': 0, 'value': 0, 'result': 0, 'resultCoef': 0}, 'progress': None, 'status': None}]\nI can help you improve your code by implementing functions and threading as per your requirements. Here\u0026rsquo;s an improved version of the code:\nimport requests from concurrent.futures import ThreadPoolExecutor import time import psycopg2 class EquityAPI: def __init__(self, url, headers): self.url = url self.headers = headers def get_equity(self): response = requests.get(self.url, headers=self.headers) return response.json() def send_to_postgres(self, data): conn = psycopg2.connect( host=\u0026#39;your_host\u0026#39;, database=\u0026#39;your_database\u0026#39;, user=\u0026#39;your_user\u0026#39;, password=\u0026#39;your_password\u0026#39; ) cur = conn.cursor() for item in data: query = \u0026#34;INSERT INTO your_table (id, cash, dividend_details) VALUES (%s, %s, %s)\u0026#34; values = (item[\u0026#39;id\u0026#39;], item[\u0026#39;cash\u0026#39;], str(item[\u0026#39;dividendDetails\u0026#39;])) cur.execute(query, values) conn.commit() conn.close() def main(): url = \u0026#39;https://demo.rading212.com/api/v0/equity/pies\u0026#39; headers = {\u0026#34;Authorization\u0026#34;: \u0026#34;SOMERANDOMAPIKEY\u0026#34;} api = EquityAPI(url, headers) while True: data = api.get_equity() if len(data) \u0026gt; 0 and data[0]: print(\u0026#34;Received data:\u0026#34;, data) # Send to Postgres api.send_to_postgres(data) time.sleep(10) # Wait for 10 seconds before next request if __name__ == \u0026#34;__main__\u0026#34;: main() In this improved code:\nWe\u0026rsquo;ve created a class EquityAPI that encapsulates the API functionality.\nThe get_equity() method sends an HTTP GET request to the specified URL and returns the JSON response.\nThe send_to_postgres() method connects to your Postgres database, inserts data into the table using SQL queries, and then commits and closes the connection.\nIn the main() function, we create an instance of the EquityAPI class and use a while loop with a 10-second sleep between requests.\nNote that you\u0026rsquo;ll need to replace \u0026lsquo;your_host\u0026rsquo;, \u0026lsquo;your_database\u0026rsquo;, \u0026lsquo;your_user\u0026rsquo;, and \u0026lsquo;your_password\u0026rsquo; in the Postgres connection string with your actual database credentials. Also, make sure to update your_table name according to your schema.\nAs this will be running in a container, TIME_BETWEEN_REQUESTS (time.sleep(10)) and AUTHORIZATION_KEY will be environment variables, can you adjust the code to load them from env?\nI\u0026rsquo;ve updated the code to use environment variables for TIME_BETWEEN_REQUESTS and AUTHORIZATION_KEY. Here\u0026rsquo;s the revised version:\nimport requests from concurrent.futures import ThreadPoolExecutor import time import psycopg2 import os class EquityAPI: def **init**(self, url): self.url = url self.headers = {\u0026#34;Authorization\u0026#34;: os.environ.get(\u0026#34;AUTHORIZATION_KEY\u0026#34;)} def get_equity(self): response = requests.get(self.url, headers=self.headers) return response.json() def send_to_postgres(self, data): conn = psycopg2.connect( host=os.environ.get(\u0026#39;POSTGRES_HOST\u0026#39;), database=os.environ.get(\u0026#39;POSTGRES_DB\u0026#39;), user=os.environ.get(\u0026#39;POSTGRES_USER\u0026#39;), password=os.environ.get(\u0026#39;POSTGRES_PASSWORD\u0026#39;) ) cur = conn.cursor() for item in data: query = \u0026#34;INSERT INTO your_table (id, cash, dividend_details) VALUES (%s, %s, %s)\u0026#34; values = (item[\u0026#39;id\u0026#39;], item[\u0026#39;cash\u0026#39;], str(item[\u0026#39;dividendDetails\u0026#39;])) cur.execute(query, values) conn.commit() conn.close() def main(): url = \u0026#39;https://demo.rading212.com/api/v0/equity/pies\u0026#39; api = EquityAPI(url) while True: data = api.get_equity() if len(data) \u0026gt; 0 and data[0]: print(\u0026#34;Received data:\u0026#34;, data) # Send to Postgres api.send_to_postgres(data) time.sleep(int(os.environ.get(\u0026#39;TIME_BETWEEN_REQUESTS\u0026#39;))) # Wait for TIME_BETWEEN_REQUESTS seconds before next request if **name** == \u0026#34;**main**\u0026#34;: main() In this revised code:\nWe\u0026rsquo;ve removed the headers parameter from the EquityAPI constructor and instead use os.environ.get(\u0026ldquo;AUTHORIZATION_KEY\u0026rdquo;) to load the authorization key from an environment variable.\nSimilarly, we\u0026rsquo;re using time.sleep(int(os.environ.get(\u0026lsquo;TIME_BETWEEN_REQUESTS\u0026rsquo;))) to wait for the specified time between requests.\nMake sure to set these environment variables in your container\u0026rsquo;s configuration. You can do this by adding a .env file with the following content:\nAUTHORIZATION_KEY=SOMERANDOMAPIKEY POSTGRES_HOST=your_host POSTGRES_DB=your_database POSTGRES_USER=your_user POSTGRES_PASSWORD=your_password TIME_BETWEEN_REQUESTS=10 Replace your_host, your_database, your_user, and your_password with your actual Postgres database credentials.\nFeel free to ask if there\u0026rsquo;s anything else I can help you improve!\nI will be using postgres as a source for grafana, please can you include the current time in the postgres INSERT command?\nHere is the revised code that includes the current timestamp in the Postgres insert statement:\nimport requests from concurrent.futures import ThreadPoolExecutor import time import psycopg2 import os from datetime import datetime class EquityAPI: def __init__(self, url): self.url = url self.headers = {\u0026#34;Authorization\u0026#34;: os.environ.get(\u0026#34;AUTHORIZATION_KEY\u0026#34;)} def get_equity(self): response = requests.get(self.url, headers=self.headers) return response.json() def send_to_postgres(self, data): conn = psycopg2.connect( host=os.environ.get(\u0026#39;POSTGRES_HOST\u0026#39;), database=os.environ.get(\u0026#39;POSTGRES_DB\u0026#39;), user=os.environ.get(\u0026#39;POSTGRES_USER\u0026#39;), password=os.environ.get(\u0026#39;POSTGRES_PASSWORD\u0026#39;) ) cur = conn.cursor() for item in data: query = \u0026#34;INSERT INTO your_table (id, cash, dividend_details, timestamp) VALUES (%s, %s, %s, %s)\u0026#34; values = (item[\u0026#39;id\u0026#39;], item[\u0026#39;cash\u0026#39;], str(item[\u0026#39;dividendDetails\u0026#39;]), datetime.now().strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;)) cur.execute(query, values) conn.commit() conn.close() def main(): url = \u0026#39;https://demo.rading212.com/api/v0/equity/pies\u0026#39; api = EquityAPI(url) while True: data = api.get_equity() if len(data) \u0026gt; 0 and data[0]: print(\u0026#34;Received data:\u0026#34;, data) # Send to Postgres api.send_to_postgres(data) time.sleep(int(os.environ.get(\u0026#39;TIME_BETWEEN_REQUESTS\u0026#39;))) # Wait for TIME_BETWEEN_REQUESTS seconds before next request if __name__ == \u0026#34;__main__\u0026#34;: main() In this revised code:\nWe\u0026rsquo;ve added a timestamp column to the Postgres insert statement.\nThe timestamp is generated using datetime.now().strftime(\u0026quot;%Y-%m-%d %H:%M:%S\u0026quot;), which returns the current date and time in the format \u0026ldquo;YYYY-MM-DD HH:MM:SS\u0026rdquo;.\nMake sure your Postgres table has a timestamp column with the correct data type.\nOne last task, can you help me write a docker compose file which will create a container to run our python code, a container for postgres (where our python script will insert to) including an init script to make the table, and a grafana instance? Can you include any environment variables which we will need to pass to our python script such as database_url and table_name\nHere is a Docker Compose file that creates three containers: one for your Python code, another for Postgres with an initial script to create the table, and finally a Grafana instance:\nversion: \u0026#34;3\u0026#34; services: api: build: . environment: - AUTHORIZATION_KEY=SOMERANDOMAPIKEY - POSTGRES_HOST=postgres - POSTGRES_DB=mydatabase - POSTGRES_USER=myuser - POSTGRES_PASSWORD=mypassword - TIME_BETWEEN_REQUESTS=10 depends_on: - postgres postgres: image: postgres environment: - POSTGRES_USER=myuser - POSTGRES_PASSWORD=mypassword - POSTGRES_DB=mydatabase volumes: - ./init.sql:/docker-entrypoint-initdb.d/init.sql ports: - \u0026#34;5432:5432\u0026#34; grafana: image: grafana/grafana environment: - GF_SECURITY_ADMIN_PASSWORD=grafanapassword depends_on: - postgres volumes: init.sql: This Docker Compose file:\nCreates a container named api that runs your Python code.\nSets the necessary environment variables for your script, including database credentials and time between requests.\nDepends on the Postgres service to ensure it\u0026rsquo;s running before starting the API.\nDefines another container called postgres, which uses an official PostgreSQL image. It sets up a new user, password, and database name as specified in the environment variables section of this file.\nMounts a volume for your initial SQL script (init.sql) at /docker-entrypoint-initdb.d/init.sql to run it during Postgres initialization.\nHere\u0026rsquo;s what you should put inside ./init.sql:\nCREATE TABLE mytable ( id SERIAL PRIMARY KEY, cash DECIMAL(10,2), dividend_details JSONB NOT NULL DEFAULT \u0026#39;{}\u0026#39;::JSONB, timestamp TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP ); This SQL script creates a table named mytable with the columns you specified in your Python code.\nRemember to replace placeholders like SOMERANDOMAPIKEY, mydatabase, myuser, and mypassword with actual values for your application. Also, update any other environment variables as needed according to their usage within your script.\nIMPRESSIVE!\nConclusion The responses are even faster than llama3 and the responses seem moderately more accurate. In comparison to previous tests, there are some observed improvements when we pivoted from python to docker compose YAML. The indicator has certainly turned slight more towards concern for programmers jobs\u0026hellip;\nTLDR Llama3.1 another strong increment if you are willing to disregard the insane computer required to train it. THe move toward open source is welcome will hopefully force more innovation and reduce \u0026lsquo;moat digging\u0026rsquo; for paid services\n","permalink":"https://exitcode0.net/posts/testing-llama3_1-with-gpt4all/","summary":"MetaAI have done it again\u0026hellip; Dropping llama3.1 under the (mostly) open source license and I have been putting it through its paces and benchmarking to find out how well it performs to models preiously tested: Testing Llama3 With LM Studio. Previously we took a look at llama3 in LM Studio which has made self hosted LLMs that rival paid services like ChatGPT and Claude possible; this time we are taking a look at llama through the lend of GPT4All.","title":"Testing Llama3.1 (8B) With LM Studio"},{"content":"Let\u0026rsquo;s face it, cats are demanding creatures. They expect their food to be served on time, and they\u0026rsquo;re not afraid to let you know when it\u0026rsquo;s late. But what if you trigger the feeding action to our Home Assistant dashboard? In this post, we\u0026rsquo;ll explore how to use Home Assistant to control a Tuya-compatible cat feeder, ensuring your feline friend never goes hungry. Get ready to unleash the power of automation and give your cat the royal treatment they deserve!\nBonus content: I try to teach my cat how to trigger the feeding script with a zigbee button.\nCredit: lalka_lapka\nAdding the Tuya Integration in Home Assistant (2024.6.4) Firstly let\u0026rsquo;s assume that you already have the Tuya app setup and your cat\u0026rsquo;s feeder configured. You can set the feeding schedule there or manually trigger a defined number of portions to drop into the bowl.\nIdeally, this would be something that can be done in Home Assistant on a separate schedule, or manually through a dashboard. The goal here is to have 1 less third party app to interact with on a daily basis. SO, let\u0026rsquo;s add the integration in Home Assistant:\nFull setup instructions can be found here: Tuya HA integration\nIn the Tuya app:\nOn the tab bar, select Me. Select the ⚙️ (cog) icon in the top-right corner. Account and Security. At the bottom, User Code will be shown; you need to when setting up this integration. Enter this code into the integration setup popup, then a QR code will appear. In the Tuya app (on the Home tab), press the + (plus) icon in the top-right corner. Choose Scan. Scan the QR code on the integration setup popup. Confirm the new device in the Tuya App. Your Tuya devices, including the cat feeder should populate into Home Assistant.\nPlease note, this is a cloud dependant integration - If Tuya cloud changed their ToS, rate limits your API connections, there is a possibility that the cat might wake oyu up at 5am demanding food.\nCreate a feeding script Whether we want to manually feed the beast with a dashboard button or we want to dispense kibble on a schedule, we must have a callable script. Here is how we do this\u0026hellip;\nNavigate to the Tuya device. Press the + button next to scripts to create a new script with the device. In the script editor, delete any auto generated sequences. Add a new action \u0026gt; Other Actions \u0026gt; Call a service. Choose or type number.set_value service. Press Choose Entity and select you cat feeder. Tick the value option and set to the number of portions you want to drop - 1. Save the script, giving it an appropriate name. Feeding script YAML Code alias: Feed The Kitten sequence: - service: number.set_value target: entity_id: number.mochifeeder_feed #replace this with you feeder entity id data: value: \u0026#34;1\u0026#34; #the number of portions you want to drop into the bowl description: \u0026#34;Feed the cat\u0026#34; icon: mdi:cat Now we can call this script in a number of ways\u0026hellip;\nCreating a dashboard button My feeder already has a schedule configured from the Tuya app, but sometimes I want to treat the cat to some extra food. A button on my (Home Assistant Dashboard display)[https://exitcode0.net/posts/home-assistant-firehd8-dashboard/] is perfect for this.\nAdd a button and set the tap action to Call Service, set that service value to that of the script we made above. You can set the entity to this script name as well, but it\u0026rsquo;s the action that is important.\nDashboard button YAML Code show_name: true show_icon: true type: button tap_action: action: call-service service: script.feed_the_kitten target: {} entity: script.feed_the_kitten show_state: false hold_action: action: more-info Creating a feeding automation schedule If you want to avoid using the feeding scheduler in the Tuya app or perhaps want to integrate a feeding schedule into other Home Assistant benifits such as presence detection, here some sample code for how you might want to do this\u0026hellip;\nHere we are feeding at 5am and 5pm every day. You could choose to make another script with an increased portion size, or call the service action multiple times.\nAutomation YAML Code description: \u0026#34;\u0026#34; mode: single trigger: - platform: time_pattern hours: \u0026#34;5\u0026#34; minutes: \u0026#34;0\u0026#34; seconds: \u0026#34;0\u0026#34; - platform: time_pattern hours: \u0026#34;17\u0026#34; minutes: \u0026#34;0\u0026#34; seconds: \u0026#34;0\u0026#34; condition: [] action: - service: script.feed_the_kitten data: {} Bonus Content: Teach the cat to feed itself at 5am So perhaps we can give the cat a zigbee button that triggers that script or maybe we can use a Ikea Tradfi PIR sensor? To be sure that she doesn\u0026rsquo;t learn how to game the system, I\u0026rsquo;ve also used a counter (helper) limiting 6 drops per day.\nHere are Mochi\u0026rsquo;s efforts to feed herself\u0026hellip; she soon figured out that the sensor = food.\n","permalink":"https://exitcode0.net/posts/feeding-my-cat-with-home-assistant/","summary":"Let\u0026rsquo;s face it, cats are demanding creatures. They expect their food to be served on time, and they\u0026rsquo;re not afraid to let you know when it\u0026rsquo;s late. But what if you trigger the feeding action to our Home Assistant dashboard? In this post, we\u0026rsquo;ll explore how to use Home Assistant to control a Tuya-compatible cat feeder, ensuring your feline friend never goes hungry. Get ready to unleash the power of automation and give your cat the royal treatment they deserve!","title":"Feeding My Cat With Home Assistant"},{"content":"I have covered Home Assistant (HA) content a number of times in previous posts, and am I a strong advocate for this project. The home automation scene is a wash with consumer offerings, but no single silicone valley big hitter has managed to provide a fully interoperable solution; most certainly not an affordable one. Ultimately, everyone has there own idea of how home automation should work. Home Assistant doesn\u0026rsquo;t solve the problem of standards dilution, but it offers an extremely robust way to tie the wide variety of standards and products together.\nSome related posts which you might want to save for later:\nHome Assistant HTTPS Certificates with Tailscale, Traefik and CoreDNS Home Assistant Enable MagicDNS and HTTPS Certificates in Tailscale How to Use a Docker Compose File for Wyoming Whisper Setting up Wyoming Piper with Docker Compose Home Assistant Introduction Home Assistant (HA) allows you to connect your various home automation systems as \u0026lsquo;Integrations\u0026rsquo;, then you can start to build routines, scene, and actions dependant on a vast range of conditions or controls. As the project has matured, the options and abilities of Home Assistant have reached a point where they are only limited by your imagination. There is also a fantastic community would are constantly pushing the boundaries and sharing their experiences and knowledge over at the HA forums: https://community.home-assistant.io/.\nDashboards Once your integrations on setup, a common way to interface with them is through the HA lovelace dashboards that you can build; particularly if you have sensors which are collecting data about your home environment.\nUsers It is often the case that there is one home automation enthusiast in the household and everyone else just wants to turn the lights without the needing to understand what a zigbee network is\u0026hellip; Home Assistant allows you to configure multiple users within the configuration. Users can then be given a role, user/admin, and be allocated access to only the lovelace dashboards that they need. For example: your partner only needs to be given access to light switch toggles and not access to the smart plugs which power your home server.\nWe can also create a user with need-to-know dashboard access for our tablet. In 99% of scenarios, this tablet will be left in an easy to access place, where even guests can interact with it. Giving the \u0026rsquo;tablet_user\u0026rsquo; account read only access to limited lovelace dashboards means that they cannot edit HA settings, or see excessive data.\nHere is how we limit dashboard access for this tablet account:\nEdit the dashboard, edit the view, locate the visibility tab and modify the toggle options, before hitting the save button. Remember to refresh all your other dashboard displaying devices to apply the changes.\nRooms Similarly, if we have multiple tablets, possible one per room, we can create users on a per room basis and use access controls to set a default dashboard for that room. Room specific dashboards might include shopping list cards in the kitchen or light and CCTV controls in the bedroom. The possibilities are almost endless and the best path to get the most out of the system would be to build automations and scenes for each room; then add controls or triggers for these on that room\u0026rsquo;s dashboard.\nAmazon FireHD8 Control Panel This where we reach the main focus of this article, testing an Amazon FireHD8 Plus (with ads) as a dashboard control panel.\nI chose the \u0026lsquo;with ads\u0026rsquo; version of the FireHD8, which is ~£10 cheaper. Here are some resources for removing the bloat and getting GApps and the HA app installed in Kiosk mode:\nFire Toolbox V34.0: https://xdaforums.com/t/windows-linux-tool-fire-toolbox-v34-0.3889604/ Fire Toolbox tutorial: https://liliputing.com/hack-your-amazon-fire-tablet-with-fire-toolbox-v10/ The fireHD8/8+ is restricted meaning that Amazon has locked down the firmware. When a firmware is restricted it means we cannot fully block OTAs, use custom launchers without Fire Launcher, lockscreen ads can no longer be permanently removed, and some system apps can\u0026rsquo;t be disabled. Even though a tablet is marked as Restricted in the table above, sometimes it may only apply to certain versions of FireOS. If your tablet is on a firmware before the listed firmware then your tablet is NOT restricted.\nIt is still possible to disable some of the fluff, and side-load GAPPs (Play Store).\nReach out if you need help with this:\n\u0026#x1f426;/\u0026#x2716;\u0026#xfe0f;: Tweet X me\n\u0026#x1f4f7;: Instagram direct message\n\u0026#x1f4e7;: Drop me an email\nPros Cost The FireHD8 is relatively cheap and can often be had a very reasonable price on the various Prime sales which occur through the year. And if you are willing to by used, the Amazon warehouse deals make these tablets an even more affordable option. The build quality is understandably poor, but there Fire tablets are a known quantity when compared to the \u0026rsquo;no name\u0026rsquo; android tablet alternatives.\nLowest Ever: £61.99 (Nov 18, 2022) - Unlikely to be this price again due to inflation\nCurrent: £119.99 (May 16, 2024)\nDocking Station NOTE: this only applies to the FireHD8 Plus.\nThe goal of the tablet is to look like an intentional home automation control panel, so the official docking station with wireless (a concealed DC power adapter is still needed for the dock) charging is a relative stylish addition to your space. If we intend to place multiple tablets in multiple rooms, a clean consistent integration is important (to some people).\nCons Screen Resolution The FireHD8 (and FireHD8 Plus) has an 8 inch screen with a resolution of 1280 x 800 (189 ppi). This works out to be a on the border between just enough and too little screen real estate for the official Home Assistant app, when running in full screen mode. You can optimise your dashboards with smaller card styles to fit more data and controls into a single viewport, but if you are looking to add an entire home\u0026rsquo;s worth of controls, you will probably fail to do so.\nI\u0026rsquo;d recommend looking into this HACS UI plugin for more dashboard design flexibility, in particular\nThere are other Fire tablets out there with higher resolution displays, but unfortunately, they don\u0026rsquo;t support the wireless charging protocol and this means that there is no dock with a concealed power cable.\nAmazon Bloat and Ads This is a very small problem if you never intend to lock the device or use it for anything other than a Home Assistant Dashboard display, but it is worth mentioning that should you ever wish to repurpose the device, you will need to pay a higher price to remove embedded adds on the tablet UI. This type of behaviour from Amazon clearly demonstrates how little they respect the user experience if they are willing to sell the same hardware with a different software image that is littered with adverts.\nWall Mounting Like many android tablets, they were never really designed with a wall mounted use case in mind and you might struggle to find a tidy wall mounting solution; particular one where the power cables are concealed.\nIt would be fantastic to emulate control devices such as those offered in Control4 setups, at a fraction of the cost and using the Home Assistant as the basis of our automation. At the time of writing this, the best option for wall mounting is 3D printed enclosures.\nThere are some novel 3D printed solutions for wall mounting:\nhttps://www.printables.com/model/383856-wall-mount-for-fire-tablet-hd-10-11th-gen-2021 Alternatives There are more cheap android tablets than you can shake a stick at, but your mileage may vary when it comes down to quality and suitability for this use case. Here are some alternatives that I have tested\u0026hellip;\nSamsung Tab S6 Lite Possibly not the cheapest option, but I have one and it would be remiss of me to not test it. The Tab S6 lite is a few year old now, but still available, offering a few benefits over the FireHD8. The build quality is superior and the screen resolution is higher, meaning that you can fit more dashboard items on screen without scrolling.\nThere is no wireless charging option for the Tab S6 lite, so unfortunately, you\u0026rsquo;re going to need a USB-C cable sticking out the side. I opted for the cheap, yet stylish Ikea bamboo tablet stand, but unfortunately, I can\u0026rsquo;t allow this visible wire to exist beyond my office desk. If you want to perpetuate the sunk cost fallacy, there are always adapters to hide the USB cable:\n180 Degree USB-C Adapter - https://www.amazon.co.uk/gp/aw/d/B0BXCTRN7F\nSlim USB C to A Power Cable - https://www.etsy.com/listing/771904711/smart-home-mount-slim-usb-power-cable\nRaspberry Pi Official Touch Screen If you are into home automation, there\u0026rsquo;s is a strong probability that you have an unloved Raspberry Pi sitting in a draw somewhere. I\u0026rsquo;m here to tell you that it could be put to work as a home assistant dashboard host. The official touch screen and case allow us to build a dashboard device that we can customise to include sensors, using spare GPIO pins.\nHowever, the huge downside is once again, a low screen resolution; extenuated by smaller overall dimensions. At least with the Pi, you are running in a full screen chromium instance, so you can define the zoom level, but you soon reach a point where it becomes unusable.\nI have taken the choice to resign this touch screen to my 3D print area, where it runs the OctoPrint service and displays a dashboard with minimal, 3D printing related information. To get the HA dashboard to open on Pi boot up, we can use the following LXDE autostart entry:\nsudo nano /etc/xdg/lxsession/LXDE-pi/autostart At the end of the file, add the following, adjusting for your home assistant IP/hostname and dashboard URL:\n... #End of file @chromium-browser --kiosk https://homeassistant:8123/lovelace/default_view? You will need to sign on the first visit, but enabling remember on sign-in will cache the authentication details in the chromium browser. You can set the zoom level of the webpage using Ctrl - or Ctrl+, this will also be stored in chromium appdata.\nAn added benefit of the Pi is that you can SSH or VNC into it, take make changes like this quite easily\u0026hellip; it\u0026rsquo;s still just a Linux computer.\nConclusion I\u0026rsquo;m still on the lookout for a sensible alternative to my FireHD8, with fewer drawbacks and compromises, but still at an affordable price point. If you can find the tablet hardware that fits your needs, they make fantastic Home Assistant controllers and go a long way to helping your setup pass the family/friends test - where anyone can walk into your home (with an invite) and interact with your smart home.\nAs the HA project progresses, I expect that someone will see the need for a dedicated dashboard/control device which offers wall mounting or neat side table aesthetics, without compromises on software, screen quality and at an strong value price point. Until then, the Amazon FireHD8 tablet is a strong candidate for use as a Home Assistant controller or dashboard viewing device, with very few compromises or caveats for most consumers. If you have a large number of dashboard cards, perhaps take a look at something with a higher DPI.\n","permalink":"https://exitcode0.net/posts/home-assistant-firehd8-dashboard/","summary":"I have covered Home Assistant (HA) content a number of times in previous posts, and am I a strong advocate for this project. The home automation scene is a wash with consumer offerings, but no single silicone valley big hitter has managed to provide a fully interoperable solution; most certainly not an affordable one. Ultimately, everyone has there own idea of how home automation should work. Home Assistant doesn\u0026rsquo;t solve the problem of standards dilution, but it offers an extremely robust way to tie the wide variety of standards and products together.","title":"Using an Amazon FireHD8 as a Home Assistant Dashboard Control Panel"},{"content":"MetaAI have just introduced Llama 3 to the world and the open source community is already putting it through its paces and pushing to find the limits of what it can produce. One fantastic tool which has made self hosted LLMs that rival paid services like ChatGPT and Claude possible is LM Studio. With version 0.2.20 they bough about support for llama 3 along with a GGUF quantized 8 billion parameter version of the model.\nInstalling the model The beautiful thing about LM studio is how simple it is to get up and running. This project has made brilliant progress towards bringing local LLMs to the world. I feel that it is of upmost importance that projects like LM studio and Jan continue to grow and prevent services like OpenAI\u0026rsquo;s ChatGPT from building a paywall around the technological revolution that is large language models. There will always be a place for paid version of this technology, but it cannot exists as the only available option.\nInstalling the new llama3 model is just as simple as ever in LM Studio. Once you have updated to version 0.2.20, you will see the lmstudio-community model on the home page. However, clicking download from here, might not given you the highest quantized version that your system is capable of running. Instead click the search icon and you will be directed to a page which lists all available version of that model.\nInstall llama3 model in LM Studio: Finding the model\nNow you can selected the version that best suits your needs or system profile. I have a GPU with 12GB VRAM and can therefore enable full GPU offload for the Meta-Llama-3-8B-Instruct-Q8_0.gguf (8.54 GB) version of the model. Hit download on the model version you want to grab.\nInstall llama3 model in LM Studio: Installing the best model\nNow head over to the chat tab (on the left) and select the llama 3 model. At this point you might be prompted to accept a new prompt or keep your existing prompt. You want to accept the new prompt.\nInstall llama3 model in LM Studio: Loading the model\nDefault llama3 prompt Our default llama3 prompt in LM Studio looks like this:\n{ \u0026#34;name\u0026#34;: \u0026#34;Llama 3\u0026#34;, \u0026#34;inference_params\u0026#34;: { \u0026#34;input_prefix\u0026#34;: \u0026#34;\u0026lt;|start_header_id|\u0026gt;user\u0026lt;|end_header_id|\u0026gt;\\n\\n\u0026#34;, \u0026#34;input_suffix\u0026#34;: \u0026#34;\u0026lt;|eot_id|\u0026gt;\u0026lt;|start_header_id|\u0026gt;assistant\u0026lt;|end_header_id|\u0026gt;\\n\\n\u0026#34;, \u0026#34;pre_prompt\u0026#34;: \u0026#34;You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user\u0026#39;s requests to the best of your ability.\u0026#34;, \u0026#34;pre_prompt_prefix\u0026#34;: \u0026#34;\u0026lt;|start_header_id|\u0026gt;system\u0026lt;|end_header_id|\u0026gt;\\n\\n\u0026#34;, \u0026#34;pre_prompt_suffix\u0026#34;: \u0026#34;\u0026lt;|eot_id|\u0026gt;\u0026#34;, \u0026#34;antiprompt\u0026#34;: [\u0026#34;\u0026lt;|start_header_id|\u0026gt;\u0026#34;, \u0026#34;\u0026lt;|eot_id|\u0026gt;\u0026#34;] } } I chose to set full GPU utilisation, keep the entire model in RAM and bumped the context length all the way up to the support maximum: 8192.\nPrompt configuration for llama3 model in LM Studio\nAs I will discus in the upcoming performance section, I did hit the context limit, which ultimately lead to some less satisfactory output.\nPerformance Let me start by providing some system specs as context. I have by 2024 standards, a modest desktop computer:\nRyzen 5600 6c/12t CPU 32GB 3200MHz DDR4 RAM Nvidia RTX 3060 12GB GDDR6 GPU It would be great to test these models on bleeding edge hardware, enterprise hardware and even Apple M3 silicon, but hardware isn\u0026rsquo;t cheap these days and I, like many, don\u0026rsquo;t have the resources to acquire it. Thankfully it seems that llama3 performance at this hardware level is very good and there\u0026rsquo;s minimal, perceivable slowdown as the context token count increases. I was experiencing speeds of 23 tokens per second in LM Studio and my chat focusing on writing a python script was remarkable.\nPerformance of llama3 model in LM Studio\nResults Ok so, what did I ask llama3? This version of the model is fine tuned for instructional tasks, so for now, I asked for help improving a basic python script. This is a great way to test how current and accurate a model is. Here is my full transcript\u0026hellip;\nGood morning, I have the following python code that I want to improve:\nimport requests url = \u0026#34;https://demo.rading212.com/api/v0/equity/pies\u0026#34; headers = {\u0026#34;Authorization\u0026#34;: \u0026#34;SOMERANDOMAPIKEY\u0026#34;} response = requests.get(url, headers=headers) print(response.status_code) if response.status_code == 200: data = response.json() if data: print(data) else: print(response.status_code) I want to implement functions and threading so that I can run this code in a docker container. I want to pass important variables like the time between requests and the Authorization key. I also want to introduce a function which sends the data returned by the successful request to a postgres database. Here is what typical json response looks like:\n[{'id': 696969, 'cash': 0.48, 'dividendDetails': {'gained': 7.94, 'reinvested': 7.41, 'inCash': 0.46}, 'result': {'investedValue': 302.4, 'value': 318.35, 'result': 15.95, 'resultCoef': 0.0527}, 'progress': 0.3986, 'status': None}, {'id': 696968, 'cash': 0.12, 'dividendDetails': {'gained': 0.72, 'reinvested': 0.63, 'inCash': 0.09}, 'result': {'investedValue': 300.14, 'value': 404.68, 'result': 104.54, 'resultCoef': 0.3483}, 'progress': 0.0792, 'status': None}, {'id': 696867, 'cash': 0.55, 'dividendDetails': {'gained': 0.55, 'reinvested': 0.0, 'inCash': 0.55}, 'result': {'investedValue': 249.27, 'value': 278.99, 'result': 29.72, 'resultCoef': 0.1192}, 'progress': None, 'status': None}, {'id': 696868, 'cash': 0.0, 'dividendDetails': {'gained': 0.0, 'reinvested': 0.0, 'inCash': 0.0}, 'result': {'investedValue': 0, 'value': 0.0, 'result': 0.0, 'resultCoef': 0}, 'progress': None, 'status': None}, {'id': 2181613, 'cash': 0.0, 'dividendDetails': {'gained': 0.0, 'reinvested': 0.0, 'inCash': 0.0}, 'result': {'investedValue': 0, 'value': 0, 'result': 0, 'resultCoef': 0}, 'progress': None, 'status': None}]\nI\u0026rsquo;d be happy to help you improve your code. Here is a refactored version of your code using functions and threading:\nimport requests import threading import time import psycopg2 # Configuration variables TIME_BETWEEN_REQUESTS = 5 # in seconds AUTHORIZATION_KEY = \u0026#34;SOMERANDOMAPIKEY\u0026#34; class Trading212API: def __init__(self, authorization_key): self.authorization_key = authorization_key def get_pies(self): url = \u0026#34;https://demo.rading212.com/api/v0/equity/pies\u0026#34; headers = {\u0026#34;Authorization\u0026#34;: self.authorization_key} response = requests.get(url, headers=headers) return response.json() class DatabaseWriter: def __init__(self, database_url, table_name): self.database_url = database_url self.table_name = table_name def write_data_to_db(self, data): conn = psycopg2.connect(self.database_url) cur = conn.cursor() for item in data: cur.execute(\u0026#34;INSERT INTO {} (id, cash, dividend_details, result, progress, status)\u0026#34;.format(self.table_name), (item[\u0026#39;id\u0026#39;], item[\u0026#39;cash\u0026#39;], item[\u0026#39;dividendDetails\u0026#39;], item[\u0026#39;result\u0026#39;], item[\u0026#39;progress\u0026#39;], item[\u0026#39;status\u0026#39;])) conn.commit() conn.close() def send_request_and_write_to_db(api, db_writer): while True: data = api.get_pies() if data: db_writer.write_data_to_db(data) time.sleep(TIME_BETWEEN_REQUESTS) if __name__ == \u0026#34;__main__\u0026#34;: api = Trading212API(AUTHORIZATION_KEY) db_writer = DatabaseWriter(\u0026#34;host=localhost port=5432 dbname=mydatabase user=myuser password=mypassword\u0026#34;, \u0026#34;mytable\u0026#34;) thread = threading.Thread(target=send_request_and_write_to_db, args=(api, db_writer)) thread.start() Here\u0026rsquo;s what I did:\nExtracted the API key and other configuration variables to separate constants. Created a Trading212API class that encapsulates the API request logic. Created a DatabaseWriter class that encapsulates the database writing logic. Introduced a send_request_and_write_to_db function that sends an API request and writes the response to the database. This function is run in a separate thread using threading. In the main code, I create instances of the Trading212API and DatabaseWriter classes and start the thread. Note: You\u0026rsquo;ll need to install the psycopg2 library to use this code with PostgreSQL. Also, make sure to replace the database URL and table name in the DatabaseWriter constructor with your actual values.\nAlso, you may want to consider handling errors and exceptions more robustly in the code. This is just a starting point, and you may need to add additional error handling depending on your specific requirements.\nAs this will be running in a container, TIME_BETWEEN_REQUESTS and AUTHORIZATION_KEY will be environment variables, can you adjust the code to load them from env?\nYes, I can help with that! Here\u0026rsquo;s an updated version of the code that loads the TIME_BETWEEN_REQUESTS and AUTHORIZATION_KEY from environment variables:\nimport requests import threading import time import psycopg2 from os import getenv # Configuration variables TIME_BETWEEN_REQUESTS = int(getenv(\u0026#34;TIME_BETWEEN_REQUESTS\u0026#34;, 5)) # in seconds AUTHORIZATION_KEY = getenv(\u0026#34;AUTHORIZATION_KEY\u0026#34;) class Trading212API: def __init__(self, authorization_key): self.authorization_key = authorization_key def get_pies(self): url = \u0026#34;https://demo.rading212.com/api/v0/equity/pies\u0026#34; headers = {\u0026#34;Authorization\u0026#34;: self.authorization_key} response = requests.get(url, headers=headers) return response.json() class DatabaseWriter: def __init__(self, database_url, table_name): self.database_url = database_url self.table_name = table_name def write_data_to_db(self, data): conn = psycopg2.connect(self.database_url) cur = conn.cursor() for item in data: cur.execute(\u0026#34;INSERT INTO {} (id, cash, dividend_details, result, progress, status)\u0026#34;.format(self.table_name), (item[\u0026#39;id\u0026#39;], item[\u0026#39;cash\u0026#39;], str(item[\u0026#39;dividendDetails\u0026#39;]), str(item[\u0026#39;result\u0026#39;]), item[\u0026#39;progress\u0026#39;], item[\u0026#39;status\u0026#39;])) conn.commit() conn.close() def send_request_and_write_to_db(api, db_writer): while True: data = api.get_pies() if data: db_writer.write_data_to_db(data) time.sleep(TIME_BETWEEN_REQUESTS) if __name__ == \u0026#34;__main__\u0026#34;: api = Trading212API(AUTHORIZATION_KEY) db_writer = DatabaseWriter(getenv(\u0026#34;DATABASE_URL\u0026#34;), getenv(\u0026#34;TABLE_NAME\u0026#34;)) thread = threading.Thread(target=send_request_and_write_to_db, args=(api, db_writer)) thread.start() In this code:\nWe\u0026rsquo;ve used the getenv function from Python\u0026rsquo;s os module to load the environment variables TIME_BETWEEN_REQUESTS and AUTHORIZATION_KEY. The default value for TIME_BETWEEN_REQUESTS is 5 seconds if it\u0026rsquo;s not provided. For loading database URL and table name, we\u0026rsquo;re using getenv function in the same way as above. Make sure to set these environment variables in your Docker container before running this script. By doing so, you can easily manage these settings from your environment or configuration files without having to modify the code itself.\nI will be using postgres as a source for grafana, please can you include the current time in the postgres INSERT command?\nHere\u0026rsquo;s an updated version of the write_data_to_db method that includes the current timestamp in the INSERT command:\nclass DatabaseWriter: def __init__(self, database_url, table_name): self.database_url = database_url self.table_name = table_name def write_data_to_db(self, data): conn = psycopg2.connect(self.database_url) cur = conn.cursor() for item in data: now = time.strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) # Get current timestamp cur.execute(\u0026#34;\u0026#34;\u0026#34; INSERT INTO {} (id, cash, dividend_details, result, progress, status, inserted_at) VALUES (%s, %s, %s, %s, %s, %s, %s) \u0026#34;\u0026#34;\u0026#34;.format(self.table_name), (item[\u0026#39;id\u0026#39;], item[\u0026#39;cash\u0026#39;], str(item[\u0026#39;dividendDetails\u0026#39;]), str(item[\u0026#39;result\u0026#39;]), item[\u0026#39;progress\u0026#39;], item[\u0026#39;status\u0026#39;], now)) conn.commit() conn.close() Can you please give me the full script with all current modifications?\nHere is the full script with all the modifications we\u0026rsquo;ve made:\nimport requests import threading import time import psycopg2 from os import getenv, environ import datetime environ[\u0026#39;TZ\u0026#39;] = \u0026#39;UTC\u0026#39; # Set timezone to UTC for timestamp conversion time.tzset() # Configuration variables TIME_BETWEEN_REQUESTS = int(getenv(\u0026#34;TIME_BETWEEN_REQUESTS\u0026#34;, 5)) # in seconds AUTHORIZATION_KEY = getenv(\u0026#34;AUTHORIZATION_KEY\u0026#34;) DATABASE_URL = getenv(\u0026#34;DATABASE_URL\u0026#34;) TABLE_NAME = getenv(\u0026#34;TABLE_NAME\u0026#34;) class Trading212API: def __init__(self, authorization_key): self.authorization_key = authorization_key def get_pies(self): url = \u0026#34;https://demo.rading212.com/api/v0/equity/pies\u0026#34; headers = {\u0026#34;Authorization\u0026#34;: self.authorization_key} response = requests.get(url, headers=headers) return response.json() class DatabaseWriter: def __init__(self, database_url, table_name): self.database_url = database_url self.table_name = table_name def write_data_to_db(self, data): conn = psycopg2.connect(self.database_url) cur = conn.cursor() for item in data: now = datetime.datetime.now().strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) # Get current timestamp cur.execute(\u0026#34;\u0026#34;\u0026#34; INSERT INTO {} (id, cash, dividend_details, result, progress, status, inserted_at) VALUES (%s, %s, %s, %s, %s, %s, %s) \u0026#34;\u0026#34;\u0026#34;.format(self.table_name), (item[\u0026#39;id\u0026#39;], item[\u0026#39;cash\u0026#39;], str(item[ \u0026#39;dividendDetails\u0026#39;]), str(item[\u0026#39;result\u0026#39;]), item[\u0026#39;progress\u0026#39;], item[\u0026#39;status\u0026#39;], now)) conn.commit() conn.close() def send_request_and_write_to_db(api, db_writer): while True: data = api.get_pies() if data: db_writer.write_data_to_db(data) time.sleep(TIME_BETWEEN_REQUESTS) if __name__ == \u0026#34;__main__\u0026#34;: api = Trading212API(AUTHORIZATION_KEY) db_writer = DatabaseWriter(DATABASE_URL, TABLE_NAME) thread = threading.Thread(target=send_request_and_write_to_db, args=(api, db_writer)) thread.start() This script should now work as expected. It loads the configuration variables from environment variables, makes API requests to the Trading212 API, writes the results to a PostgreSQL database with the current timestamp, and runs in an infinite loop with the specified interval between requests.\nI am getting this error because I am developing on windows: AttributeError: module 'time' has no attribute 'tzset'. Is there an alternative way to do this?\nYes, you\u0026rsquo;re correct that time.tzset() is not available in Windows. The tzset function is a Unix-specific function that sets the timezone environment variable.\nTo set the timezone on Windows, you can use the os.environ['TZ'] = 'UTC' line before calling time.tzset(). However, since time.tzset() is not available in Windows, this line will have no effect.\nSo, if you\u0026rsquo;re developing on Windows, you can simply remove the lines that set the timezone and use the datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') to get the current timestamp. This will give you the local system time without considering any timezone settings.\nHere is the modified code:\nimport requests import threading import time import datetime import psycopg2 from os import getenv, environ # Configuration variables TIME_BETWEEN_REQUESTS = int(getenv(\u0026#34;TIME_BETWEEN_REQUESTS\u0026#34;, 5)) # in seconds AUTHORIZATION_KEY = getenv(\u0026#34;AUTHORIZATION_KEY\u0026#34;) DATABASE_URL = getenv(\u0026#34;DATABASE_URL\u0026#34;) TABLE_NAME = getenv(\u0026#34;TABLE_NAME\u0026#34;) class Trading212API: def __init__(self, authorization_key): self.authorization_key = authorization_key def get_pies(self): url = \u0026#34;https://demo.rading212.com/api/v0/equity/pies\u0026#34; headers = {\u0026#34;Authorization\u0026#34;: self.authorization_key} response = requests.get(url, headers=headers) return response.json() class DatabaseWriter: def __init__(self, database_url, table_name): self.database_url = database_url self.table_name = table_name def write_data_to_db(self, data): conn = psycopg2.connect(self(database_url)) cur = conn.cursor() for item in data: now = datetime.datetime.now().strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) # Get current timestamp cur.execute(\u0026#34;\u0026#34;\u0026#34; INSERT INTO {} (id, cash, dividend_details, result, progress, status, inserted_at) VALUES (%s, %s, %s, %s, %s, %s, %s) \u0026#34;\u0026#34;\u0026#34; .format(self.table_name), (item[\u0026#39;id\u0026#39;], item[\u0026#39;cash\u0026#39;], str(item[ \u0026#39;dividendDetails\u0026#39;]), str(item[ \u0026#39;result\u0026#39;]), item[\u0026#39;progress\u0026#39;], item[\u0026#39;status\u0026#39;], now)) conn.commit() conn.close() def send_request_and_write_to_db(api, db_writer): while True: data = api.get_pies() if data: db_writer.write_data_to_db(data) time.sleep(TIME_BETWEEN_REQUESTS) if __name__ == \u0026#34;__main__\u0026#34;: api = Trading212API(AUTHORIZATION_KEY) db_writer = DatabaseWriter(DATABASE_URL, TABLE_NAME) thread = threading.Thread(target=send_request_and_write_to_db, args=(api, db_writer)) thread.start() This code should work as expected on both Unix and Windows.\nOne last task, can you help me write a docker compose file which will create a container to run our python code, a container for postgres (where our python script will insert to) including an init script to make the table, and a grafana instance? Can you include any environment variables which we will need to pass to our python script such as database_url and table_name\nHere is a Docker Compose file that creates three containers: one for your Python script, one for PostgreSQL with an initial script to create the table, and one for Grafana:\nversion: \u0026#34;3\u0026#34; services: db: image: postgres restart: always environment: POSTGRES_USER: myuser POSTGRES_PASSWORD: mypassword POSTGRES_DB: mydatabase volumes: - db-data:/var/lib/postgresql/data init-db: build: ./init-db depends_on: - db command: python init.py python-app: build: . depends_on: - db environment: DATABASE_URL: \u0026#34;postgresql://myuser:mypassword@db:5432/mydatabase\u0026#34; TABLE_NAME: \u0026#34;mytable\u0026#34; TIME_BETWEEN_REQUESTS: 5 AUTHORIZATION_KEY: \u0026#34;your-authorization-key\u0026#34; command: python app.py grafana: image: grafana/grafana restart: always ports: - \u0026#34;3000:3000\u0026#34; volumes: db-data: Here\u0026rsquo;s what this file does:\nIt creates a db service that runs the official PostgreSQL image, sets environment variables for the user, password, and database name, and mounts a volume to persist data. It creates an init-db service that builds from a directory named init-db in the current directory, depends on the db service, and runs a command to create the table using a Python script named init.py. It creates a python-app service that builds from the current directory, depends on the db service, sets environment variables for the database URL, table name, time between requests, and authorization key, and runs the command to start the Python app. It creates a grafana service that runs the official Grafana image, exposes port 3000, and restarts always. To use this file, create a directory named init-db in the same directory as your Docker Compose file, and add an init.py script to it with the following contents:\nimport psycopg2 conn = psycopg2.connect( host=\u0026#34;localhost\u0026#34;, database=\u0026#34;mydatabase\u0026#34;, user=\u0026#34;myuser\u0026#34;, password=\u0026#34;mypassword\u0026#34; ) cur = conn.cursor() cur.execute(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE IF NOT EXISTS mytable ( id SERIAL PRIMARY KEY, cash REAL, dividend_details TEXT, result REAL, progress REAL, status TEXT, inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); \u0026#34;\u0026#34;\u0026#34;) conn.commit() This script connects to the PostgreSQL database, creates a table named mytable with the specified columns if it doesn\u0026rsquo;t exist, and commits the changes.\nThen, you can run docker-compose up -d in your terminal to start all services in detached mode.\nConclusion The responses are very fast and accurate, the model takes care to consider all aspects of the prompt and for the most part holds onto the context of the conversation. When I pivoted towards asking about Docker configurations, things started to fall apart and mistakes were introduced. For example our postgres init script was never going to run without a Dockerfile and the practice of splitting this out into a stand alone services makes the deployment less idempotent. The robots won\u0026rsquo;t be taking out jobs just yet\u0026hellip;\nThey didn\u0026rsquo;t take our jerbs\u0026hellip; yet\nUltimately, I reach the token count and the responses started to drop previously fixed sections of code. This is a common issue with all LLM services and one that Clause 3 looks to solve with enormous context token values\u0026hellip; unfortunately this could get very expensive due to the token based pricing structure.\nThe people of the LM Studio discord server are begging for RAG type document referencing features, which would be great. But, relatively small context token counts will mean that this feature could quickly lead to model confusion.\nTLDR Llama3 is another step in the right direction and LM Studio once again proves to be an exceptional demonstration of free open source software, but there is always more work to be done\n","permalink":"https://exitcode0.net/posts/testing-llama3-with-lmstudio/","summary":"MetaAI have just introduced Llama 3 to the world and the open source community is already putting it through its paces and pushing to find the limits of what it can produce. One fantastic tool which has made self hosted LLMs that rival paid services like ChatGPT and Claude possible is LM Studio. With version 0.2.20 they bough about support for llama 3 along with a GGUF quantized 8 billion parameter version of the model.","title":"Testing Llama3 With LM Studio"},{"content":"As I write this, my computer\u0026rsquo;s Nvidia RTX GPU is busy converting watts into fictional images of robots and given the current cost of electricity, I am asking myself if the results it is producing are worth the resources required to produce them. In the article I hope to demonstrate how I am using AUTOMATIC1111/stable-diffusion-webui and Mikubill/sd-webui-controlnet to generate some borderline \u0026ldquo;art\u0026rdquo;. If you are looking to skip ahead to certain section, you can jump ahead using the table of contents above.\nPrompt: \u0026lsquo;A robot emerging from a cave\u0026rsquo;\nDefinitions and Abbreviations: Some common abbreviations and terms that you might encounter in this article or other sites whilst researching this topic:\nSD: stable-diffusion\na111: AUTOMATIC1111\nCFG: Classifier Free Guidance Scale\nSeed: A value that determines the output of random number generator - if you create an image with same parameters and seed as another image, you\u0026rsquo;ll get the same result\nSampling Steps: How many times to improve the generated image iteratively; higher values take longer; very low values can produce bad results\nCFG scale: how strongly the image should conform to prompt - lower values produce more creative results\nInstalling AUTOMATIC1111\u0026rsquo;s stable-diffusion-webui (Windows) AUTOMATIC1111\u0026rsquo;s stable-diffusion-webui is a web interface for Stable Diffusion, implemented using Gradio library. This project makes txt2img and img2img generation possible much like a lot of popular online services such as DALL·E and Midjourney, but instead allowing you to run it locally and for free (minus the cost of computer hardware and power). You should know that Stable Diffusion is in fact the product of Stability AI and AUTOMATIC1111\u0026rsquo;s stable-diffusion-webui allows us to make use of the open source models that Stability AI and many others have released.\nPre-Requisite Dependencies Before you get started, there are some additional components that you will need to install first. You may chose to download a newer version of python3, such as python 3.11.X; your mileage may vary and you might have some package compatibility issues.\nPython 3.10.6: https://www.python.org/downloads/release/python-3106/\nGit: https://git-scm.com/download/win\nCode from the AUTOMATIC1111/stable-diffusion-webui repository:\ngit clone https://github.com/AUTOMATIC1111/stable-diffusion-webui Once you have complete the above steps. Open a command prompt in your stable-diffusion-webui directory and run the following command:\n./webui-user.bat Addition arguments such as --api can be added if you want to run the tool with additional features enabled. These arguments can only be passed to the program by launching from the command prompt, rather than just running the .bat file from Windows explorer. A full ist of arguments: https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Command-Line-Arguments-and-Settings. The full command that I use to launch (with additional performance arguments) is as follows:\n.\\webui-user.bat --api --no-half --opt-split-attention --xformers Once the .bat completes the local packages and dependency installation, it will launch your default browser to http://127.0.0.1:7860/. Now you are ready to start generating \u0026ldquo;art\u0026rdquo; - be sure to keep your command prompt window running; closing this window will stop stable-diffusion-webui.\nAUTOMATIC1111/stable-diffusion-webui running on on our local machine.\nIf you are looking to run this on an Apple Silicon based system (M1/M2/M3), instructions can be found here: https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon\nAdding Models Models can often be referred to as \u0026ldquo;checkpoints\u0026rdquo;; they are files that contain a collection of neural network parameters and weights trained using images as inspiration. Models trained on a particular type of subject matter will have a better chance of creating similar images; want anime style images\u0026hellip; use a model trained on anime.\nYou can add any additional model files to your stable-diffusion-webui models path:\nFor example: C:\\AI\\stable-diffusion-webui\\models\\Stable-diffusion\nHere are some links to models that I have tested and a highly subjective and unscientific success rating:\nModel Repository URL Personal Success Rating v1-5-pruned-emaonly.safetensors huggingface.co/runwayml 3/5 stars (v2)512-base-ema.ckpt huggingface.co/stabilityai/stable-diffusion-2-base 3/5 stars v2-1_768-ema-pruned.ckpt huggingface.co/stabilityai/stable-diffusion-2-1 3/5 stars robo-diffusion-v1.ckpt huggingface.co/nousr/robo-diffusion 2/5 stars Chilloutmix-Ni-pruned-fp16-fix.safetensors huggingface.co/swl-models/chilloutmix-ni fp16 4/5 stars chilloutmix-Ni-non-ema-fp32.safetensors huggingface.co/swl-models/chilloutmix-ni non-ema-fp32 4/5 stars You should note that it is possible to use the SD Refiner feature to switch models part way through the generation process and define at what stage of generation that should happen. So in the example of the following prompt, we can switch models to something based on a relevant training dataset:\na lonely bipedal robot walking through a valley. best quality. photograph. nuclear. cataclysmic. end of the world Enabling refinement with a model based on a highly specific training dataset.\nThe Switch At setting determines the fraction of sampling steps when the switch to refiner model should happen; 1=never, 0.5=switch in the middle of generation.\nPrompt Techniques Before we continue, I would like to warn you that most certainly from a relative beginner\u0026rsquo;s perspective, image generation with SD is a bit of a fuzzy logic. There appears to be no certain method to govern the type of image that is produced; you can only tweak guidance where possible using the tools available to you and the model will ultimate decided on the output. These models are trained against a vast number of parameters and are essentially guessing what each pixel should look like based on trends observed in the train data set.\nTherefore I would suggest that you approach this with patience and try to record your methodology and steps taken to reach a visual output that you are happy with. Change only one variable at a time, but understand that the inherent randomness out the image generation could still provide an unsatisfactory result.\nNegative Prompts In the same way that (positive) prompts help define your expected outcomes, negative prompt help to shape the generated image by providing SD with a list ob objects and types to avoid when it is consulting its inference model.\nFor example, you want to generate a portrait, but Stable Diffusion provided duplicates faces or bodies. A negative prompt including the phrase \u0026ldquo;duplicates\u0026rdquo; would provide a high-dimension anchor, which the process seek to stray away from.\nPrompt Examples Prompt: a lonely bipedal robot walking through a valley. best quality. photograph. nuclear. cataclysmic. end of the world Negative: human. duplicates. hair Prompt: A hyper realistic avatar of a man riding on a black honda cbr 650r in leather suit,high detail, high quality,8K,photo realism Negative: flying mirrors,low quality Prompt: electronik robot and ofice ,unreal engine, cozy indoor lighting, artstation, detailed, digital painting,cinematic,character design by mark ryden and pixar and hayao miyazaki, unreal 5, daz, hyperrealistic, octane render Negative: ugly hands, disjointed More fantastic prompt examples can be found here: Prompt examples - Stable Diffusion .\nYou can skip ahead to see more results here in the results comparison section.\nProvided that no generation configuration values are changed and a fixed seed is defined (including Hires Fix settings), you will always get the same output from the vision model. Each time to you generate an image, the webui shows your the parameters used; there for you could replicate any image using these parameters. Try this:\nUsing the SD output parameters to recreate identical images.\nOne thing you must understand, if you increase the Batch Count, the seed value still increments (by 1, at default).\nBetter still, if you are using the --api, you could pass parameters in your API GET request. You can read more about the API here: stable-diffusion-webui/wiki/API. There you will find a link to this useful python script for getting started with requests over the API: txt2img and img2img example python script.\nStyles Style are an amendment to a prompt and can have a offer a very strong level of control over the type of image that SD is going to produce for you. Ordinarily, an artist would have their style of work classified as a particular type of work. Therefore a model trained on a wide array of data should have an understanding of what each of these styles might look like and be able to make better predictions for the next generated pixel.\nOne of the best resources I have encountered is this style cheat sheet: supagruen.github.io/StableDiffusion-CheatSheet. Simply pick a style you are interested in, copy the style prompt and append it to your existing prompt in stable-diffusion-webui. The sit splits out the styles with tags and categories to help you pick your perfect style. This has to be one of the most valuable AI image generation resources to date.\nStable Diffusion 1.5 - Cheat Sheet.\nInstalling Mikubill\u0026rsquo;s sd-webui-controlnet Installing extensions in stable-diffusion-webui is ordinarily a very user friendly, low skill process. There are a large number of community produced plugins that enable a huge amount of features in SD.\nSo let\u0026rsquo;s install ControlNet:\nOpen \u0026ldquo;Extensions\u0026rdquo; tab. Open \u0026ldquo;Install from URL\u0026rdquo; tab in the tab. Enter https://github.com/Mikubill/sd-webui-controlnet.git to \u0026ldquo;URL for extension\u0026rsquo;s git repository\u0026rdquo;. Press \u0026ldquo;Install\u0026rdquo; button. Wait until you see the message \u0026ldquo;Installed into stable-diffusion-webui\\extensions\\sd-webui-controlnet\u0026rdquo;. Go to \u0026ldquo;Installed\u0026rdquo; tab, click \u0026ldquo;Check for updates\u0026rdquo;, and then click \u0026ldquo;Apply and restart UI\u0026rdquo;. Completely restart A1111 webui - end the command-prompt/terminal window and launch again. Installing Mikubill\u0026rsquo;s sd-webui-controlnet\nShould you need to forcefully remove this or any other extension, you can simply delete the extension named folder from the following path:\n~ \\stable-diffusion-webui\\extensions Controlnet Techniques Canny I have found that sometimes enabling ControlNet can harm the creativity of text2img generation, irrespective of your classifier free guidance (CFG) scale value. Controlnet can be tweaked to respect the prompt with a higher priority, but it can lead to some questionable outputs due to it\u0026rsquo;s influence. My experimenting with Controlnet functionality is currently incomplete and I hope to cover my findings in a future article. It is best used with very loose control influence and with an image that closely resembles your desired outcome. This is why, it can be great for human faces, when paired with an appropriate model and style.\nUsing Canny to paint myself as a soldier in World War 2 and World War 3.\nOpenPose This ContorlNet module aims to by add extra conditions to the neural network structure in order to control diffusion model outcomes; OpenPose does this with human pose estimation. Needless to say, that works best on humanoid subjects. The module can analyse an image to produce a wireframe for human poses and this is applied to conditions of generation.\nHere is how we can configure openpose in the controlnet plug section of the SD-webui interface:\nControlNet OpenPose using default values.\nThis results in the following wireframe pose detection - not a perfect capture due to a slightly ambiguous input image of a non-human entity:\nControlNet OpenPose generating a pose structure based on an input image.\nUltimately resulting in some very nice output images, which adhere to our prompt, provide creativity and strike the same pose. I believe that the slight issue in the pose is what caused SD to confuse the direction that the subject was facing in some images.\nControlNet OpenPose generating a batch of 4 images based on our wireframe and prompt.\nImage Resolution and Performance You\u0026rsquo;re available hardware is going to make a large difference in generation time. As a general rule of thumb, graphics cards with a high memory availability offer better performance during inference based task such as Stable Diffusion. Howwever, the Nvidia GPUs, particularly RTX GPUs which saw the introduction of Tensor cores, clearly have the computational advantage:\nTom\u0026rsquo;s Hardware Stable Diffusion benchmark article\nWhilst it is not entirely necessary to have an RTX graphics card, using one will significantly reduce your generation times. Nvidia is quite literally selling shovels in a gold rush\u0026hellip;\nHere are some performance optimization options that can be applied using command-prompt/terminal flags:\nCommandline argument Explanation --opt-sdp-attention May results in faster speeds than using xFormers on some systems but requires more VRAM. (non-deterministic) --opt-sdp-no-mem-attention May results in faster speeds than using xFormers on some systems but requires more VRAM. (deterministic, slightly slower than --opt-sdp-attention and uses more VRAM) --xformers Use xFormers library. Great improvement to memory consumption and speed. Nvidia GPUs only. (deterministic as of 0.0.19 [webui uses 0.0.20 as of 1.4.0]) --force-enable-xformers Enables xFormers regardless of whether the program thinks you can run it or not. Do not report bugs you get running this. --opt-split-attention Cross attention layer optimization significantly reducing memory use for almost no cost (some report improved performance with it). Black magic. On by default for torch.cuda, which includes both NVidia and AMD cards. --disable-opt-split-attention Disables the optimization above. --opt-sub-quad-attention Sub-quadratic attention, a memory efficient Cross Attention layer optimization that can significantly reduce required memory, sometimes at a slight performance cost. Recommended if getting poor performance or failed generations with a hardware/software configuration that xFormers doesn\u0026rsquo;t work for. On macOS, this will also allow for generation of larger images. --opt-split-attention-v1 Uses an older version of the optimization above that is not as memory hungry (it will use less VRAM, but will be more limiting in the maximum size of pictures you can make). --medvram Makes the Stable Diffusion model consume less VRAM by splitting it into three parts - cond (for transforming text into numerical representation), first_stage (for converting a picture into latent space and back), and unet (for actual denoising of latent space) and making it so that only one is in VRAM at all times, sending others to CPU RAM. Lowers performance, but only by a bit - except if live previews are enabled. --lowvram An even more thorough optimization of the above, splitting unet into many modules, and only one module is kept in VRAM. Devastating for performance. *do-not-batch-cond-uncond Only before 1.6.0: prevents batching of positive and negative prompts during sampling, which essentially lets you run at 0.5 batch size, saving a lot of memory. Decreases performance. Not a command line option, but an optimization implicitly enabled by using --medvram or --lowvram. In 1.6.0, this optimization is not enabled by any command line flags, and is instead enabled by default. It can be disabled in settings, Batch cond/uncond option in Optimizations category. --always-batch-cond-uncond Only before 1.6.0: disables the optimization above. Only makes sense together with --medvram or --lowvram. In 1.6.0, this command line flag does nothing. --opt-channelslast Changes torch memory type for stable diffusion to channels last. Effects not closely studied. --upcast-sampling For Nvidia and AMD cards normally forced to run with --no-half, should improve generation speed. If unlimited power is what you seek and you are running SD on Windows, in Nvidia control panel, 3d parameters, change your power profile to \u0026ldquo;maximum performance\u0026rdquo;. You are now trading even more money for faster images.\nSize Matters Unfortunately, size does matter when it comes to performance\u0026hellip; the larger the resolution, the more memory resources you are going to require. By default SD targets 512px x 512px. My best advice is to target a small resolution whilst you build an understanding of the models and controls needed to reach your ambitions with SD. This will allow you to quickly make small, iterative changes and have a faster feedback loop, whilst you work out which settings are best for your use case.\nUnfortunately, your available memory resources may be the limit to the maximum output resolution; a work around is to use upscaling.\nHires Fix Uses a two step process to partially create an image at smaller resolution (such as 512px x 512px), upscale, and then improve details in it without changing composition. So let\u0026rsquo;s say that we want to optimise our image output for max quality Instagram stories, we can set a width of 540px, a height of 960px and an upscale factor of 2; giving a 1080px by 1920px image.\nOne setting that we must consider is Denoising strength - this determines how little respect the algorithm should have for image\u0026rsquo;s content. At 0, nothing will change, and at 1 you\u0026rsquo;ll get an unrelated image. With values below 1.0, processing will take less steps than the Sampling Steps slider specifies. I have found that best results are when left at the default value of 0.7.\nAUTOMATIC1111/stable-diffusion-webui with Hires set to scale images by a factor of 2.\nUpscaling It is the year 2024 and if we are being truthful, 512px by 512px images look like an early Fallout game and 1024px by 1024px is also unacceptable in this age of retina displays. Fortunately, we can use the upscaling feature in SD-webui to grace our eyeballs with higher resolution images, after generation. Let\u0026rsquo;s take a look at how to upscale in SD-webui:\nUpscaling with the Extras tab.\nThe above figure takes out 1920px by 1080px image and upscales it to 3840px by 2160px. The main caveat to upscaling is that unlike Hires Fix, we are not adding detail when we expand the image because no sampling is taking place.\nPower Consumption A quick note on power consumption. Living in a western nation, power costs are not an insignificant expense and as we have learned, running local software like stable-diffusion-webui and LMStudio induces a significant load on a computer for an extended period of time. Here is some back of a napkin maths for an RTX 3060 (12GB) running at stock clock settings (Ryzen 5600, 32GB DDR4, 1x m.2 SSD):\nConsumption in watts during generation: 480 Time take in hours to generate 10 images at 1024 x 1024: 1.2 Electricity rate per kWh: £0.24 £0.14 per 1.2 hours ~£0.01 per image This might not seem like much but I have spent somewhere in the region of 48 hours running generation just in the processing of learning and developing this article. That\u0026rsquo;s £5.64 at current rates\u0026hellip; so consider buying me a coffee?\nResults So if you haven\u0026rsquo;t already gathered, there are a lot of variables that we can modify to guide the image generation process and produce a result that we are aiming for. Let\u0026rsquo;s take a look at what sand can do when we teach it to think\u0026hellip; here are some of my favourite images so far:\nLonely robots walking away into a cataclysmic valley part 1.\nLonely robots walking away into a cataclysmic valley part 2.\nLonely robots walking away into a cataclysmic valley part 3.\nLonely robots walking away into a cataclysmic valley part 4.\nLonely robots walking away into a cataclysmic valley part 5.\nLonely robots walking away into a cataclysmic valley part 6.\nLonely robots walking away into a cataclysmic valley part 7.\nLonely robots walking away into a cataclysmic valley part 8.\nIf you found this article useful, consider sharing it in your social circles or bookmarking this page as I have lots more stable-diffusion content in the works\u0026hellip;\n","permalink":"https://exitcode0.net/posts/generating-artwork-with-stable-diffusion-april-2024/","summary":"As I write this, my computer\u0026rsquo;s Nvidia RTX GPU is busy converting watts into fictional images of robots and given the current cost of electricity, I am asking myself if the results it is producing are worth the resources required to produce them. In the article I hope to demonstrate how I am using AUTOMATIC1111/stable-diffusion-webui and Mikubill/sd-webui-controlnet to generate some borderline \u0026ldquo;art\u0026rdquo;. If you are looking to skip ahead to certain section, you can jump ahead using the table of contents above.","title":"Generating Artwork With Stable Diffusion - April 2024 Edition"},{"content":"In previous posts, I\u0026rsquo;ve explained how to use Tailscale\u0026rsquo;s MagicDNS and HTTPS certificate feature to generate a TLS cert for your Home Assistant install: Homeassistant Enable MagicDNS and HTTPS Certificates in Tailscale. That setup required you to reformat the TLS cert and copy it to the Home Assistant container\u0026hellip; it wouldn\u0026rsquo;t be to much of a task to automate those actions, but fortunately there is now a superior method of applying and renewing a Tailscale cert for HASS (and many other self-hosted services). Let us dig in to that\u0026hellip;\nTailscale is a virtual private network (VPN) service that allows secure remote access to resources across different networks. It offers a feature called MagicDNS that enables you to access your resources using a domain name instead of an IP address. Additionally, you can use HTTPS certificates to encrypt traffic between clients and servers, ensuring secure communication. In this tutorial, we\u0026rsquo;ll explain how to enable MagicDNS and HTTPS certificates in Tailscale and how to add a TLS certificate to Home Assistant using Tailscale. The most practical benifit for using Home Assistant within a Tailscale network is that it removes the requirement for network port forwarding and exposing services publicly, whilst still allowing your devices to access it from outside your local network. This blog post aims to give you the starting stes required to setup MagicDNS and HTTPS certificates in Tailscale, create a certificate on your home server and install that certificate in a Home Assistant docker container.\nWHY?! Why do I want a valid TLS certificate and a HTTPS connection to Home Assistant without a browser warning? Per the 2023.5 relase (https://www.home-assistant.io/blog/2023/05/03/release-20235/) voice control is all the rage using local voice assistants; in order to use the microphone in a modern browser connected to you Home Assistant dashboard, you need to have a valid HTTPS connection!\nEnabling MagicDNS Sign in to your Tailscale admin console and select your network. Click on \u0026ldquo;Nodes\u0026rdquo; and select the node that you want to enable MagicDNS for (You might only have one default node like me). Under \u0026ldquo;DNS\u0026rdquo; click \u0026ldquo;Enable MagicDNS.\u0026rdquo; Once you\u0026rsquo;ve enabled MagicDNS, you can access your devices using the tailnet domain allocated to your network. For example, mypc.exampletail.ts.net, where mypc is the device name.\nEnabling HTTPS Certificates Navigate to the DNS page of the admin console. Under HTTPS Certificates, click Enable HTTPS. Acknowledge that your machine names and your tailnet name will be published on a public ledger. Configure Tailnet Nameservers With MagicDNS enabled, we need to turn on splitDNS and set the nameserver to the Tailnet IP of the host which will be running our CoreDNS container - Yes, you need to have your docker host on your tailnet for this to work.\nHere is what you should end up with:\nThis is where things change from the previous post. This method is slightly more complex, as it required the adition of a Traefik and CoreDNS container, but once it is configured, it is exponentially more effective and scales well to other services.\nYou are going to need a domain that you own and that you can control the DNS configuration of. This setup is using the Certbot DNS-01 Challenge. This challenge asks you to prove that you control the DNS for your domain name by putting a specific value in a TXT record under that domain name. The easiest was to get your domain ready for DNS-01 is using cloudflare, but this is beyond the scope of the post. The best guide I could find for cloudflare, DNS-01 readiness is here: How to create Let’s Encrypt certificate using Cloudflare API token. This API token is essential and referenced below in our Traefik Docker Compose yaml file.\nCoreDNS Docker Compose version: \u0026#39;3\u0026#39; services: coredns: image: coredns/coredns:1.11.1 container_name: coredns restart: always dns: 127.0.0.1 ports: - 53:53/udp volumes: - ./config:/root/ command: -conf /root/Corefile Inside our config directory, we have a corefile which holds the DNS records we need.\nCoreDNS Config File . { forward . 8.8.8.8 9.9.9.9 log errors } \u0026lt;ignored-subdomain.domain.tld\u0026gt; { forward . 8.8.8.8 9.9.9.9 } auto \u0026lt;domain.tld\u0026gt; { root /root file zones/\u0026lt;domain.tld\u0026gt;.zone prometheus errors log } If like me, you have some A records which point to public services and you don\u0026rsquo;t want lookups for these subdomains to be done in CoreDNS, you need to instruct CoreDNS to forward them; in this case the lookup is forwarded to 8.8.8.8 (or 9.9.9.9).\nCoreDNS Zone File This is the file listed in our config file:\nfile zones/\u0026lt;domain.tld\u0026gt;.zone Our \u0026lt;domain.tld\u0026gt;.zone file should look like this:\n$TTL 300 ; generated with dnscontrol 2023-11-14T21:02:31Z @ 3600 IN SOA sns.dns.icann.org. noc.dns.icann.org. 2023111400 7200 3600 1209600 3600 IN NS ns1.example.com. IN NS ns2.example.com. IN NS ns3.example.com. IN NS ns4.example.com. home IN A \u0026lt;your_docker_host_tailnet_IP\u0026gt; www IN A 127.0.0.1 IN AAAA ::1 This config assumes that your Home Assistant container is running on the same Tailnet host as CoreDNS and Traefik.\nTraefik Docker Compose version: \u0026#34;3\u0026#34; services: traefik: container_name: traefik image: traefik:v2.10 labels: - traefik.enable=true - traefik.docker.network=traefik_proxy ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - \u0026#34;/var/run/docker.sock:/var/run/docker.sock\u0026#34; - \u0026#34;./certs:/letsencrypt\u0026#34; - \u0026#34;./traefik.yml:/etc/traefik/traefik.yml\u0026#34; - \u0026#34;./dynamic.yml:/etc/traefik/dynamic.yml\u0026#34; restart: unless-stopped environment: - \u0026#34;CF_API_EMAIL=\u0026lt;your_email_address\u0026gt;\u0026#34; - \u0026#34;CF_API_KEY=\u0026lt;your_cloudflare_API_key\u0026gt;\u0026#34; networks: - traefik_proxy networks: traefik_proxy: external: true Traefik Config YAML files We need these two yaml files which are reference in the compose file as they define some important settings within Traefik\u0026rsquo;s running configuration.\ntraefik.yml entryPoints: http: address: \u0026#34;:80\u0026#34; http: redirections: entryPoint: to: https scheme: https https: address: \u0026#34;:443\u0026#34; serversTransport: insecureSkipVerify: true providers: docker: endpoint: \u0026#34;unix:///var/run/docker.sock\u0026#34; exposedByDefault: false file: directory: /etc/traefik/ certificatesResolvers: le: acme: email: \u0026lt;your_email_address\u0026gt; storage: /letsencrypt/acme.json dnschallenge: provider: cloudflare resolvers: - \u0026#34;1.1.1.1:53\u0026#34; log: level: debug dynamic.yml tls: options: default: minVersion: VersionTLS12 cipherSuites: - TLS_AES_128_GCM_SHA256 - TLS_CHACHA20_POLY1305_SHA256 - TLS_AES_256_GCM_SHA384 - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 Home Assistant Docker Compose All that is left is to configure our Home Assistant container to use Traefik as a proxy. This is done by setting labels:\nversion: \u0026#39;3\u0026#39; services: homeassistant: container_name: homeassistant image: \u0026#34;ghcr.io/home-assistant/home-assistant:stable\u0026#34; volumes: - ./data:/config - /etc/localtime:/etc/localtime:ro restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=traefik_proxy\u0026#34; - \u0026#34;traefik.http.routers.home.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.home.rule=Host(`home.\u0026lt;youdomain.tld\u0026gt;`)\u0026#34; - \u0026#34;traefik.http.routers.home.tls.certresolver=le\u0026#34; - \u0026#34;traefik.http.services.home.loadbalancer.server.port=8123\u0026#34; networks: - traefik_proxy networks: traefik_proxy: external: true Now all that\u0026rsquo;s left is to bring all our containers up; in the following order: CoreDNS \u0026gt; Traefik \u0026gt; HomeAssistant.\ndocker compose up -d This post uses Docker Compose V2, if you are still using V1, consider upgrading otherwise use docker-compose instead of docker compose.\nPlease note that we MUST use the traefik_proxy docker network; this essentialls means that all trafic destined for the Home Assistant web application, routes through Traefik.\nAs an added bonus, this means that we can access our love lace dashboards without the :8123 port in the URL. The even more exciting thing is, we can now enable Mic usage in our browser, which means we are ready to make the most of all the great voice assistant feature that the HA developers have been working on: Home Assistant voice control.\nConclusion These Docker Compose file seamlessly integrates Home Assistant with Traefik, providing a flexible and scalable solution for managing home automation services with secure access through Traefik\u0026rsquo;s reverse proxy capabilities. The use of labels in Traefik configuration enhances the control and routing capabilities, making it an efficient and secure setup for Home Assistant deployment.\n","permalink":"https://exitcode0.net/posts/homeassistant-tls-with-tailscale-traefik/","summary":"In previous posts, I\u0026rsquo;ve explained how to use Tailscale\u0026rsquo;s MagicDNS and HTTPS certificate feature to generate a TLS cert for your Home Assistant install: Homeassistant Enable MagicDNS and HTTPS Certificates in Tailscale. That setup required you to reformat the TLS cert and copy it to the Home Assistant container\u0026hellip; it wouldn\u0026rsquo;t be to much of a task to automate those actions, but fortunately there is now a superior method of applying and renewing a Tailscale cert for HASS (and many other self-hosted services).","title":"Home Assistant HTTPS Certificates with Tailscale, Traefik and CoreDNS"},{"content":"Docker Compose is a powerful tool that allows developers to define and manage multi-container applications. In this blog post, we will explore a Docker Compose file for developing Python applications. Specifically, we will analyze the following Docker Compose file:\nversion: \u0026#39;3\u0026#39; services: mypthontool: image: python:3.9 volumes: - ./:/app/ tty: true Now all that is left to do is bring the container online with docker compose up -d and access the bash terminal in the container to run our code in a repeatable environment using docker exec -it \u0026lt;container name\u0026gt; /bin/bash.\nThis post uses Docker Compose V2, if you are still using V1, consider upgrading otherwise use docker-compose instead of docker compose.\nUnderstanding the Docker Compose File Let\u0026rsquo;s break down the Docker Compose file and understand each component:\nversion: '3': This line specifies the version of Docker Compose being used. In this case, we are using version 3 - more information on compose versions can be found here.\nservices: This section defines the services or containers in the application. In our case, we have a single service named mypthontool.\nmypthontool: This is the name of our service. You can modify it according to your project\u0026rsquo;s requirements.\nimage: python:3.9: This line specifies the Docker image that will be used to create the container for our Python application. In this case, we are using the official Python 3.9 image. This can be updated to whatever image meets your needs: Python Docker Builds.\nvolumes: - ./:/app/: Here, we define a volume mapping between our local directory (the current directory denoted by ./) and the /app/ directory within the container. This allows us to share code between our host machine and the container.\ntty: true: This line enables a pseudo-TTY for the container, allowing us to interact with it in an interactive manner.\nBenefits of Developing Python Applications in a Container Environment Developing Python applications in a container environment offers several advantages:\nPortability: With Docker, you can create a containerized environment that encapsulates all the dependencies and configurations required for your Python application. This ensures consistent behavior across different development machines and deployment environments.\nIsolation: Containers provide a sandboxed environment where your application and its dependencies run independently of the host system. This isolation helps avoid conflicts between different applications or libraries.\nReproducibility: By defining your application\u0026rsquo;s environment in a Docker Compose file, you can easily recreate the exact same environment on any machine. This eliminates the notorious \u0026ldquo;it works on my machine\u0026rdquo; problem and makes it easier to collaborate with other developers.\nScalability: Docker allows you to scale your application by spinning up multiple containers to handle increased workload. This makes it convenient to develop and test distributed systems or microservices architectures.\nComparison: Development in a Container vs. Python Virtual Environments (venv) While Python virtual environments (venv) also provide a way to isolate Python environments, there are some notable differences when compared to containerized development:\n1. Dependency Isolation: In a container environment, you can encapsulate not only your Python dependencies but also the entire operating system and runtime environment. This ensures that your application behaves consistently across different systems. With venv, you can only isolate Python packages.\n2. System-Level Dependencies: Containers allow you to include system-level dependencies like databases, message queues, or other services required by your application. These dependencies can be easily managed and replicated across different environments. In venv, you would need to install and configure these dependencies separately on each development machine.\n3. Platform Compatibility: Containers are platform-agnostic, meaning you can develop and\ntest your application on one platform (e.g., Windows) and deploy it on another (e.g., Linux) without worrying about compatibility issues. venv, on the other hand, is tied to the specific host system and may not work seamlessly across different platforms.\n4. Development Environment Consistency: Docker Compose provides a standardized environment for the entire development team. It ensures that everyone is working with the same set of dependencies and configurations. venv relies on developers to create and maintain their own virtual environments, leading to potential inconsistencies and compatibility problems.\nIn conclusion, developing Python applications in a container environment using Docker Compose offers enhanced portability, isolation, reproducibility, and scalability. While venv provides dependency isolation at the Python package level, it lacks the comprehensive environment encapsulation and platform compatibility offered by Docker containers. Consider using Docker Compose for your Python projects to streamline development, improve collaboration, and ensure consistent application behavior across different environments.\nStart containerizing your Python applications today and experience the benefits of a containerized development workflow with Docker Compose!\nNote: This blog post is for informational purposes only and does not cover all aspects of Docker Compose or Python virtual environments. Make sure to refer to official documentation and best practices for detailed instructions.\nStable Diffusion\u0026rsquo;s summary of this post - have no fear artists of the world!\n","permalink":"https://exitcode0.net/posts/python-development-in-docker-containers/","summary":"Docker Compose is a powerful tool that allows developers to define and manage multi-container applications. In this blog post, we will explore a Docker Compose file for developing Python applications. Specifically, we will analyze the following Docker Compose file:\nversion: \u0026#39;3\u0026#39; services: mypthontool: image: python:3.9 volumes: - ./:/app/ tty: true Now all that is left to do is bring the container online with docker compose up -d and access the bash terminal in the container to run our code in a repeatable environment using docker exec -it \u0026lt;container name\u0026gt; /bin/bash.","title":"Python Development in Docker Containers"},{"content":"Introduction to Running PostgreSQL in a Docker Container using Docker Compose In this blog post, we will explore how to run PostgreSQL in a Docker container using Docker Compose. We will also break down and explain the init-user-db.sh script that is executed at startup to initialize the PostgreSQL tables. Running PostgreSQL in a Docker container provides several benefits, including ease of deployment, portability, and isolation. So let\u0026rsquo;s dive in and understand the process!\nThis post uses Docker Compose V2, if you are still using V1, consider upgrading otherwise use docker-compose instead of docker compose.\nWhy Run PostgreSQL in a Docker Container? Running PostgreSQL in a Docker container offers numerous advantages. Here are a few key benefits:\nEasy Deployment: Docker simplifies the deployment process by encapsulating PostgreSQL and its dependencies into a container, making it easy to set up and manage.\nPortability: Docker containers are self-contained and can be run on any system that supports Docker, ensuring consistent behavior across different environments.\nIsolation: Running PostgreSQL in a container provides isolation from the host system, preventing potential conflicts with existing installations or dependencies.\nSetting Up PostgreSQL in a Docker Container with Docker Compose Before we start, ensure that you have Docker and Docker Compose installed on your system. Once both are set up, you can proceed with the following steps:\nCreate a Docker Compose file: Open a text editor and create a file called docker-compose.yml. Copy and paste the following contents into the file: version: \u0026#39;3.3\u0026#39; services: postgres: container_name: my-postgres image: postgres:latest restart: always environment: POSTGRES_USER: ${POSTGRES_USER} POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} POSTGRES_DB: ${POSTGRES_DB} PGDATA: /var/lib/postgresql/data/pgdata ports: - \u0026#34;5432:5432\u0026#34; volumes: - ./data:/var/lib/postgresql/data/pgdata - ./init-user-db.sh:/docker-entrypoint-initdb.d/init-user-db.sh In this Docker Compose configuration:\nThe postgres service is defined with the specified container name (my-postgres), the latest PostgreSQL image (postgres:latest), and the restart policy set to always to ensure that the container restarts automatically if it stops.\nThe environment section sets the PostgreSQL environment variables (POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB) using the provided values from the environment. In this example I have a .env file in the same directory as the docker-compose.yml file, which means that there is no need for sensitive data such as passwords to be stored in our infrastructure code. This .env file could be replaced by git secrets in part of a pipeline deployment.\nThe ports section maps the host machine\u0026rsquo;s port 5432 to the container\u0026rsquo;s port 5432, allowing you to access the PostgreSQL database from your host machine.\nThe volumes section mounts two directories:\n./data:/var/lib/postgresql/data/pgdata: This maps the ./data directory on your host machine to the /var/lib/postgresql/data/pgdata directory inside the container. It allows you to persist the PostgreSQL data files across container restarts. ./init-user-db.sh:/docker-entrypoint-initdb.d/init-user-db.sh: This mounts the init-user-db.sh script from your host machine to the /docker-entrypoint-initdb.d/init-user-db.sh path inside the container. This script will be executed during container startup to initialize the PostgreSQL tables. Start the PostgreSQL container: Open your terminal or command prompt, navigate to the directory containing the docker-compose.yml file, and run the following command: docker compose up -d The -d flag runs the containers in detached mode, allowing\nthem to run in the background.\nBreaking Down the init-user-db.sh Script The init-user-db.sh script is executed at startup to initialize the PostgreSQL tables. Let\u0026rsquo;s examine the script and understand its purpose:\n#!/bin/bash set -e set -u export PGDATABASE=${POSTGRES_DB} export PGUSER=${POSTGRES_USER} export PGPASSWORD=${POSTGRES_PASSWORD} RUN_PSQL=\u0026#34;psql -X --set AUTOCOMMIT=on --set ON_ERROR_STOP=on \u0026#34; ${RUN_PSQL} \u0026lt;\u0026lt;SQL CREATE TABLE public.mytable ( id uuid NOT NULL, sensor_name text NOT NULL, battery double precision, humidity double precision, link_quality double precision, temperature double precision, voltage integer, \u0026#34;time\u0026#34; timestamp with time zone NOT NULL ) TABLESPACE pg_default; ALTER TABLE IF EXISTS public.mytable OWNER to dbadm; SQL Here\u0026rsquo;s a breakdown of the script:\nThe set -e and set -u commands ensure that the script exits immediately if any command fails or encounters an unset variable.\nThe export statements set environment variables to configure the PostgreSQL connection. The variables PGDATABASE, PGUSER, and PGPASSWORD are assigned values based on the corresponding Docker environment variables (derived from the .env file).\nThe RUN_PSQL variable defines the psql command with specific options. -X disables transaction management, --set AUTOCOMMIT=on ensures that each command is executed in its own transaction, and --set ON_ERROR_STOP=on stops the script execution if any error occurs.\nThe ${RUN_PSQL} \u0026lt;\u0026lt;SQL syntax starts a here-document that allows us to provide SQL commands inline.\nThe SQL commands enclosed within the SQL delimiter create a table named mytable in the public schema with the specified columns and data types. The ALTER TABLE statement sets the owner of the table to dbadm if the table already exists.\nWith this updated Docker Compose configuration, you can easily manage and deploy your PostgreSQL container along with the initialization script. Running docker compose up -d will start the containers and execute the init-user-db.sh script to initialize the PostgreSQL tables.\nConclusion In this blog post, we have learned how to run PostgreSQL in a Docker container using Docker Compose. We have seen the advantages of running PostgreSQL in a container and how to set it up with the docker-compose.yml file. Additionally, we have explored the init-user-db.sh script and its role in initializing the PostgreSQL tables.\nBy leveraging Docker and Docker Compose, you can easily deploy and manage PostgreSQL in a portable and isolated environment. This approach brings flexibility and scalability to your PostgreSQL deployments, making it an ideal choice for various applications.\nHopefully this blog post has been helpful in understanding the process of running PostgreSQL in a Docker container using Docker Compose. Stay tuned for more exciting tutorials and guides. Consider buying me a coffee to feed the addiction and help me solve more automation headaches!\n","permalink":"https://exitcode0.net/posts/automating-postgres-deployment/","summary":"Introduction to Running PostgreSQL in a Docker Container using Docker Compose In this blog post, we will explore how to run PostgreSQL in a Docker container using Docker Compose. We will also break down and explain the init-user-db.sh script that is executed at startup to initialize the PostgreSQL tables. Running PostgreSQL in a Docker container provides several benefits, including ease of deployment, portability, and isolation. So let\u0026rsquo;s dive in and understand the process!","title":"Automating Postgres Deployment with Docker Compose and Init Scripts"},{"content":"In previous posts I have been using Docker Compose to deploy the constituent components of a fully local Home Assistant voice assistant. In this blog post, we will guide you through setting up Wyoming Piper using Docker Compose. Piper is a fast, local neural text to speech system originally optimised for the Raspberry Pi 4. It supports many languages, and voice samples: https://rhasspy.github.io/piper-samples.\nWyoming Piper is a speech recognition and natural language understanding system that can be used for voice control in various applications. It uses the Rhasspy framework and provides support for different languages and voices.\nPrerequisites Before you begin, make sure you have Docker and Docker Compose installed on your system. You can find installation instructions for your operating system on the Docker website and Docker Compose website.\nDocker Compose File Create a new file called docker-compose.yml and open it in a text editor. Copy the following content into the file:\nversion: \u0026#34;3\u0026#34; services: wyoming-piper: image: rhasspy/wyoming-piper ports: - \u0026#34;10200:10200\u0026#34; volumes: - \u0026#34;./piper-data:/data\u0026#34; command: [ \u0026#34;--voice\u0026#34;, \u0026#34;en-gb-southern_english_female-low\u0026#34; ] restart: unless-stopped Let\u0026rsquo;s go through the different sections of this Docker Compose file.\nVersion The version section specifies the version of the Docker Compose file format. In this case, we\u0026rsquo;re using version \u0026ldquo;3\u0026rdquo;.\nServices The services section defines the services that make up your application. In our case, we have a single service called wyoming-piper.\nWyoming Piper Service Under the wyoming-piper service, we have the following configurations:\nimage: Specifies the Docker image to use for the service. In this case, we\u0026rsquo;re using the rhasspy/wyoming-piper image. ports: Maps the container\u0026rsquo;s port 10200 to the host\u0026rsquo;s port 10200. This allows us to access Wyoming Piper\u0026rsquo;s web interface from our local machine. volumes: Mounts the ./piper-data directory on the host to the /data directory inside the container. This is used to persist Wyoming Piper\u0026rsquo;s data. command: Specifies the command-line arguments to pass to the container. In this example, we\u0026rsquo;re using the English (GB) Southern English Female (Low) voice sample. restart: Sets the restart policy for the container. In this case, the container will be automatically restarted unless explicitly stopped. Starting Wyoming Piper To start Wyoming Piper, open a terminal or command prompt, navigate to the directory where you saved the docker-compose.yml file, and run the following command:\ndocker-compose up -d The -d flag runs the containers in the background (detached mode).\nWait for Docker Compose to download the necessary Docker images and start the Wyoming Piper container. You can check the progress in the terminal output.\nOnce the container is up and running, you can access Wyoming Piper\u0026rsquo;s web interface by opening a web browser and navigating to http://localhost:10200.\nConclusion In this blog post, we\u0026rsquo;ve walked you through setting up Wyoming Piper using Docker Compose. Docker Compose allows you to manage the different components of Wyoming Piper in a unified and reproducible way. You can customize the configurations in the docker-compose.yml\nAt this point we have all the components needed for a fully local voice assistant stack, deployed with docker compose. It is now possible to follow the remainder of the Home Assistant docuemntation in configuring your assistant: https://www.home-assistant.io/docs/assist/voice_remote_local_assistant.\nRelevant and supporting posts:\nHow to Use a Docker Compose File for Wyoming Whisper Homeassistant Enable MagicDNS and HTTPS Certificates in Tailscale ","permalink":"https://exitcode0.net/posts/wyoming-piper-docker-compose/","summary":"In previous posts I have been using Docker Compose to deploy the constituent components of a fully local Home Assistant voice assistant. In this blog post, we will guide you through setting up Wyoming Piper using Docker Compose. Piper is a fast, local neural text to speech system originally optimised for the Raspberry Pi 4. It supports many languages, and voice samples: https://rhasspy.github.io/piper-samples.\nWyoming Piper is a speech recognition and natural language understanding system that can be used for voice control in various applications.","title":"Setting up Wyoming Piper with Docker Compose"},{"content":"In this blog post, we will go over how to use a Docker Compose file to deploy and configure Wyoming Whisper. Wyoming Whisper is an open-source, lightweight voice assistant designed to run on a Raspberry Pi or other low-powered device. The impetus for this compose defined container is to intergate with a Home Assistant 2023.5 container and ultimate have a fully local voice assistant. Whisper will provide our speech-to-text service and the Wyoming protocol is how it will be integrated with Home Assistant.\nNOTE: For your reference, I am using Home Assistant in a container which is why I have not simply setup the wyoming wishper addons.\nPrerequisites Before we get started, make sure you have Docker and Docker Compose installed on your system. If you need help with installation, please refer to the official Docker documentation.\nStep 1: Create a Docker Compose File Create a file called docker-compose.yml in a directory of your choosing and add the following content to it:\nversion: \u0026#39;3\u0026#39; services: wyoming-whisper: image: rhasspy/wyoming-whisper ports: - \u0026#34;10300:10300\u0026#34; volumes: - ./whisper-data:/data command: [ \u0026#34;--model\u0026#34;, \u0026#34;medium-int8\u0026#34;, \u0026#34;--language\u0026#34;, \u0026#34;en\u0026#34; ] restart: unless-stopped This file defines a single service named wyoming-whisper. Let\u0026rsquo;s go over what each line does:\nversion: '3': specifies the version of Docker Compose file format being used. image: rhasspy/wyoming-whisper: specifies the Docker image to use for the service. In this case, we are using the rhasspy/wyoming-whisper image. ports: - \u0026quot;10300:10300\u0026quot;: specifies the port mapping between the container and host. The 10300:10300 mapping means that the container\u0026rsquo;s port 10300 is mapped to the host\u0026rsquo;s port 10300. volumes: - ./whisper-data:/data: specifies the volume mapping between the container and host. The ./whisper-data:/data mapping means that the ./whisper-data directory on the host is mapped to the /data directory in the container. command: [ \u0026quot;--model\u0026quot;, \u0026quot;medium-int8\u0026quot;, \u0026quot;--language\u0026quot;, \u0026quot;en\u0026quot; ]: specifies the command to run when the container starts. In this case, we are running Wyoming Whisper with the medium-int8 model and English language. restart: unless-stopped: specifies the restart policy for the container. In this case, the container will automatically restart unless it is explicitly stopped by the user. Whispher models You might be trying to run this container on a lightweight device such as a Raspberry Pi, in which case it would be wise to sellect a lightweight model for improved performance. Here are the model options, sorted from least to most accurate (fastest to slowest):\ntiny-int8 (43 MB) tiny (152 MB) base-int8 (80 MB) base (291 MB) small-int8 (255 MB) small (968 MB) medium-int8 (786 MB) medium (3.1 GB) The model file is downloaded on first run of the container.\nStep 2: Start the Container To start the container, navigate to the directory where your docker-compose.yml file is located and run the following command:\ndocker-compose up -d The -d flag runs the container in the background.\nStep 3: Integrate Wyoming Whisper with Home Assistant Now that the server is up and running we can add a new integration in our Home Assistant settings. This can be done under: Settings \u0026gt; Devices \u0026amp; Services \u0026gt; Add Integration.\nConclusion That\u0026rsquo;s it! You now know how to use a Docker Compose file to deploy and configure Wyoming Whisper. Docker Compose is a powerful tool that simplifies the deployment and management of complex applications. If you are looking to prepare your HA install for a fully local voice assistant and you need to setup HTTPS, consider reading the previous post - linked below - on how this can be done with Tailscale MagicDNS and HTTPS certificates.\n","permalink":"https://exitcode0.net/posts/wyoming-whisper-docker-compose/","summary":"In this blog post, we will go over how to use a Docker Compose file to deploy and configure Wyoming Whisper. Wyoming Whisper is an open-source, lightweight voice assistant designed to run on a Raspberry Pi or other low-powered device. The impetus for this compose defined container is to intergate with a Home Assistant 2023.5 container and ultimate have a fully local voice assistant. Whisper will provide our speech-to-text service and the Wyoming protocol is how it will be integrated with Home Assistant.","title":"How to Use a Docker Compose File for Wyoming Whisper"},{"content":"Tailscale is a virtual private network (VPN) service that allows secure remote access to resources across different networks. It offers a feature called MagicDNS that enables you to access your resources using a domain name instead of an IP address. Additionally, you can use HTTPS certificates to encrypt traffic between clients and servers, ensuring secure communication. In this tutorial, we\u0026rsquo;ll explain how to enable MagicDNS and HTTPS certificates in Tailscale and how to add a TLS certificate to Home Assistant using Tailscale. The most practical benifit for using Home Assistant within a Tailscale network is that it removes the requirement for network port forwarding and exposing services publicly, whilst still allowing your devices to access it from outside your local network. This blog post aims to give you the starting stes required to setup MagicDNS and HTTPS certificates in Tailscale, create a certificate on your home server and install that certificate in a Home Assistant docker container.\nWHY?! Why do I want a valid TLS certificate and a HTTPS connection to Home Assistant without a browser warning? Per the 2023.5 relase (https://www.home-assistant.io/blog/2023/05/03/release-20235/) voice control is all the rage using local voice assistants; in order to use the microphone in a modern browser connected to you Home Assistant dashboard, you need to have a valid HTTPS connection!\nEnabling MagicDNS Sign in to your Tailscale admin console and select your network. Click on \u0026ldquo;Nodes\u0026rdquo; and select the node that you want to enable MagicDNS for (You might only have one default node like me). Under \u0026ldquo;DNS\u0026rdquo; click \u0026ldquo;Enable MagicDNS.\u0026rdquo; Once you\u0026rsquo;ve enabled MagicDNS, you can access your devices using the tailnet domain allocated to your network. For example, mypc.exampletail.ts.net, where mypc is the device name.\nEnabling HTTPS Certificates Navigate to the DNS page of the admin console. Under HTTPS Certificates, click Enable HTTPS. Acknowledge that your machine names and your tailnet name will be published on a public ledger. Once you\u0026rsquo;ve enabled HTTPS certificates, you can generate the certificate and private key by running the following command on each machine (linux):\nsudo tailscale cert xxxx.xxxx.ts.net Replace xxxx.xxxx.ts.net with your tailnet domain, found on the DNS page of the admin console.\nThis will download two files: xxxx.xxxx.ts.net.crt and xxxx.xxxx.ts.net.key.\nAdding the TLS Certificate to Home Assistant To add the TLS certificate to Home Assistant, you need to convert the private key to PEM format and copy the certificate files to the Home Assistant container\u0026rsquo;s SSL directory. The following steps are based on a docker compose installation of HA, assuming the .crt and .key files created above are stored in /home/user/ (your file path will likely differ).\nConvert the private key to PEM format using the following command: sudo openssl pkcs8 -topk8 -nocrypt -in /home/user/xxxx.xxxx.ts.net.key -out /home/user/private.pem Replace /home/user/xxxx.xxxx.ts.net.key with the path to the downloaded key file.\nMake a directory for the TLS certificate files in the Home Assistant container\u0026rsquo;s file structure: mkdir /home/user/docker/homeassistant/ssl Add a volume mapping for the newly created ssl folder to your docker-compose.yml file, this will be where Home Assistant stores and locates the TLS certificate files: volumes: - /home/user/docker/homeassistant/ssl:/ssl You might alreaady have other volumes like I do for config and data; we are just adding another in this step.\nCopy the certificate files to the TLS directory: sudo cp /home/user/xxxx.xxxx.ts.net.crt /home/user/docker/homeassistant/ssl/fullchain.pem sudo cp /home/user/private.pem /home/user/docker/homeassistant/ssl/privkey.pem Replace /home/user/xxxx.xxxx.ts.net.crt with the path to the downloaded certificate file.\nUpdate the Home Assistant configuration.yml file to include the SSL certificate and key paths: http: ssl_certificate: /ssl/fullchain.pem ssl_key: /ssl/private.pem Save the file and restart the Home Assistant container:\ndocker compose up -d Now, you should be able to access Home Assistant securely using the MagicDNS domain name: https://mypc.example.ts.net:8123/.\nNOTE: You will need to be accessing this URL from a Tailscale connected device which is configured to use Tailscale DNS!\n","permalink":"https://exitcode0.net/posts/homeassistant-tls-with-tailscale/","summary":"Tailscale is a virtual private network (VPN) service that allows secure remote access to resources across different networks. It offers a feature called MagicDNS that enables you to access your resources using a domain name instead of an IP address. Additionally, you can use HTTPS certificates to encrypt traffic between clients and servers, ensuring secure communication. In this tutorial, we\u0026rsquo;ll explain how to enable MagicDNS and HTTPS certificates in Tailscale and how to add a TLS certificate to Home Assistant using Tailscale.","title":"Homeassistant Enable MagicDNS and HTTPS Certificates in Tailscale"},{"content":"ESXi is a powerful hypervisor that allows for the virtualization of multiple operating systems on a single physical machine. However, there are times when you may need to pass through a physical device, such as an NVMe drive, to a virtual machine. In this blog post, we will explore the steps required to pass through an NVMe drive to an ESXi virtual machine.\nPrerequisites Before proceeding, ensure that you have the following:\nroot access to an ESXi server An NVMe drive that you want to pass through to a virtual machine Knowledge of the datastore and existing virtual machine name that you want to use Step 1: Enable SSH on your ESXi Server The first step is to enable SSH on your ESXi server. This can be done by logging into the ESXi web client and navigating to \u0026ldquo;Manage\u0026rdquo; \u0026gt; \u0026ldquo;Settings\u0026rdquo; \u0026gt; \u0026ldquo;Security \u0026amp; Users\u0026rdquo; \u0026gt; \u0026ldquo;Services.\u0026rdquo; From here, you can enable SSH and start the SSH service.\nStep 2: SSH into Your ESXi Server Once SSH is enabled, you can SSH into your ESXi server by opening a terminal and running the following command:\nssh root@192.168.1.10 Replace 192.168.1.10 with the IP address of your ESXi server.\nStep 3: Create a Storage Directory Next, create a directory where you can store the virtual disk file. This can be done with the following command:\nmkdir /vmfs/volumes/datastore_name/existing_vm_name/storage Replace datastore_name with the name of the datastore that you want to use and existing_vm_name with the name of the existing virtual machine that you want to use.\nStep 4: List Available Disks List the available disks on your ESXi server with the following command:\nls -l /vmfs/devices/disks Make a note of the name of the NVMe disk that you want to pass through to the virtual machine. It should begin with t10.NVMe.\nStep 5: Create the Virtual Disk File Create the virtual disk file with the following command:\nvmkfstools -z /vmfs/devices/disks/t10.NVMe_____YOUR_DISK_NAME_____ \u0026#34;/vmfs/volumes/datastore_name/existing_vm_name/storage/disk_RDM_1.vmdk\u0026#34; Replace YOUR_DISK_NAME with the name of the NVMe disk that you want to pass through (found in step 4), datastore_name with the name of the datastore that you want to use, and existing_vm_name with the name of the existing virtual machine that you want to use.\nStep 6: Edit the Existing VM Settings Back in the ESXi web interface, edit the settings of the existing virtual machine that you want to pass the NVMe drive through to. Navigate to Edit Settings, click Add Other Device and select NVMe Controller.\nStep 7: Add the Virtual Hard Disk Add the virtual hard disk to the virtual machine by selecting \u0026ldquo;Existing Hard Disk\u0026rdquo; and browsing to the virtual disk file that you created in Step 5. Following along with our example, the file would be found at datastore_name/existing_vm_name/storage/disk_RDM_1.vmdk in the Datastore browser.\nStep 8: Configure the Virtual Disk Expand the hard disk options of the newly added disk and configure it with the following settings:\nSet Controller Location to: NVMe Controller 0, NVMe (0:0). Set Disk Compatibility to: Virtual Set Disk Mode to: Independent - Persistent Set Disk Compatibility Mode back to: Physical Cleanup and Conclusion Now you can save and close the VM settings, before powering on an letting the VM boot. In my case, I passed through my old laptop NVMe disk so that I could access and backup some old file. Lastly, don\u0026rsquo;t forget to resecure your ESXi server, by disabling SSH.\n","permalink":"https://exitcode0.net/posts/esxi-nvme-disk-passthrough/","summary":"ESXi is a powerful hypervisor that allows for the virtualization of multiple operating systems on a single physical machine. However, there are times when you may need to pass through a physical device, such as an NVMe drive, to a virtual machine. In this blog post, we will explore the steps required to pass through an NVMe drive to an ESXi virtual machine.\nPrerequisites Before proceeding, ensure that you have the following:","title":"Passing Through an NVMe Drive to an ESXi Virtual Machine"},{"content":"Introduction Uptime-Kuma is an open-source, self-hosted website monitoring tool. It can check the uptime and response time of websites, APIs, and other services at regular intervals and alert you if any of them go down. Uptime-Kuma can be installed on your own server and customized to suit your needs. In this article, we will explore how to set up and run Uptime-Kuma using Docker Compose.\nThe Docker Compose File The following is the Docker Compose file for the Uptime-Kuma service:\nversion: \u0026#39;3.3\u0026#39; services: uptime-kuma: image: louislam/uptime-kuma:latest container_name: uptime-kuma volumes: - ./data:/app/data ports: - 3001:3001 restart: always This Docker Compose file defines a single service named \u0026ldquo;uptime-kuma\u0026rdquo;. The service is built from the \u0026ldquo;louislam/uptime-kuma\u0026rdquo; Docker image, which is the latest version available. The container name is set to \u0026ldquo;uptime-kuma\u0026rdquo; as well.\nThe \u0026ldquo;volumes\u0026rdquo; section maps the \u0026ldquo;/app/data\u0026rdquo; directory inside the container to the \u0026ldquo;./data\u0026rdquo; directory on the host machine. This allows us to persist data across container restarts. Uptime-Kuma stores all its configuration and monitoring data in this directory, so it\u0026rsquo;s important to keep it intact.\nThe \u0026ldquo;ports\u0026rdquo; section maps port 3001 on the host machine to port 3001 in the container. This allows us to access the Uptime-Kuma web interface from our local machine. By default, Uptime-Kuma runs on port 3001, but you can change it if necessary.\nFinally, the \u0026ldquo;restart\u0026rdquo; section ensures that the container will always be restarted if it exits. This is useful for ensuring that the Uptime-Kuma service is always available, especially if it crashes or is shut down unexpectedly.\nRunning Uptime-Kuma with Docker Compose To run Uptime-Kuma with Docker Compose, first, make sure you have Docker and Docker Compose installed on your machine. Then, create a new directory for your Uptime-Kuma project and save the above Docker Compose file as \u0026ldquo;docker-compose.yml\u0026rdquo; in that directory.\nNext, run the following command from the same directory:\ndocker-compose up -d This command will start the Uptime-Kuma container in the background and detach from it. You can then access the Uptime-Kuma web interface by going to http://localhost:3001 in your web browser.\nConclusion In this article, we explored how to set up and run the Uptime-Kuma website monitoring tool using Docker Compose. We looked at the different sections of the Docker Compose file and explained how they work together to create a functional Uptime-Kuma service. By running Uptime-Kuma with Docker Compose, you can easily deploy and manage the tool on your own server, without having to worry about dependencies or configuration.\n","permalink":"https://exitcode0.net/posts/running-uptime-kuma-with-docker-compose/","summary":"Introduction Uptime-Kuma is an open-source, self-hosted website monitoring tool. It can check the uptime and response time of websites, APIs, and other services at regular intervals and alert you if any of them go down. Uptime-Kuma can be installed on your own server and customized to suit your needs. In this article, we will explore how to set up and run Uptime-Kuma using Docker Compose.\nThe Docker Compose File The following is the Docker Compose file for the Uptime-Kuma service:","title":"Running Uptime Kuma With Docker Compose"},{"content":"A new beinging for this blog\u0026hellip; The ExitCode0 blog is currently migrating. Welcome to the site, please hang tight for more content, coming soon. This site should now be in hosted on GitHub pages!\n","permalink":"https://exitcode0.net/posts/welcome/","summary":"A new beinging for this blog\u0026hellip; The ExitCode0 blog is currently migrating. Welcome to the site, please hang tight for more content, coming soon. This site should now be in hosted on GitHub pages!","title":"Welcome [Back]"},{"content":"Windows 11 became available on the 5th October 2021 and for those people with compatible machines, this guide will help you jump the queue and force the Windows 11 upgrade.\nBefore you start…\nTake a backup all all important data onto an external storage device/location. set aside at least 1 hour for the upgrade – maybe longer, depending on your internet connection speed. Make sure that your Windows 10 installation is up to date with no pending updates. Compatibility You might have seen a number of articles and news stories floating around about Windows 11 dropping support for a large number of computers, particularly older systems. The quickest way to test your system’s compatibility with Windows 11 is using the Microsoft WindowsPCHealthCheckSetup tool: https://aka.ms/GetPCHealthCheckApp\nWindows Health Check Tool Once you have the application installed and running, press the ‘Check now‘ button to test Windows 11 compatibility.\nDownloading the Windows 11 installer To get started with the upgrade, first download the installer:\nhttps://www.microsoft.com/en-us/software-download/windows11 Once the installer is running, it will start to download Windows 11 and begin the in-place upgrade. This will not perform a clean install of Windows and retain most of your settings and keep all of your files and applications.\nOnce the installer reaches 100% on stage 3/3, your computer will ask to restart or automatically restart after a 15 minutes countdown. This reboot might take longer than usual as the upgrade is applied. On completion, expect to see a Windows 11 welcome screen.\nAnd finally, your system will now be running Windows 11!\n","permalink":"https://exitcode0.net/posts/how-to-force-the-windows-11-upgrade/","summary":"Windows 11 became available on the 5th October 2021 and for those people with compatible machines, this guide will help you jump the queue and force the Windows 11 upgrade.\nBefore you start…\nTake a backup all all important data onto an external storage device/location. set aside at least 1 hour for the upgrade – maybe longer, depending on your internet connection speed. Make sure that your Windows 10 installation is up to date with no pending updates.","title":"How to force the Windows 11 upgrade"},{"content":" The HS100/HS110 LED can be bright and obnoxious. If you are looking to turn off the LED status lights on the TPLink Kasa smart plugs, then look no further – the solution resides in this post. It does involve some Python interaction, but I promise that it is a gentle passing with the programming language.\nThis solution works for:\nHS100 HS103 HS105 HS110 The only real drawback is that when the LED is disabled, you can not see when the device might have become disconnected from the wireless network – this is normally indicated by a RED or orange LED status.\nThe problem: How do you turn off the LED Indicator in a TPLink HS105 Smart plug? from HomeNetworking\nThe reddit solution: put some tape over it\nThe Reddit users go on to explain that the tape solution was working rather well. I am sure that using tape is quite effective, but if you don’t want the light emitted by the LED, then you might as well not have the energy consumption either.\nThe real solution: Turn off Kasa smartplug LED lights (HS100/HS110) https://github.com/Tombo1001/Kasa-Dark-Mode\n0 forks.\n1 stars.\n0 open issues.\nRecent commits: * bug fixdevice discovery notice bug fix, Tom Cocking\nUpdate README.mdimproved readme, Tom Cocking Initial commitv1.0 code, requirements and a basic readme, Tom Cocking Initial commit, Tom Cocking The project’s README file has a full set of instructions on setup and how to use the python script. The code has two usage modes:\nOne for all – apply LED status on or off for all discovered smart plugs: \u0026gt; python kasa-dark-mode.py -d Plug Alias: Fan Current LED state: True New LED state: False --- Plug Alias: Network Current LED state: True New LED state: False Interactive mode – walk through each discovered smart plug and make choice on the LED status: \u0026gt; python kasa-dark-mode.py -i You have selected interactive mode Searching for smartplugs... --plug found-- Plug Alias: Fan Current LED state: False Do you wish to turn ON \u0026#39;Fan\u0026#39; LED [Y/n]: n --- --plug found-- Plug Alias: Network Current LED state: True Do you wish to turn OFF \u0026#39;Network\u0026#39; LED [Y/n]: New LED state: False --- No tape. No LED.\n","permalink":"https://exitcode0.net/posts/turn-off-tplink-smart-plug-led/","summary":"The HS100/HS110 LED can be bright and obnoxious. If you are looking to turn off the LED status lights on the TPLink Kasa smart plugs, then look no further – the solution resides in this post. It does involve some Python interaction, but I promise that it is a gentle passing with the programming language.\nThis solution works for:\nHS100 HS103 HS105 HS110 The only real drawback is that when the LED is disabled, you can not see when the device might have become disconnected from the wireless network – this is normally indicated by a RED or orange LED status.","title":"Turn off TPLink Smart plug LED"},{"content":"Depending on your reason for using the PowerShell prompt at it, it might be useful to include data such as timestamps in your recorded output, to enable better auditing. This post will walk you through the simple steps needed for customising the PowerShell prompt, including how to show the current date and time.\nWhat exactly do I mean by the PowerShell prompt and what does the typical prompt look like by default? Well let me dial it back to default and show you:\nWindows 10 21H1 default PowerShell prompt – Windows Terminal The default Windows 10 (21H1) PowerShell prompt simply includes PS (PowerShell), followed by the current working directory; in my case: C:\\Users\\admin. In circumstances where we are looking to record our terminal sessions, it could be extremely useful to have the inclusion of time stamps. This is not only useful for auditing and log correlation but for accountability purposes.\nCreating a PowerShell profile Assuming that this is your first time modifying your PowerShell prompt for the current Windows user, first, we must create the file which stores our settings. Once available, this file is referenced each time PowerShell is launched.\nPowerShell supports several profile files. Also, PowerShell host programs can support their own host-specific profiles. For example, the PowerShell console supports the following basic profile files. The profiles are listed in precedence order. The first profile has the highest precedence.\nDescription Path All Users, All Hosts $PSHOME\\Profile.ps1 All Users, Current Host $PSHOME\\Microsoft.PowerShell_profile.ps1 Current User, All Hosts $Home[My ]Documents\\PowerShell\\Profile.ps1 Current user, Current Host $Home[My ]Documents\\PowerShell\\Microsoft.PowerShell_profile.ps1 Let’s create the file, from the PowerShell terminal:\nNew-Item -ItemType File -Path $PROFILE -Force now let’s edit that file (launching the editor from the terminal):\nise $PROFILE In my case, I have my Documents path mapped to a folder which I sync with Nextcloud. Now that we have a profile file created, we can start to store modifications, however, this script is not signed and will likely fail to run unless you modify your execution policy:\nSet-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy Unrestricted PowerShell ‘function prompt’ A good amount of code and inspiration for my selection of prompt modifications were inspired by this great post: https://www.norlunn.net/2019/10/07/powershell-customize-the-prompt, so I would strongly suggest checking that out.\nHistory ID $HistoryId: $MyInvocation.HistoryId Write-Host -Object \u0026#34;$HistoryId`: \u0026#34; -NoNewline -ForegroundColor Cyan Date and Time Write-Host -Object \u0026#34;$(Get-Date) \u0026#34; -NoNewline -ForegroundColor Green Username Write-Host -Object \u0026#34;-$($env:USERNAME)- \u0026#34; -NoNewline -ForegroundColor DarkRed Current path (working directory) $Drive: $pwd.Drive.Name $Pwds: $pwd -split \u0026#34;\\\\\u0026#34; | Where-Object { -Not [String]::IsNullOrEmpty($_) } $PwdPath: if ($Pwds.Count -gt 3) { $ParentFolder: Split-Path -Path (Split-Path -Path $pwd -Parent) -Leaf $CurrentFolder: Split-Path -Path $pwd -Leaf \u0026#34;..\\$ParentFolder\\$CurrentFolder\u0026#34; } elseif ($Pwds.Count -eq 3) { $ParentFolder: Split-Path -Path (Split-Path -Path $pwd -Parent) -Leaf $CurrentFolder: Split-Path -Path $pwd -Leaf \u0026#34;$ParentFolder\\$CurrentFolder\u0026#34; } elseif ($Pwds.Count -eq 2) { Split-Path -Path $pwd -Leaf } else { \u0026#34;\u0026#34; } Write-Host -Object \u0026#34;$Drive`:\\$PwdPath\u0026#34; -NoNewline -ForegroundColor Magenta return \u0026#34;\u0026gt; \u0026#34; My full prompt $PROFILE function prompt { $Success: $? ## History ID $HistoryId: $MyInvocation.HistoryId Write-Host -Object \u0026#34;$HistoryId`: \u0026#34; -NoNewline -ForegroundColor Cyan ## Date Time Write-Host -Object \u0026#34;$(Get-Date) \u0026#34; -NoNewline -ForegroundColor Green ## User Write-Host -Object \u0026#34;-$($env:USERNAME)- \u0026#34; -NoNewline -ForegroundColor DarkRed ## Path $Drive: $pwd.Drive.Name $Pwds: $pwd -split \u0026#34;\\\\\u0026#34; | Where-Object { -Not [String]::IsNullOrEmpty($_) } $PwdPath: if ($Pwds.Count -gt 3) { $ParentFolder: Split-Path -Path (Split-Path -Path $pwd -Parent) -Leaf $CurrentFolder: Split-Path -Path $pwd -Leaf \u0026#34;..\\$ParentFolder\\$CurrentFolder\u0026#34; } elseif ($Pwds.Count -eq 3) { $ParentFolder: Split-Path -Path (Split-Path -Path $pwd -Parent) -Leaf $CurrentFolder: Split-Path -Path $pwd -Leaf \u0026#34;$ParentFolder\\$CurrentFolder\u0026#34; } elseif ($Pwds.Count -eq 2) { Split-Path -Path $pwd -Leaf } else { \u0026#34;\u0026#34; } Write-Host -Object \u0026#34;$Drive`:\\$PwdPath\u0026#34; -NoNewline -ForegroundColor Magenta return \u0026#34;\u0026gt; \u0026#34; } Save your file and load a new PowerShell terminal session to admire your work:\nColourful and functional!\n","permalink":"https://exitcode0.net/posts/customising-the-powershell-prompt-to-show-date-and-time/","summary":"Depending on your reason for using the PowerShell prompt at it, it might be useful to include data such as timestamps in your recorded output, to enable better auditing. This post will walk you through the simple steps needed for customising the PowerShell prompt, including how to show the current date and time.\nWhat exactly do I mean by the PowerShell prompt and what does the typical prompt look like by default?","title":"Customising the PowerShell prompt to show date and time"},{"content":"I own a basic VW Caddy MK3 (2009) which was equipped with a RCD 300 head unit. It did not come with the aux function/connectivity and this post explains how I added Bluetooth connectivity. This was a very simple installation with no requirement for VCDS coding; the Bluetooth adapter replicates a CD changer.\nAt the time of writing this article, I was able to purchase this adapter on Amazon UK for ~£35 – https://amazon.co.uk/gp/product/B0773P8HM5\nInstallation The installation for this Bluetooth adaptation is a relatively simple one and require very few tools. The manufacturer of the adapter also includes a QR code linking to a easy to follow YouTube video:\nInstallation steps: Remove the surround trim with a trim tool or plastic implement – a screwdriver may leave marks on the plastic trim. Remove 4 T20 screws around the RCD 300 head unit, then pull the RCD 300 out of the dash. Clip the Bluetooth adapter’s white block connector into the bottom left of the block connector (when looking at the back of the head unit). Unscrew the small torx screw on the back of the RCD 300 then screw back in with the small grounding connector between the screw head and the back of the head unit. Plug in the optional microphone and route this to an appropriate position in the vehicle. Reverse the removal steps to secure the head unit and trim in place. The 12 pin VAG connector Using the adapter with the RCD300\nOnce installed, you need to switch the RCD300 to CD mode – press the CD button. Whilst you can still use the unit as a CD player, please remove any CDs before proceeding with these instructions.\nOnce in CD mode, search for available Bluetooth devices on your phone and connect to the available device. Once paired, the RCD 300 should replicate the connection of a CD multi-changer. Tracks on your phone show up as CDs, therefore you can skip tracks with the left and right arrow buttons. These two buttons also control accepting or rejecting incoming calls on the phone. If your vehicle has corresponding steering wheel controls, these will carry out the same functions.\nFrom there onwards, your phone will auto-connect to the Bluetooth device which becomes available after the vehicle ignition is enabled (and the head unit is powered on). I have found that because of the slight delay – 2 seconds – between the device powering on and connecting to the phone, the RCD 300 defaults back to FM/AM, so it is a requirement to press the CD button every time.\nPlaying Spotify over bluetooth from my grubby phone!\n","permalink":"https://exitcode0.net/posts/adding-bluetooth-to-a-vw-rcd-300/","summary":"I own a basic VW Caddy MK3 (2009) which was equipped with a RCD 300 head unit. It did not come with the aux function/connectivity and this post explains how I added Bluetooth connectivity. This was a very simple installation with no requirement for VCDS coding; the Bluetooth adapter replicates a CD changer.\nAt the time of writing this article, I was able to purchase this adapter on Amazon UK for ~£35 – https://amazon.","title":"Adding Bluetooth to a VW RCD 300"},{"content":" This is part 2 from my previous post on RTX 3060 Nicehash overclocking settings. I don’t want to edit the previous article because the content still stands to be accurate for the hash rate I achieved. However, I have since learned even more about the card and managed to improve my Nicehash quick miner hash rate by a further 10%!\nIf you want to see the first/part1 post, you can do so here: RTX 3060 Nicehash mining overclock settings\nAnd if you are looking for my post (and the download) on unlocking the hash rate with the 470.05 driver, that’s here: Unlock RTX 3060 mining hash rate.\nMy previous settings to acheive 44 MH/s used the following sentiment:\nLower the core clock rate Lower the power limit(%) Increasethe memory clock rate But I have since made some overclocking improvements and some additional changes to improve the longevity of the card. So much so that my overall uplift from stock (using the 470.05 driver) is sitting at around 25%! I am now able to reach 48MH/s with the DaggerHashimoto algorithm.\nBehold, 48 MH/s on an RTX 3060 (the card which NVidia does not want you to mine on)\nRTX 3060 overclocking with MSI Afterburner Your millage may vary with your individual card, but my improved settings to reach higher quickminer hash rates are as follows:\nPower limit: 75% Core Clock: -500 MHz – as low as it will go Memory Clock + 1300 MHz Fan Speed: Auto – with a more aggressive fan curve, see below… The MSI Afterburner settings to achieve 48MH/s on a RTX 3060 Keeping the GPU safe A more aggressive fan curve to cool VRAM I had been warned by a kind comment on my last post that whilst my GPU’s core clock temp might be low, the overclocked VRAM temp might be extremely high and limiting the performance of the card. What’s more it would not be conducive to a long living graphics card.\nI use my profiles in MSI Afterburner to switch back to stock settings when I want to play some video games – I am not a dedicated miner. So killing my card prematurely is not something I want to proceed in doing. I have chose to keep my fan on auto, to spare my ears, but applied a more aggressive fan curve to help keep the card temps down.\nMy particular Gigabyte card does not have a dedicated VRRAM temperature sensor – or at least it is not recognized by GPU-Z or HWinfo. Therefore, my only hope is to keep the average temperate of the card under control.\n(If you are going to follow this step, remember to tick the box to ‘Enable user defined software automatic fan control’)\n","permalink":"https://exitcode0.net/posts/more-rtx-3060-nicehash-overclocking/","summary":"This is part 2 from my previous post on RTX 3060 Nicehash overclocking settings. I don’t want to edit the previous article because the content still stands to be accurate for the hash rate I achieved. However, I have since learned even more about the card and managed to improve my Nicehash quick miner hash rate by a further 10%!\nIf you want to see the first/part1 post, you can do so here: RTX 3060 Nicehash mining overclock settings","title":"More RTX 3060 Nicehash overclocking"},{"content":"I recently managed to buy a Gigabyte RTX 3060 without lining the pockets of a scaler, lucky me! I recently discussed how I was able to circumvent the Nvidia crypto mining limitations: Unlock RTX 3060 mining hash rate (that post also includes access to the driver). This doubled the RTX 3060 Nicehash mining rate using the ‘unlocked’ Nvidia beta driver – going from ~22MH/s up to ~40MH/s. Impressive!\nNow I am going to share my RTX 3060 overclock settings, which enabled me to achieve a further 10% uplift in Nicehash performance. But first I must disclaimer that you are responsible for your own hardware and any changes you make are at your own risk. What’s more, modern silicone is a lottery, you might not get these exact mining hash rates, or you might get more; keep an eye on your thermals and protect your card and safety of your system accordingly!\n**Important Note - this post is based on a V1 (non LHR RTX 3060)**\nRTX 3060 mining overclock settings Before the overclock I was seeing between 38 and 41.5 MH/s when Nicehash was running the DaggerHashimoto algorithm (on the 470.05 driver). After some tweaks in MSI Afterburner, I was able to consistently strike 44+MH/s, a nice 10% increase.\nMy settings are mainly based on those recommended for the RTX 3060Ti here: https://miningchamber.com/gpu-mining/rtx-3060-ti-mining-settings. The synopsis is as follows:\nLower the core clock rate Lower the power limit(%) Increase the memory clock rate\n**My RTX 3060 MSI afterburner settings:**\nPower limit: 65% Core Clock: -400 MHz Memory Clock + 800 MHz Fan Speed: Auto **EDIT**: I have since managed even greater hash rates, which I have covered in a new post: More RTX 3060 Nicehash overclocking.\nMy current RTX 3060 Overclock for 47MH/s\nI would suggest that these settings rest on the side of caution and are stretching my card too hard. I am happy that the card is stable with these settings and is in fact running a little cooler at the same or lower fan speed than at stock. What’s more, Nicehash suggests that my total wattage has dropped from 140w to **110w** – lowering the ROI for the card.\nI am using the profile switching function in MSI Afterburner to make it easy to flick back to stock settings and play some video games when I want to. I’d recommend this for anyone else mining on their card to record the outrageous cost of buying it.\nHave you overclocked your RTX 3060? Share your afterburner settings and hashrate in the comments below… Let’s see who won the silicon lottery!\n","permalink":"https://exitcode0.net/posts/rtx-3060-nicehash-mining-overclock-settings/","summary":"I recently managed to buy a Gigabyte RTX 3060 without lining the pockets of a scaler, lucky me! I recently discussed how I was able to circumvent the Nvidia crypto mining limitations: Unlock RTX 3060 mining hash rate (that post also includes access to the driver). This doubled the RTX 3060 Nicehash mining rate using the ‘unlocked’ Nvidia beta driver – going from ~22MH/s up to ~40MH/s. Impressive!\nNow I am going to share my RTX 3060 overclock settings, which enabled me to achieve a further 10% uplift in Nicehash performance.","title":"RTX 3060 Nicehash mining overclock settings"},{"content":"There has been lots of news coverage around the recent mistake made by Nvidia with their RTX 3060 driver allowing uninhibited mining hash rates: a great round-up from The Verge. A developer driver inadvertently included code used for internal development which removes the hash rate limiter on RTX 3060 in some configurations. The driver has been removed, but there are lots of copies in circulation.\nThere are still some limitations, you must have a monitor attached and can only run a single RTX 3060 at a time. However, it is only a matter of time before motivated miners get over the remaining hurdles.\nGeForce RTX 3060 GAMING OC 12G How to unlock RTX 3060 mining hash rate My setup:\nA single RTX 3060 connected to a monitor Running Nicehash on Windows 10 I am just a gamer looking to recoup some of the extortionate price I paid for a graphics card which I spent weeks trying to buy 😊 I use NiceHash, because it is convenient and profitable for me, I am not a dedicated miner. If you want to get started with NiceHash, here is my referral link: https://www.nicehash.com/?refby=b5363bcf-2c11-4482-82d2-d224cd1895ab.\nNiceHash covers the impact that the driver limitation has on the RTX 3060’s hashing rate here: https://www.nicehash.com/blog/post/nvidia-rtx-3060-mining-hashrate. On the Gigabyte OC card that I have, I was seeing ~22MH/s before installing the development driver. Whilst this still profitable, it is close to 50% of what the card is capable of.\nInstalling Nvidia development driver 470.05 I was early on the news as was able to get a copy of the driver from a forum user who had repackaged it – noting that the driver didn’t work for MSI cards. The download was hosted on Mega, which always gives me a reason to worry, so I ran the download through virus total first and it came back clean. But it really is the case that when you start installing drivers from alternative sources, you are putting yourself at considerable risk.\nWhilst I can’t confirm how long this download will remain available, I wish you the best of luck in obtaining it:\nhttps://drive.google.com/file/d/12jIy0zkJlikzfUevEmsQRWj8-IP91K-9/view?usp=sharing\nI would recommend running a PowerShell hash check on the file to make sure that it matches below – this will tell you if it has been co-opted:\nPS C:\\Drivers\u0026gt; **Get-FileHash .\\470.05.zip** Algorithm Hash Path --------- ---- ---- SHA256 C97C25360E76ED1252B18E583B93F6446EB3801136F38E9214B770C1A4053A16 C:\\Drivers\\470.05.zip It installs much like installing any other Nvidia driver, run the setup file and reboot when you are done. After a reboot, open up the Nvidia control panel and check your driver version:\nRTX 3060 running driver 470.05 Now that I am running this beta driver, here is some output from NiceHash running the daggerhashimoto algorithm. I am seeing between 39 and 42 MH/s; this is without any overclocking tweaks to the GPU.\nEffecive hashrate between 39 and 42 MH/s – Driver version 470.05 Big disclaimer… You are responsible for your own hardware and system integrity. This is a very bleeding edge subject, so your mileage may vary considerably. Happy mining!\n","permalink":"https://exitcode0.net/posts/unlock-rtx-3060-mining-hash-rate/","summary":"There has been lots of news coverage around the recent mistake made by Nvidia with their RTX 3060 driver allowing uninhibited mining hash rates: a great round-up from The Verge. A developer driver inadvertently included code used for internal development which removes the hash rate limiter on RTX 3060 in some configurations. The driver has been removed, but there are lots of copies in circulation.\nThere are still some limitations, you must have a monitor attached and can only run a single RTX 3060 at a time.","title":"Unlock RTX 3060 mining hash rate"},{"content":" So Honeygain has finally arrived as a Docker container and this article will give you everything you need to build your own docker-compose YAML file for faster deployments.\nYou can see the docs provided by the Honeygain devs on the matter here:\nhttps://honeygain.zendesk.com/hc/en-us/articles/360018979919-How-to-run-Honeygain-on-Docker-Linux-\nHowever, they do not provide a nice way to deploy time and again from a docker-compose file, scroll down for a template! At the time of writing, you are permitted to run the service on two devices per public IP. Unfortunately, the docker image doesn’t currently support the content delivery feature\nRunning the Honeygain docker image (without docker-compose) If oyu just want to run the container here are the steps to do so, provided that you have a docker environment ready to go:\nPull the Docker image docker pull honeygain/honeygain Open Honeygain Terms of Use. If you agree with our Terms of Use, please continue docker run honeygain/honeygain -tou-get Start the Honeygain Docker container docker run honeygain/honeygain -tou-accept -email ACCOUNT_EMAIL -pass ACCOUNT_PASSWORD -device DEVICE_NAME Replace ACCOUNT_EMAIL with your Honeygain account email\nReplace ACCOUNT_PASSWORD with your Honeygain account password\nReplace DEVICE_NAME with a name that you would like to give to your Docker container. This name will be visible on the Dashboard.\nNOTE: Use different DEVICE_NAME for every container that you create.\nHoneygain docker-compose YAML example Now here is how an example of a docker-compose file for the Honeygain container. Please note that you will still need to change your device name between deployments.\nversion: \u0026#39;3\u0026#39; services: honeygain: container_name: honeygain image: honeygain/honeygain command: -tou-accept -email my.name@email.com -pass mylamepassword -device sweethoney01 restart: unless-stopped Change the email, password and device name to suit! And be careful not to publish your compose file with real credentials to any public code repository.\nAnd here is how to run your copose file in detached mode (fromt he direcotry where you YAML file is stored):\ndocker-compose run -d ","permalink":"https://exitcode0.net/posts/honeygain-docker-compose-setup/","summary":"So Honeygain has finally arrived as a Docker container and this article will give you everything you need to build your own docker-compose YAML file for faster deployments.\nYou can see the docs provided by the Honeygain devs on the matter here:\nhttps://honeygain.zendesk.com/hc/en-us/articles/360018979919-How-to-run-Honeygain-on-Docker-Linux-\nHowever, they do not provide a nice way to deploy time and again from a docker-compose file, scroll down for a template! At the time of writing, you are permitted to run the service on two devices per public IP.","title":"Honeygain docker-compose setup"},{"content":" For the longest time, I have been running VMs on my Unraid server and had the need to passthrough USB devices. I will be the first to admit that I have too many computers and a shrink might classify it as an addition. Coupled with my quest for a clutter-free desk, I have been faced with the issue of swapping USB devices like a keyboard and mouse, from one machine to another Unraid virtual machine (vm).\nSolving the physical component of changing the device from one machine to another is easy. Just acquire a USB switcher/KVM such as this:\nA simple USB switch / KVM There are hundreds of these and they are relatively cheap. However, the real problem is that when you switch the keyboard and mouse into the Unraid server (in an attempt to ‘plug’ them into a VM), they are assigned to the Unraid host, not a VM.\nIn the past I have developed a way to use a Pi with a button attached to send virsh commands to Unraid, connecting the USB device to a particular VM. This worked well, but required me to push 2 buttons; one to physically switch the USB devices, followed by another to run the virsh command from a Pi – only mildly irritating. The main irritation was the limit to the number of times you could ‘attach’ a USB device to a VM before its virtual hub became full. So I tried removing the device before attaching it (all using virsh) and ultimately ran out of skill and/or patients.\nWhat I really needed was a USB controller to be permanently pass through to the VM. Then, whenever a device was plugged into that USB controller, it would be connecting to the VM and never the Unraid host. And so then next issue (which we are about to solve), which USB controller and how to pass it through?\nInateck Superspeed 4 Ports PCI-E to USB 3.0 I found the ‘Inateck Superspeed 4 Ports PCI-E to USB 3.0’ on Amazon and took a bit of an educated guess on whether I would be able to pass it through. The supported card/controllers list is very thin on the ground.\nImportantly, it used the Fresco FL1100 series chipset, which to the best of my reading, can be passed through to a VM. It is also popular with crypto mining for this reason.\nHow to pass through the USB controller Prerequisites:\nA working VM CPU and Bios that supports VT-d (and has it enabled) USB pcie card Find the device identifier With the USB card plugged in, we must first identify it. Sadly, I cannot confirm that this card will show up in its own IOMMU group as it did for me, so your mileage may vary. My card shows up in its own IOMMU group and I can see its ID: 1b73:1100.\nFresco FL1100 USB Controller\nEdt Boot config Now we need to tell Unraid to ignore/backlist the device by its ID, so that it will be available for passthrough. Go to the Main page, and click on your flash device to start editing the Syslinux boot config:\nUnder ‘kernel /bzimage‘ paste the following line, replacing the ID with yours appropriately:\nappend initrd=/bzroot pci-stub.ids=1b73:1100 Save this, then it is time to reboot.\nEdit the VM Now we can edit the VM (with it powered off) and add the pcie USB card:\nSUCCESS!\nYou should now have true USB hot swap in your UNRAID VM. I have had this working reliably for a number of weeks at this point, the passthrough config survives a complete system reboot and UNRAID upgrades.\n","permalink":"https://exitcode0.net/posts/unraid-usb-controller-passthrough-vm-hot-swap-usb/","summary":"For the longest time, I have been running VMs on my Unraid server and had the need to passthrough USB devices. I will be the first to admit that I have too many computers and a shrink might classify it as an addition. Coupled with my quest for a clutter-free desk, I have been faced with the issue of swapping USB devices like a keyboard and mouse, from one machine to another Unraid virtual machine (vm).","title":"Unraid USB Controller Passthrough - VM hot swap USB"},{"content":"Cryptocurrencies have been back on the popular agenda lately, thanks mostly to the surging value of Bitcoin. I have been interested, but not massively invested in cryptocurrency for many years. If I had the courage to buy and hold all those years ago, I might be writing to you from a beach – I assure you, this is not the case! I have been getting into InfluxDB lately and its fantastic ability to store heaps of time series data. Well, using python and Coinbase, I am going to show you how to collect bitcoin (and other crypto assets) price data and store it in InfluxDB… then we are going to make some neat panels in Grafana to help us realise our data. This is going to be a longer write up than most, so here is here are links to each section:\nContents Table: Prerequisites Using the Coinbase API with python Writing to InfluxDB with python Creating Coinbase data panels in Grafana Ways to support this blog Prerequisites Before we can jump into programming, there are a number of boxes we need to check off.\nYou must have a Coinbase account. If not, consider using my referral link to get $10 free when you sign up and buy/sell $100 of crypto: https://www.coinbase.com/join/cockin_u Your programming environment has python3 and pip3 installed – pip3 will be used to install the Coinbase module: pip install coinbase You have InfluxDB and Grafana setup. There are a number of great tutorials out there on how to do this, here are some places to get started: Unraid – https://unraid.net/blog/ultimate-unraid-dashboard InfluxDB getting started guide, including docker setup – https://docs.influxdata.com/influxdb/v2.0/get-started/ Grafana setup guide – https://grafana.com/docs/grafana/latest/installation/ Using the Coinbase API with python Whist you can use the Coinbase API without an account there are rate-limiting restrictions, so if you want more representative, ‘live’ data, you will need to create a Coinbase account and apply for an API key:\nWhere to find API options in your Coinbase account For the purposes of what we are doing in this exercise, there is no need to give the API key anything more than read level access to our account:\nCoinbase API read permissions\nPython and Coinbase You can access the full Coinbase API reference here, but we will be focusing on getting price data: https://developers.coinbase.com/api/v2#prices\nBasic Python Request – BTC Buy Price USD from coinbase.wallet.client import Client client: Client(\u0026lt;api_key\u0026gt;, \u0026lt;api_secret\u0026gt;) price: client.get_buy_price(currency_pair: \u0026#39;BTC-USD\u0026#39;) Basic Response – BTC Buy Price USD { \u0026#34;data\u0026#34;: { \u0026#34;amount\u0026#34;: \u0026#34;1020.25\u0026#34;, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34; } } So now that we have the basics from the Coinbase API reference, let us flesh this out into something more reliable and error-proof. Now is also a great time to make this into a function and call it continuously to gather time-series data.\nImproved Python Request The main function, giving us a continuous loop and skipping sending data when a Coinbase API request fails (checking for BTC and ETH):\nimport requests import sys import os import json from time import sleep from coinbase.wallet.client import Client def main(): timer: 0 timer_stop: 120 portfolio_total: 0 coins: [\u0026#34;BTC\u0026#34;, \u0026#34;ETH\u0026#34;] while True: for coin in coins: reqdvalue: func_req_price(coin, \u0026#39;USD\u0026#39;, \u0026#39;sell\u0026#39;) if reqdvalue != False: func_logPrice(coin, reqdvalue, \u0026#39;USD-sell\u0026#39;) reqdvalue: func_req_price(coin, \u0026#39;USD\u0026#39;, \u0026#39;buy\u0026#39;) if reqdvalue != False: func_logPrice(coin, reqdvalue, \u0026#39;USD-buy\u0026#39;) sleep(5) A function to get the price data for a provided coin and kind (please note that we DON’T actually need an API key for this particular method):\ndef func_req_price(coin, currency, kind): try: price: float(requests.get( f\u0026#39;https://api.coinbase.com/v2/prices/{coin}-{currency}/{kind}\u0026#39; ).json()[\u0026#39;data\u0026#39;][\u0026#39;amount\u0026#39;]) return price except: print(\u0026#34;Error getting price for: {}\u0026#34;.format(coin)) return False This returns a price value or false if the request failed for any reason.\nWe now have three values – coin, kind (USD buy/sell) and price) – which we can throw into InfluxDB…\nIMPORTANT NOTE: If you are requesting data more frequently from Coinbase or you are pulling data for far more than just BTC and ETH, you will run into rate limits and receive error 429 responses for your requests. This is way Coinbase suggest using the API supported method of getting prices:\nfrom coinbase.wallet.client import Client client: Client(\u0026lt;api_key\u0026gt;, \u0026lt;api_secret\u0026gt;) price: client.get_buy_price(currency_pair: \u0026#39;BTC-USD\u0026#39;) 3 – Writing to InfluxDB with python I have covered most of the groundwork for this section already in this post: https://exitcode0.net/writing-to-an-influxdb-server-with-python3/.\nGeneric InfluxDB example from datetime import datetime from influxdb import InfluxDBClient def func_log(unit, kind, value): client: InfluxDBClient(host=\u0026#39;127.0.0.1\u0026#39;, port=8086) client.create_database(\u0026#39;DBname\u0026#39;) client.switch_database(\u0026#39;DBname\u0026#39;) json_body: [ { \u0026#34;measurement\u0026#34;: \u0026#34;MeasureName\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;unit\u0026#34;: unit, \u0026#34;kind\u0026#34;: kind }, \u0026#34;fields\u0026#34;: { \u0026#34;value\u0026#34;: value }, \u0026#34;time\u0026#34;: f\u0026#39;{datetime.utcnow().isoformat()}Z\u0026#39; } ] try: if client.write_points(json_body): success: True else: print(\u0026#34;Error writing to InfluxDB\u0026#34;) except: print(\u0026#34;Error writing to InfluxDB\u0026#34;) #Call the above function: func_log(unit, kind, value) Coinbase specific InfluxDB example def func_logPrice(coin, price, kind): client: InfluxDBClient(host=\u0026#39;127.0.0.1\u0026#39;, port=8086) client.switch_database(\u0026#39;PyDCA\u0026#39;) json_body: [ { \u0026#34;measurement\u0026#34;: \u0026#34;stability\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;coin\u0026#34;: f\u0026#39;{coin}\u0026#39;, # e.g. BTC \u0026#34;kind\u0026#34;: f\u0026#39;{kind}\u0026#39; #e.g. USD-sell }, \u0026#34;fields\u0026#34;: { \u0026#34;price\u0026#34;: price }, \u0026#34;time\u0026#34;: f\u0026#39;{datetime.utcnow().isoformat()}Z\u0026#39; } ] try: if client.write_points(json_body): #do nothing success: True else: print(\u0026#34;ERROR writing to InfluxDB\u0026#34;) except: print(\u0026#34;ERROR writing to InfluxDB\u0026#34;) Combining all this code, we can pass any value that we retrieve from the Coinbase API and store it in InfluxDB with a time stamp. This will be very useful when we start creating panels in Grafana!\n4 – Creating Coinbase data panels in Grafana In my last segment of code, you can see that I specify the PyDCA data group when writing to influxDX, remember to set this accordingly in your code and Grafana panels.\nHere is a basic query to display BTC and ETH, USD sell prices as a time series graph; I have set ETH on the right-hand axis as the two values differ so greatly, bring the graphs into a tighter range:\nA basic Grafana time series query Showing the USD sell price provided by the Coinbase API API data flat spot As you can see from the above figure, there are some gaps in our data, this can be caused by a lack of response from the Coinbase API request, an API limit, or a failure to write to the InfluxDB. In my experience, this is a frequent occurrence for BTC and ETC prices in USD; most probably because it is the most popular API endpoint. I do not see correlating flat spots in the data for GBP. In Grafana we can make smooth the graph out by setting the GROUP BY: fill(none) tag. Grafana draws a neat line between data points where there is NULL data.\n5 – Ways to support this blog As previously mentioned, if you are new to Coinbase, you can sign up with my referral link. If you do so, you will receive $10 and so will I (You'll both get free Bitcoin when a friend buys or sells $100 of crypto – Coinbase).\nhttps://www.coinbase.com/join/cockin_u\nAlternatively, you can fund my coffee misadventures by throwing a coin in the tip jar:\nFREE BONUS: Alternatively, you can share this post or leave a comment below to let the search algorithms know I’m not totally useless.\nSee you on the moon HODLers!\n","permalink":"https://exitcode0.net/posts/collect-bitcoin-price-data-with-python-and-coinbase/","summary":"Cryptocurrencies have been back on the popular agenda lately, thanks mostly to the surging value of Bitcoin. I have been interested, but not massively invested in cryptocurrency for many years. If I had the courage to buy and hold all those years ago, I might be writing to you from a beach – I assure you, this is not the case! I have been getting into InfluxDB lately and its fantastic ability to store heaps of time series data.","title":"Collect bitcoin price data with python and Coinbase"},{"content":"A quick GitHub gist for anyone looking to write to an InfluxDB server with Python3. This is a generic function that accepts three inputs; as an example, I am using temperature data in degrees Celsius.\nTurning DHT11 readings into beautiful graphs in Grafana Once you have your data in influxDB, a great way to visualise it is using Grafana. I hope to bring you more posts in the future about visualising your python data with Grafana.\nhttps://gist.github.com/Tombo1001/1f096ca224f0e61ac66548e2abd36a25\nNow that you have the basics of writing events to influxDB with Python, feel free to leave links in the comments on the wonderful things you were able to graph with Grafana.\nMore Python Guides CALCULATING COMPOUND INTEREST WITH PYTHON 3 HOW TO CALL A BASH COMMAND WITH VARIABLES IN PYTHON MONITOR YOUR PUBLIC IP ADDRESS WITH PYTHON Worth noting that all of the above data could find its way onto a Grafana dashboard… the possibilities are endless!\n","permalink":"https://exitcode0.net/posts/writing-to-an-influxdb-server-with-python3/","summary":"A quick GitHub gist for anyone looking to write to an InfluxDB server with Python3. This is a generic function that accepts three inputs; as an example, I am using temperature data in degrees Celsius.\nTurning DHT11 readings into beautiful graphs in Grafana Once you have your data in influxDB, a great way to visualise it is using Grafana. I hope to bring you more posts in the future about visualising your python data with Grafana.","title":"Writing to an InfluxDB server with Python3"},{"content":"Following on from a previous post on how to setup a VLAN on a Fortigate hardware switch, this post is going to explain how we can link an AP-bridge SSID to a hardware switch and VLAN.\nFor the most part, the only reference material you will need to complete this configuration can be found here: https://docs.fortinet.com/document/fortiap/6.4.0/fortiwifi-and-fortiap-cookbook/252439/configuring-the-fortigate-interface-to-manage-fortiap-units. However, if you have been working with Fortigates and by extension FortiOS for quite some time, you may be wonder where the CAPWAP option vanished to? Fortinet adopted this into the security fabric naming convention. CAPWAP has even been replaced in the CLI by ‘fabric’.\nAP-Bridge with a VLAN The confif that I a looking to build will give me an SSID for a given VLAN; a client which is connected to this SSID wil be given a DHCP address and will be subject to firewall policy for that VLAN. The client will not be required to set their own VLAN tag – traffic will be tagged by the SSID interface.\nV33 is an AP-brige to VLAN 33 on the hardware switch. We are also going to have an SSID on the ‘default VLAN’ aka a typical wireless LAN – sharing the same subnet and multicast zone as the ports in our hardware switch.\nUltimately we will have an SSID which is isolated from all other ‘LAN’ traffic, perfect for wireless CCTVs cameras or IoT device which we don’t trust.\nThe Configuration Hardware Switch and VLAN Much of this configuration was covered in a previous post, see here: https://exitcode0.net/fortigate-add-a-vlan-to-a-hardware-switch.\nAt this point we will have a hardware switch and a VLAN assigned to it – VLAN 33.\nOne important change is to enabled security fabric management (previously CAPWAP) on our hardware switch:\nSecurity Fabric Connection is enabled – previous known as CAPWAP. If you choose to operate in the CLI, here is how you would implement this:\nconfig system interface edit LAN #(hardware switch name) set allow-access fabric set ap-discover enable next end SSID Configuration Setting traffic mode to AP-bridge and Optional VLAN ID to 33, we create an SSID with a suitabe BSSID and security settings as follows:\nan SSID for VLAN 33 In AP-bridge mode, all traffic on SSID V33 will be tunneled to the Fortigate’s security fabric controller and to the VLAN interface there onwards.\nAnd that is it! It really is that simple. It is worth mentioning, that this is also possible with a software switch, but not without significant throughput penalties – all switching passes through the the Fortigate’s CPU in a software switch.\n","permalink":"https://exitcode0.net/posts/fortigate-ap-bridge-with-a-hardware-switch/","summary":"Following on from a previous post on how to setup a VLAN on a Fortigate hardware switch, this post is going to explain how we can link an AP-bridge SSID to a hardware switch and VLAN.\nFor the most part, the only reference material you will need to complete this configuration can be found here: https://docs.fortinet.com/document/fortiap/6.4.0/fortiwifi-and-fortiap-cookbook/252439/configuring-the-fortigate-interface-to-manage-fortiap-units. However, if you have been working with Fortigates and by extension FortiOS for quite some time, you may be wonder where the CAPWAP option vanished to?","title":"Fortigate: AP-Bridge with a hardware switch"},{"content":" In this post, we are going to discuss how to add a VLAN to a hardware (sometimes referred to as physical) switch or interface on a Fortigate. It is worth noting that I actually do my testing on a FortiWifi, so I can assure you that this also applies there too. I will be focusing on the configuration which is relevant to FortiOS v6.0 and above, so your millage vary between versions. However, you need can usually be found over at the Fortinet Handbook: https://docs.fortinet.com/product/fortigate/6.0.\nSwitch mode Vs Interface Mode I’m not going to get too far into the minutia of switch vs interface mode. There are many arguments for both and I believe that the correct answer is very much dependant on your intended implementation. I have my Fortigate 60E configured in interface mode, with most of my LAN ports, 2-7, configured in a hardware switch. This hardware switch is operating in what you may class as the default VLAN. Most of the configuration which we are going to discuss is basic and will apply to a Fortigate in (software) switch mode. I will try my best to point out any obvious configuration differences where they may arise.\nVLANs in NAT mode It is important to mention that I have my Fortigate 60E running in NAT mode. In NAT mode, the FortiGate supports VLAN trunk links with IEEE 802.1Q‑compliant switches or routers. The trunk link transports VLAN-tagged packets between physical subnets or networks. When you add VLAN sub-interfaces to the FortiGate physical interfaces, the VLANs have IDs that match the VLAN IDs of packets on the trunk link. The FortiGate directs packets with VLAN IDs to sub‑interfaces with matching IDs.\nWhereas in Transparent mode, the Fortigate behaves like a layer-2 bridge but it can still provide services such as antivirus scanning, web filtering, spam filtering, and intrusion protection to traffic. There are some limitations in transparent mode because you can’t use SSL VPN, PPTP/L2TP VPN, DHCP server, or easily perform NAT on traffic. The limits in transparent mode apply to IEEE 802.1Q VLAN trunks passing through the device.\nFortigate VLAN Config Example Physical Switch Config I will start by splitting an interface out of the default hardware switch. I am chosing to do this because I want to preseve the rest of the LAN ports for use in my ‘regular’ LAN. Interface LAN1 will be used for VLANs and connected to a managed switch whih supports VLANs.\nTwo hardware switches – one with VLANs The reality of having ‘two hardware switches’ is that the Fortigate created two virtual-switches, nested under one physical-switch. This is easily demonstrated in the subsequent config:\nconfig system physical-switch edit \u0026#34;sw0\u0026#34; set age-val 0 next end config system virtual-switch edit \u0026#34;internal\u0026#34; set physical-switch \u0026#34;sw0\u0026#34; config port edit \u0026#34;internal2\u0026#34; next edit \u0026#34;internal3\u0026#34; next edit \u0026#34;internal4\u0026#34; next edit \u0026#34;internal5\u0026#34; next edit \u0026#34;internal6\u0026#34; next edit \u0026#34;internal7\u0026#34; next end next edit \u0026#34;VTRUNK\u0026#34; set physical-switch \u0026#34;sw0\u0026#34; config port edit \u0026#34;internal1\u0026#34; next end next end VLAN Config Now that we have split out our hardware switches, we can start adding VLANs. Enabling a DHCP server on a VLAN is optional, but I have no alternatives in my home network and most of the devices I plan to isolate in my VLANs can be troublesome to set static addresses on.\nBasic VLAN config on a hardware switch It is worth noting that, I did not enable a DHCP server on my LAN1 interface, so if my device is connected and does not have a valid VLAN tag set, it will be assigned an IP address. I named internal1 one to reflect that it will be used as a trunk port for the traffic of multiple VLANs to be carried over to a managed switch. Here is the CLI config for each of my VLAN interfaces:\nconfig system interface edit \u0026#34;internal\u0026#34; set vdom \u0026#34;root\u0026#34; set ip ---.---.---.--- ---.---.---.--- set allowaccess ping https ssh set type hard-switch set alias \u0026#34;LAN\u0026#34; set stp enable set snmp-index 6 next edit \u0026#34;VTRUNK\u0026#34; set vdom \u0026#34;root\u0026#34; set ip 172.16.30.1 255.255.255.0 set allowaccess ping set type hard-switch set description \u0026#34;VLAN TRUNK\u0026#34; set alias \u0026#34;VLAN TRUNK\u0026#34; set device-identification enable set lldp-transmission enable set role lan set snmp-index 11 next edit \u0026#34;V31-IoT\u0026#34; set vdom \u0026#34;root\u0026#34; set ip 172.16.31.1 255.255.255.0 set allowaccess ping set description \u0026#34;VLAN IoT\u0026#34; set alias \u0026#34;VLAN IoT\u0026#34; set device-identification enable set role lan set snmp-index 10 set interface \u0026#34;VTRUNK\u0026#34; set vlanid 31 next edit \u0026#34;V32-Hue\u0026#34; set vdom \u0026#34;root\u0026#34; set ip 172.16.32.1 255.255.255.0 set allowaccess ping set description \u0026#34;VLAN32 Hue\u0026#34; set alias \u0026#34;VLAN Hue\u0026#34; set device-identification enable set role lan set snmp-index 12 set interface \u0026#34;VTRUNK\u0026#34; set vlanid 32 next end ","permalink":"https://exitcode0.net/posts/fortigate-add-a-vlan-to-a-hardware-switch/","summary":"In this post, we are going to discuss how to add a VLAN to a hardware (sometimes referred to as physical) switch or interface on a Fortigate. It is worth noting that I actually do my testing on a FortiWifi, so I can assure you that this also applies there too. I will be focusing on the configuration which is relevant to FortiOS v6.0 and above, so your millage vary between versions.","title":"Fortigate: Add a VLAN to a hardware switch"},{"content":" I have covered this a number of times in the past and the posts have proved popular and useful to many. So, here is my guide for updating to the latest version of Python 3 (3.9) on Debian 10 Buster.\nTo clarify the purpose of this guide, Debian 10 ships with Python 2 (2.7) and Python 3 (3.7) installed at my time of writing. For those wishing to upgrade from python 3.7.X to 3.8.X or 3.9.X, this is the guide for you. If you are trying to configure python 3.7 as your default interpreter when you call ‘python‘, try this: CHANGING THE DEFAULT PYTHON VERSION IN DEBIAN. This method involves using the ‘update-alternatives‘ command. We will be using a similar method in this guide, however this time we only do so to give 3.9.X a higher priority to 3.7.X, rather than uninstalling older versions.\nThe basic premise is, install the version of python 3 desire, 3.9, then configure Debian to use python 3.7 at a higher priority to python 3.9.\nThis guide is written to target those using Debian 10, but the same principles apply to older versions of Debian and other operating system based on Debian, such as Kali Linux.\nThe Debian 10, Python upgrade process Check your version\nStep 1 is to check your current python version:\npython3 -V or\npython3 --version Download the latest or desired version of python 3\nNext, we need to download the latest version or desired version of python 3 from the python website. In my case, I selected 3.9.1. Once downloaded we need to extract the tar file.\nwget https://www.python.org/ftp/python/3.9.1/Python-3.9.1.tar.xz tar xf Python-3.9.1.tar.xz cd Python-3.9.1 You can find all versions of Python here: https://www.python.org/ftp/python\nPython 3.9.1 downloading…\nMake and Install\nNow that we have the files downloaded and extracted, it is time to compile them.\n./configure make make install ‘make’ commands can take quite some time to run, this is normal when compiling large programs from source…\nNOTE: if you are running a minimal install of Debian 10, you might need to install a C compiler before you can run ‘./configure‘ and the tool for running ‘make‘ commands:\napt-get update -y apt-get upgrade -y apt-get install build-essential -y apt-get install make -y Switch to the new Python version Finally, after compiling the new version of python from source, we can now configure Debian to make it our default version of python3.\nupdate-alternatives --install /usr/bin/python python3 /usr/local/bin/python3.9 10 The integer at the end of this command (10) sets the priority for the python version; the greater the integer, the higher the priority. At this point, we can rerun the previously used version commands and we should see that we now have Python 3.9.1 active.\nFixing and Updating Pip It was at this point that I attempted to install some required addons using pip and discovered that the upgrade to Python 3.9.1 had broken a few things. These were the commands I used to resolve issues with lsb_release and pip:\nln -s /usr/share/pyshared/lsb_release.py /usr/local/lib/python3.9/site-packages/lsb_release.py pip3 install --upgrade pip If you have found this guide useful or it has solved a burning issue for you, please consider throw a coin in the tip jar to help this site stay active:\nhttps://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick\u0026amp;hosted_button_id=BTQD4GN8TTWJN\u0026amp;source=url\nSome useful guides I found along the way: https://jcutrer.com/linux/upgrade-python37-ubuntu1810 – How to upgrade to python 3.7 on ubuntu 18.10. https://tecadmin.net/install-python-3-7-on-ubuntu-linuxmint/ – How to Install Python 3.7 on Ubuntu, Debian and LinuxMint. https://linuxconfig.org/how-to-change-default-python-version-on-debian-9-stretch-linux – How to change default python version on Debian 9 Stretch Linux Other Useful Debian tips: https://exitcode0.net/posts/debian-9-running-a-python-script-at-startup/ – Debian 9 – Running a python script at boot. ","permalink":"https://exitcode0.net/posts/debian-10-how-to-upgrade-python-3-7-to-python-3-9/","summary":"I have covered this a number of times in the past and the posts have proved popular and useful to many. So, here is my guide for updating to the latest version of Python 3 (3.9) on Debian 10 Buster.\nTo clarify the purpose of this guide, Debian 10 ships with Python 2 (2.7) and Python 3 (3.7) installed at my time of writing. For those wishing to upgrade from python 3.","title":"Debian 10 - How to upgrade python 3.7 to python 3.9"},{"content":" Compound stonks only go up! I have a growing interest in finance and analytics, so it felt like a great idea to start creating my own set of financial tools with Python. This article will explain how I created a simple but effective script for calculating compound interest with Python. I wrote this in Python 3 (as all new Python projects should be from now onwards), but the library dependencies are very lightweight, making this something that could easily be re-written from Python 2.\nCalculating Compound Interest This small programming challenge hinges on the mathematics behind compound interest. If you don’t already have a clear understanding of how compound interest is calculated vs ‘simple’ interest, I would recommend starting here:\nhttps://www.mathsisfun.com/money/interest.html\nEssentially we need to take the following logic and create it in Python:\nWith **compounding,**we work out the interest for the first period, add it to the total, and then calculate the interest for the next period, and so on\nsource: https://www.mathsisfun.com/money/interest.html The Code import math def func_invest(input1, input2): output: input1 + input2 return output def func_interest(input1, input2): rate: input2 / 100 interest: input1 * rate output: input1 + interest return output def func_pl_calc(input1, input2, input3): output: (input1 - input2) - input3 return output def func_round(input, decimals=2): multiplier: 10 ** decimals return math.floor(input*multiplier + 0.5) / multiplier def func_main(): currency: \u0026#34;£\u0026#34; #change to suit monthly_invest: 10 #change to suit total_invested: 0 starting_portfolio: 10000 #change to suit total_portfolio: starting_portfolio cur_month: 1 investment_period: 120 #(months) change to suit interest_period: 12 #(months) change to suit interest_percent: 5 #(%) change to suit print(\u0026#34;-------------\\nInterest rate during investment period: {}%\u0026#34;.format(interest_percent)) print(\u0026#34;Month investment amount: {}{}\u0026#34;.format(currency, monthly_invest)) print(\u0026#34;Starting balance: {:.2f}\\n-------------\u0026#34;.format(starting_portfolio)) while cur_month \u0026lt;= investment_period: #investment math total_invested: func_invest(total_invested, monthly_invest) total_portfolio: total_portfolio + monthly_invest #check if interest payment due on balance if cur_month % interest_period == 0: total_portfolio: func_interest(total_portfolio, interest_percent) total_portfolio: func_round(total_portfolio) #increment investment period cur_month: cur_month + 1 print(\u0026#34;Total amount invested at end of period: {}{:.2f}\u0026#34;.format(currency, total_invested)) print(\u0026#34;Interest value during investment period ({:.0f} year[s]): {}{:.2f}\u0026#34;.format(investment_period / 12, currency, func_pl_calc(total_portfolio, total_invested, starting_portfolio))) print(\u0026#34;\\nPortfolio Value at end of period: {}{:.2f}\\n-------------\u0026#34;.format(currency, total_portfolio)) if __name__ == \u0026#39;__main__\u0026#39;: func_main() Also available on GitHub: https://github.com/Tombo1001/Py-Compound/\nSome of my functions are incredibly basic and completely unnecessary, but I find that it helps to remove the mathematics from the main logic of the script. Feel free to consolidate these into the main function.\nThe Outcome With the code above you should get the following output, noting that interest is calculated at the end of each interest period:\nStarting with the current variables defined in the GitHub repository Changing when interest is calculated (before or after the monthly investment), impacts the end portfolio value.\nIt is possible to realise our compounded interest with a graph, showing that our interest gains increase over time as our portfolio value increases:\nsource: https://www.thecalculatorsite.com/finance/calculators/compoundinterestcalculator.php The Next Steps Now that we have some basic code to calculate compound interest, here are some great next steps for this project:\nCreate a Python Django web app and publish the tool for everyone on the internet to use. Integrate variable interest rates during the investment period. Create graphs of the data as seen above using the popular Matplotlib library. Perhaps you will see all over the above in a post here sooner or later!\n","permalink":"https://exitcode0.net/posts/calculating-compound-interest-with-python-3/","summary":"Compound stonks only go up! I have a growing interest in finance and analytics, so it felt like a great idea to start creating my own set of financial tools with Python. This article will explain how I created a simple but effective script for calculating compound interest with Python. I wrote this in Python 3 (as all new Python projects should be from now onwards), but the library dependencies are very lightweight, making this something that could easily be re-written from Python 2.","title":"Calculating compound interest with Python 3"},{"content":"Are you still waiting for your Windows 10 computer to receive the 20H2 update? Here is how to force the windows 10 20H2 update. For many, the update may already available the Windows 10 update settings:\nThe 20H2 update when available in the Windows 10 update settings. Microsoft is staggering the over-the-air update as they usually do with new major releases. However if you are tired of hitting ‘check for updates’ in the windows 10 update settings, there is a way to press the issue:\nWindows 10 update/download site (EN-GB) https://www.microsoft.com/en-gb/software-download/windows10\nVisit the official Windows 10 download site that allows you to download and install the in-place upgrade tool. This will handle the download and installation of the 20H2 update.\nOne tip for ensuring a seamless upgrade is to disable or suspend BitLocker or any other 3rd party full disk encryption system; particularly if you are upgrading from 1809 or earlier.\nWhat’s new in 20H2? Not much if we are honest and this likely why it marks to start of a new update naming convention.\nRefreshing updates from the Start menu to the Taskbar and more: Source: https://blogs.windows.com/windowsexperience/2020/10/20/whats-new-in-the-windows-10-october-2020-update/\nThe new version of Microsoft Edge by default for new installs – installed for those who upgrade. More updates for commercial and education customers: Simpler device management More secure biometric sign-on Stronger app protection ","permalink":"https://exitcode0.net/posts/how-to-force-the-windows-10-20h2-update/","summary":"Are you still waiting for your Windows 10 computer to receive the 20H2 update? Here is how to force the windows 10 20H2 update. For many, the update may already available the Windows 10 update settings:\nThe 20H2 update when available in the Windows 10 update settings. Microsoft is staggering the over-the-air update as they usually do with new major releases. However if you are tired of hitting ‘check for updates’ in the windows 10 update settings, there is a way to press the issue:","title":"How to force the Windows 10 20H2 update"},{"content":"Here is my quick and easy way to raise the privilage level of a Windows Batch script; allowing you to run your code at an administrator level.\nThis is not a new question and has been asked many times on StackOverflow forums. The best answer I was able to find was the following from dbenham:\nhttps://stackoverflow.com/questions/1894967/how-to-request-administrator-access-inside-a-batch-file/10052222#10052222\nThat being said there are many ways to skin this cat, so I came up with my own method, all be it derivative. The main difference in the following code is that you will always get a UAC prompt in my variant of the code, even if you are currently running it as an account that is a member of the local administrators group:\nUAC prompt invoked! Full Code: ::::::::::::::::::::::::::::::::::::::::: :: Automatically check \u0026amp; get admin rights ::::::::::::::::::::::::::::::::::::::::: @echo off CLS ECHO. ECHO ============================= ECHO Running Admin shell ECHO ============================= :checkPrivileges NET FILE 1\u0026gt;NUL 2\u0026gt;NUL if \u0026#39;%errorlevel%\u0026#39; == \u0026#39;0\u0026#39; ( goto gotPrivileges ) else ( goto getPrivileges ) :getPrivileges if \u0026#39;%1\u0026#39;==\u0026#39;ELEV\u0026#39; (shift \u0026amp; goto gotPrivileges) ECHO. ECHO ************************************** ECHO Invoking UAC for Privilege Escalation ECHO ************************************** setlocal DisableDelayedExpansion set \u0026#34;batchPath=%~0\u0026#34; setlocal EnableDelayedExpansion ECHO Set UAC: CreateObject^(\u0026#34;Shell.Application\u0026#34;^) \u0026gt; \u0026#34;%temp%\\OEgetPrivileges.vbs\u0026#34; ECHO UAC.ShellExecute \u0026#34;!batchPath!\u0026#34;, \u0026#34;ELEV\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;runas\u0026#34;, 1 \u0026gt;\u0026gt; \u0026#34;%temp%\\OEgetPrivileges.vbs\u0026#34; \u0026#34;%temp%\\OEgetPrivileges.vbs\u0026#34; exit /B :gotPrivileges :::::::::::::::::::::::::::: ::START :::::::::::::::::::::::::::: setlocal \u0026amp; pushd . ECHO Hello World, I am your admin: whoami pause Find the code on GitHub: https://github.com/Tombo1001/UACprod\n","permalink":"https://exitcode0.net/posts/how-to-invoke-uac-in-a-windows-batch-script/","summary":"Here is my quick and easy way to raise the privilage level of a Windows Batch script; allowing you to run your code at an administrator level.\nThis is not a new question and has been asked many times on StackOverflow forums. The best answer I was able to find was the following from dbenham:\nhttps://stackoverflow.com/questions/1894967/how-to-request-administrator-access-inside-a-batch-file/10052222#10052222\nThat being said there are many ways to skin this cat, so I came up with my own method, all be it derivative.","title":"How to invoke UAC in a Windows Batch script"},{"content":"A quick Google of the above only seemed to give me answers for the inverse, calling a python script from a bash shell and handing it variables. At least, the first 3 results showed this and I’m probably not alone when it comes to scrolling down past the StackOverflow articles.\nSo I am had to go it alone and the way that I figured out how to call a bash command with variables in Python (3), is a total hack… But it works. This is commonly referred to as ‘getting the job done’ code.\nhttps://xkcd.com/979/ Background Here is some quick background info about what I am trying to do, to help you understand some of my variable names and the particular bash commands I am using.\nI am currently playing around/working on a tool to build easy listening playlist videos with ffmpeg. For millions of reasons, but specifically one (I gave up on figuring out the python library), I want to use ffmpeg in bash, but write everything else my script does is python. Therefore, I need a way to call bash commands from python: easy; and pass variables to that bash command: less easy.\nMaxiumum effort photoshop The Code This is all based on using the ‘Subprocess’ library rather than just the classic ‘os.system’ method. Subprocess lets us do some cool things – technical things – so we must first go ahead and import it (install it with pip3 if required):\nImport subprocess Next, I will define my command which I want to send over to bash for execution, and the variables I want to pass to it (not in that order):\nmy_variable_1: \u0026#34;Goodbye\u0026#34; my_variable_2: \u0026#34;Jupiter\u0026#34; command: (\u0026#34;echo {}\u0026#34;.format(my_variable1, my_variable_2) At this point we can choose to print out the command to the python console to check our syntax:\nprint(command) # optional Now we can send our command to the bash shell and await it’s completion.\nprocess: subprocess.call(command, shell=True) Upon execution of the command, the process variable can be used to check the status of our efforts in bash, allowing us to form a condition for proceeding or returning if we are inside a function:\nif process == 0: print(\u0026#34;we did it, exitcode0 (.net)\u0026#34;) #return else: print(\u0026#34;check your command syntax\u0026#34;) exit(1) The key to passing variables revolves around building our command before calling it with subprocess: “command: (“echo {}”.format(my_variable1, my_variable_2)“\nWith the code above we are currently only checking the status code of the command which we have run. However, it is possible to capture the standard output of the command and bring that back into python for further programming; to do this we need to use subprocess.popen:\ncommand: \u0026#34;ls -la /home/foo\u0026#34; process: subprocess.Popen(command, stdout=subprocess.PIPE) output: process.stdout.read() If we were to print ‘output‘ we should hope to see the standard output of ‘ls -la‘ in the directory ‘/home/foo‘. You could take things further in bash by piping the output to grep before reading it back in with python; an alternative to further string processing in Python.\nMore Python snippets: KASA HOME AUTOMATION WITH IFTTT AND PYTHON WEBHOOKS – https://exitcode0.net/kasa-home-automation-with-ifttt-and-python-webhooks/ PYTHON 3 SSH WITH PARAMIKO – https://exitcode0.net/posts/python-3-ssh-with-paramiko/ ","permalink":"https://exitcode0.net/posts/how-to-call-a-bash-command-with-variables-in-python/","summary":"A quick Google of the above only seemed to give me answers for the inverse, calling a python script from a bash shell and handing it variables. At least, the first 3 results showed this and I’m probably not alone when it comes to scrolling down past the StackOverflow articles.\nSo I am had to go it alone and the way that I figured out how to call a bash command with variables in Python (3), is a total hack… But it works.","title":"How To call a bash command with variables in Python"},{"content":"This is my lean and effective way to monitor your public IP address with Python, specifically Python3. The script – code and GitHub link below – runs on a continuous loop, which I covered in a previous post: here.\nNOTE: this code is reliant on the dig system command – so this essentially only work on a unix based system. I was running this on a Debian install.\nimport os import subprocess import re import time print(\u0026#34;start monitor vpn monitor check\u0026#34;) expected_IP: \u0026#34;0.0.0.0\u0026#34; # ENTER YOUR EXPECTED PUBLIC IPv4 ADDRESS HERE current_IP: subprocess.check_output(\u0026#34;dig +short myip.opendns.com @resolver1.opendns.com\u0026#34;, shell=True) try: while True: if expected_IP in str(current_IP): print(\u0026#34;IPs Match - Things are normal\u0026#34;) else: print(\u0026#34;Current IP: \u0026#34; + str(current_IP) + \u0026#34;\\nIP NOT AS EXPECTED!\u0026#34;) #Code to complete actions called here time.sleep(60) # Change this timer to reduce/increase time between checks (seconds) except KeyboardInterrupt: print(\u0026#34;\\nHard Exit Initiated. Goodbye!\u0026#34;) https://github.com/Tombo1001/PyIPmonitor\nAlternative uses There are a number of use cases for this code, mine was checking if my computer is still connected to a VPN. Monitor my public IP address meant that I could tell if the VPN had disconnected.\nChecking my IP matches my VPN endpoint IP The only thing that was changed here from the current repository code is the print statements – the output just matches the context of the code’s use.\nMore python snippets Here are some links to some of my other python code samples:\nMy python code samples – https://exitcode0.net/code-samples/#python Python3 SSH with paramiko – https://exitcode0.net/posts/python-3-ssh-with-paramiko/ ","permalink":"https://exitcode0.net/posts/monitor-your-public-ip-address-with-python/","summary":"This is my lean and effective way to monitor your public IP address with Python, specifically Python3. The script – code and GitHub link below – runs on a continuous loop, which I covered in a previous post: here.\nNOTE: this code is reliant on the dig system command – so this essentially only work on a unix based system. I was running this on a Debian install.\nimport os import subprocess import re import time print(\u0026#34;start monitor vpn monitor check\u0026#34;) expected_IP: \u0026#34;0.","title":"Monitor your public IP address with Python"},{"content":"I recently migrated my ‘physical’ Windows 10 desktop to a ‘virtual’ machine running under UNRAID (libvert). This is a quick guide on how I was able to use my existing installation on a SSD and pass it through to a VM.\nWhy passthrough a drive to a VM? In some cases, it might be beneficial to start from scratch when creating a Windows 10 VM on UNRAID, creating a new virtual disk and having a fresh install. I chose to take an alternative path because I was contempt with my Windows install as it was. I had my Steam/Epic Games library downloaded onto one of the two disks already in the system and a number of applications installed and configured just the way I like them.\nIn short, being able to preserve my disks in their current state would save me hours of unnecessary reconfiguration.\nSo why not create an image of those disks and place them on the UNRAID array or cache? Well, I plan to have multiple VMs on my UNRAID server (other Physical to Virtual conversions), a number of docker containers, applications and uses for the array which will all be IO heavy operations. Giving my Windows VM an independent disk subsystem will ensure that it never encounters Disk IO bottlenecks. For me, this is a desirable feature.\nMy Windows 10 VM, with it’s two physical disks. The UNRAID prerequisites To pass through a drive to a UNRAID VM, there are a number of prerequisites that must be fulfilled. The following plugin will be needed:\nDlandon’s Unassigned Devices – this is a great plugin that allows a drive that is not a part of the array or cache to be used in a number of ways. It makes it possible to mount, share, or pass through an unassigned device (disk).\nMy two Windows disks marked for ‘pass thru’ Once you have enabled ‘pass thru’ for your disk, it is worth taking a note of the disk IDs, these will be needed for the following stages:\nINTEL_SSDSC2BF240A4L_CVDA411205W12403GN WDC_WD5000LPLX-75ZNTT0_WXP1EC54W1JD UNRAID VM Configuration Now we can look to create our Windows 10 VM, using our passed through Disks (Intel SSD boot drive and Western Digital HDD data drive). Here is a brief list of my settings:\nMachine: Q35-4.2 BIOS: OVMF Hyper-V: No Primary Disk: Manual, /dev/disk/by-id/, sata 2nd Disk: Manual, /dev/disk/by-id/, sata Any other devices, CPU cores and Memory allocations. My Windows 10 VM configuration. NOTE: My physical Windows installation was configured with UEFI (The C: drive was a GPT disk). If you were using legacy/BIOS, to boot into windows (Your C: drive is a BMR disk) you will need to set the UNRAID VM’s BIOS to SeaBIOS.\nThe Result A Windows 10 VM with ‘physical’ disks and in my case a physical GPU too! My Windows instance booted up, reconfigured to account for the change in available memory and CPU, and rebooted again once this was complete.\nI did need to mount the virtIO ISO file and install chipset and network drivers – this also makes Windows observe a VM stop command as a shutdown signal, much like VMware tools.\nMore UNRAID Nuggets https://exitcode0.net/posts/convert-an-esxi-vm-to-unraid-kvm-with-qume-img-convert/ – CONVERT AN ESXI VM TO UNRAID KVM WITH QUME-IMG CONVERT ","permalink":"https://exitcode0.net/posts/how-to-pass-through-a-drive-to-a-unraid-vm/","summary":"I recently migrated my ‘physical’ Windows 10 desktop to a ‘virtual’ machine running under UNRAID (libvert). This is a quick guide on how I was able to use my existing installation on a SSD and pass it through to a VM.\nWhy passthrough a drive to a VM? In some cases, it might be beneficial to start from scratch when creating a Windows 10 VM on UNRAID, creating a new virtual disk and having a fresh install.","title":"How to pass through a drive to a UNRAID VM"},{"content":" I recently moved to a new Windows 10 laptop and experience a number of problems with my Bose SoundLink II, Bluetooth headphones. Fortunately, after some playing around with drivers I was able to solve my problem and this post explains how…\nThe Problem Poor sound quality – with the default drivers, the sound quality was poor and there was a considerable amount of audible noise being delivered to the headphones via the Bluetooth interface. The audio was very lo-fi and the frequency range was dramatically reduced. This resulted in a very thin, tinny sound.\nMic feedback loop – the Bose Soundlink headphones have a built-in mic, meaning that you can use it as a mic enabled headset for calls. With the default drivers which Windows was allocating to the headphones, I could hear a constant mic feedback loop whilst there was active audio output from the system. It was possible to hear yourself tapping on your keyboard through your headphones.\nThe Solution Connect your headphones to the laptop via Bluetooth. Open device manager (devmgmt.msc) as an administrator. Open the properties of the Bluetooth Audio device, go to the Driver tab: My headphones are called ‘KeepYourVinyl’ 4. Update Driver and Browse my computer for driver software. Pick from a list of available drivers on my computer.\n5. Find and select ‘Microsoft Bluetooth A2dp Source’ and proceed with the installation.\nDriver selection 6. Your audio device will likely stop working at this point and you will receive a dialog prompting you to reboot to complete the driver setup.\nFull driver details After a reboot, my headphones connected o the laptop and audio was as you would expect! Audio quality is now on par with that when connecting to an iPhone or Android device.\n","permalink":"https://exitcode0.net/posts/fixed-bose-bluetooth-headphones-with-windows-10/","summary":"I recently moved to a new Windows 10 laptop and experience a number of problems with my Bose SoundLink II, Bluetooth headphones. Fortunately, after some playing around with drivers I was able to solve my problem and this post explains how…\nThe Problem Poor sound quality – with the default drivers, the sound quality was poor and there was a considerable amount of audible noise being delivered to the headphones via the Bluetooth interface.","title":"[FIXED] Bose Bluetooth headphones with Windows 10"},{"content":"Are you still waiting for your Windows 10 computer to receive the 2004 update? Eager to play with WSL2 like I was? Here is how to force the windows 10 2004 update.\nMicrosoft is staggering the over-the-air update as they usually do with new major releases. However if you are tired of hitting ‘check for updates’ in the windows 10 update settings, there is a way to press the issue:\nhttps://www.microsoft.com/en-gb/software-download/windows10\nVisit the official Windows 10 download site allows you to download and install the in-place upgrade tool. This will handle the download and installation of the 2004 update.\nOne tip for ensuring a seamless upgrade is to disable or suspend BitLocker or any other 3rd party full disk encryption systems.\nWhat’s new in 2004? Well, there is definitely more to come on this from me, so consider dropping a bookmark or joining my notification list (bell icon bottom left). I am very keen to test WSL2 with Ubuntu and Kali Linux. Watch this space!\n","permalink":"https://exitcode0.net/posts/how-to-force-the-windows-10-2004-update/","summary":"Are you still waiting for your Windows 10 computer to receive the 2004 update? Eager to play with WSL2 like I was? Here is how to force the windows 10 2004 update.\nMicrosoft is staggering the over-the-air update as they usually do with new major releases. However if you are tired of hitting ‘check for updates’ in the windows 10 update settings, there is a way to press the issue:","title":"How to force the Windows 10 2004 update"},{"content":"Error Correcting Code (ECC) RAM is a variation of coputer memory which helps to ilimintate data curruption or ‘bit rot’, but it is not always imediately apparent if your system memory is running in ECC mode; here is a quick guide on how to check if your system memory is running in ECC mode. This guide covers Windows and Linux systems, but please ensure that if you are running either system in a virtual machine configuration that this command is ran on the host machine.\nLinux – Check if ECC Mode is enabled To check if a system’s memory is running in ECC mode on a Linux system, open up the terminal (elevation may be required in some instances) and type the following command:\ndmidecode -t memory or\ndmidecode --type memory Along with other memory statistics and information, your will receive the following item line if ECC is active and the mode in which it is running:\nError Correction Type: Multi-bit ECC Furthermore, the dmidecode tool is very useful for learning more about the other areas of your system from the Linux terminal. You can find a great breakdown of some of its features here: https://www.redhat.com/sysadmin/linux-tools-dmidecode\nWindows – Check if ECC Mode is enabled Open the windows command prompt and enter the following wmic command:\nwmic memphysical get memoryerrorcorrection Using CMD to check if ECC is enabled\nNote: if you are struggling to run WMIC command in your command prompt and receiving an error that ‘wmic is not recognized as an internal or external command‘, consider checking out this post where I demonstrate how to fix that: https://exitcode0.net/posts/adding-wmic-command-to-the-windows-path/\nAs you can see from the example above, my Windows laptop returned a value of 3, but what does this mean?\nValue Meaning 0 (0x0) Reserved 1 (0x1) Other 2 (0x2) Unknown 3 (0x3) None 4 (0x4) Parity 5 (0x5) Single-bit ECC 6 (0x6) Multi-bit ECC 7 (0x7) CRC Unsurprisingly my laptop is not running any kind of ECC memory and returns None.\n","permalink":"https://exitcode0.net/posts/how-to-check-if-ram-is-running-in-ecc-mode/","summary":"Error Correcting Code (ECC) RAM is a variation of coputer memory which helps to ilimintate data curruption or ‘bit rot’, but it is not always imediately apparent if your system memory is running in ECC mode; here is a quick guide on how to check if your system memory is running in ECC mode. This guide covers Windows and Linux systems, but please ensure that if you are running either system in a virtual machine configuration that this command is ran on the host machine.","title":"How to check if RAM is running in ECC mode"},{"content":"wmic is not recognized as an internal or external command – I was quite shocked to find that a command I use on a very regular basis was not working on a fresh installation of Windows 10 1909 on my trust old ThinkPad.\nI don’t want to spend hours trying to find out why this was not correct in my system path, but instead, I fixed it and spent the time sharing how to fix the problem.\nAdding WBEM to the Windows path Adding the WMIC command to the Windows path is a very simple process (administrator privileges required) and is completed as follows:\nFind the Advanced System Settings item in your start menu search:\nView advanced system settings Open the Advanced tab and select the Environment Variables button:\nOn the advanced tab, open Environment Variables Under System variables, highlight the Path variable and select the edit button:\nSelect the Path system variable and Edit… Now we need to added the following line to our Path variable:\nC:\\Windows\\System32\\wbem\\ We can do that as follows, ensuring to include the trailing backslash as this is a folder:\nOnce you have clicked OK, closing all the windows we have opened so far, you need to REBOOT your computer to apply the change.\nOnce you have completed the reboot, open up the command prompt, and test the WMIC command:\nLearn more about this WMIC command from this link.\n","permalink":"https://exitcode0.net/posts/adding-wmic-command-to-the-windows-path/","summary":"wmic is not recognized as an internal or external command – I was quite shocked to find that a command I use on a very regular basis was not working on a fresh installation of Windows 10 1909 on my trust old ThinkPad.\nI don’t want to spend hours trying to find out why this was not correct in my system path, but instead, I fixed it and spent the time sharing how to fix the problem.","title":"Adding WMIC command to the Windows path"},{"content":"The Pi enthusiasts have been waiting for official USB boot support on the Raspberry Pi for what feels like a lifetime, but finally it is on the horizon. In this post I will explain how to make your Raspberry Pi boot from USB.\nWARNING: Although this is official, it is still in beta testing, so rock-solid stability is far from certain. Learn more about this beta release here.\nWhy should I make my Raspberry Pi boot from USB? Firstly let me address why you would want to do this and what makes many people relieved that the feature is on the horizon.\nA more robust, long-lasting boot device. Micro SD cards, particularly high-quality cards, have come a long way and the days of a dead micro SD card seem to be dwindling. However, the small flash cards have a much shorter read/write life expectancy, and are not really designed to be used as a bootable OS media. Being able to use a SATA (over USB) device which was designed for the typical IO expected from an active operating system will ensure a long-lasting Pi setup.\n32GB Sandisk Card Price over time – Camel Camel Camel SATA based storage is cheaper per GB. Prices of micro SD cards have dropped significantly over the years, but SATA based storage is till better value for money at larger capacities, as well as the reliability benefits.\nHow do I make my Raspberry Pi boot from USB? Difficulty: Medium\nThere are a number of steps involved in this process and it will require you to bounce some files between devices that can be done on the Pi or by connecting the two bootable devices to another computer – this method won’t be covered in this guide.\nRequirements: Raspberry Pi https://amzn.to/3eiF2XT Pi 4 Starter kit The latest version of Raspbian Buster https://www.raspberrypi.org/downloads/raspbian/ USB hard drive/flash drive https://amzn.to/2Twk4wm 256GB Sandisk USB drive. Micro SD Card (still required at this point in BETA) https://amzn.to/3ggknoX 32GB SanDisk Micro SD card Burn the Rasbian ISO to your SD card using something like Etcher: https://www.balena.io/etcher/ and plug it into the Pi.\nProcess (as of 23-05-2020): The breakdown of the process:\nChange the content of the EEPROM on the Raspberry Pi4 and set the boot device to USB. Copy files from the boot directory of the SD card to the boot directory of the USB boot device. We must start by updating the Raspbian installation:\nsudo apt update -y \u0026amp;\u0026amp; sudo apt upgrade -y \u0026amp;\u0026amp; sudo rpi-update -y Now that we are up to date, we must reboot before installing rpi-eeprom:\nsudo reboot now sudo apt install rpi-eeprom Now we have to change the path of the Pi’s EEPROM firmware- this is done by moving from the critical channel to beta channel:\nsudo nano /etc/default/rpi-eeprom-update Replace ‘critical‘ with ‘beta‘ followed by ‘ctrl+x, y‘ to save and exit.\nNow we can program the EEPROM as follows:\nsudo rpi-eeprom-update -d -f /lib/firmware/raspberrypi/bootloader/beta/pieeprom-2020-05-15.bin Followed by a reboot:\nsudo reboot now Once the reboot is complete, we can check the bootloader version as follows:\nvcgencmd bootloader_version If we run the follow…\nvcgencmd bootloader_config … we should see ‘BOOTORDER=0xF41. 4‘\n‘.4’ means that USB boot is active; ‘.1’ means that SD boot is active.\nAt this point the Pi is ready, but the USB media is not. All that is left is to prepare the USB bootable media with the necessary files form our SD card installation.\nStart by plugging in the USB getting rasbian flashed to your USB device in the same manner as the SD card – personally I use Etchor: https://www.balena.io/etcher/\nNow we need to copy all ‘.elf’ and ‘.dat’ files from the boot directory of our SD card to the boot directory on the USB. Let’s start by mounting the USB drive.\nsudo mkdir /mnt/usbdisk sudo mount /dev/sd1 /mnt/usbdisk Now that the USB drive is mounted, we can copy the required files:\nsudo cp /boot/*.elf /mnt/usbdisk sudo cp /boot/*.dat /mnt/usbdisk We are done!\nNow we can now power down (sudo shutdown now), remove the SD card, then power back up and check that we have successfully booted from out USB!\nMore Raspberry Pi Antics: [SOLVED] – RUNNING DOCKER ON A RASPBERRY PI 4 – https://exitcode0.net/posts/solved-running-docker-on-a-raspberry-pi-4/ USING A DHT11 SENSOR WITH A RASPBERRY PI – https://exitcode0.net/using-a-dht11-sensor-with-a-raspberry-pi/ CREATING A LOCAL DNS SERVER WITH PI HOLE – https://exitcode0.net/creating-a-local-dns-server-with-pi-hole/ ","permalink":"https://exitcode0.net/posts/how-to-raspberry-pi-boot-from-usb/","summary":"The Pi enthusiasts have been waiting for official USB boot support on the Raspberry Pi for what feels like a lifetime, but finally it is on the horizon. In this post I will explain how to make your Raspberry Pi boot from USB.\nWARNING: Although this is official, it is still in beta testing, so rock-solid stability is far from certain. Learn more about this beta release here.\nWhy should I make my Raspberry Pi boot from USB?","title":"How To: Raspberry Pi boot from USB"},{"content":" I’ve spent enough time building VMs in vSphere to know that the first few minutes between starting the VM and getting VMware Tools installed is agonising. Similar to my ‘Copy Paste With Powershell Sendkeys‘ script, I have another tool written in AutoIT which sends defined keyboard strokes to the system. So this is not exactly ‘copy paste in vSphere’, but it achieves the same goal.\nIf you actually want to enable clipboard sync between host and VM, you can follow this guide here: Enable Copy and Paste Operations Between the Guest Operating System and Remote Console. This is going to need VMware Tools installed on the host though.\nCopy Paste in vSphere with AutoIT Yes this tool is written in AutoIT, apparently my favorite script kiddie language from the 201X years, so Windows only on this one – sorry. Quite a simple bit of code, we prompt for the text we want to put on the ‘clipboard’ announce the waiting period before sending the keystrokes (allowing the user time to place the cursor in the VM) and then fire off the keystrokes.\n#include \u0026lt;MsgBoxConstants.au3\u0026gt; Local $sAnswer: InputBox(\u0026#34;Send Keys\u0026#34;, \u0026#34;Enter the text you want to send:\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, _ - 1, -1, 0, 0) Local $iTimeout: 7 Local $iSecUntil: 10 MsgBox($MB_SYSTEMMODAL, \u0026#34;Send Keys\u0026#34;, \u0026#34;Your keys will be sent in \u0026#34; \u0026amp; $iSecUntil \u0026amp; \u0026#34; seconds or select the OK button to start a 3 second countdown.\u0026#34;, $iTimeout) Sleep(3000) Send($sAnswer) Exit Code Enhancements? This tool has got me out of a number of binds where the clipboard has failed me – usually some form of remote session. I don’t trust myself to type things accurately in high-pressure situations, especially when character obfuscation is in play.\nAt the moment this is a one-shot executable; you run it once and then the process terminates. One handy feature/enhancement would be to have it run perpetually and listen for a hotkey (definable) on the host triggering the main function of the code to be called.\nThat being said, there are lots of other things I am working on right now, and digging back into an AutoIT project from 2016 is not very high on the priorities list. Perhaps one for a rainy day…\nMore AutoIT ramblings: Automatically pause Spotify with the 3CX client – https://exitcode0.net/posts/automatically-pause-spotify-with-the-3cx-client/ ","permalink":"https://exitcode0.net/posts/copy-paste-in-vsphere-before-installing-vmware-tools/","summary":"I’ve spent enough time building VMs in vSphere to know that the first few minutes between starting the VM and getting VMware Tools installed is agonising. Similar to my ‘Copy Paste With Powershell Sendkeys‘ script, I have another tool written in AutoIT which sends defined keyboard strokes to the system. So this is not exactly ‘copy paste in vSphere’, but it achieves the same goal.\nIf you actually want to enable clipboard sync between host and VM, you can follow this guide here: Enable Copy and Paste Operations Between the Guest Operating System and Remote Console.","title":"Copy Paste in vSphere before installing VMware tools"},{"content":"This is a really old script that I just found in my archive, but it still works all the same. The goal was to write a small script that would automatically pause Spotify with the 3CX client when there was an inbound call.\nThe script is written in AutoIT, meaning that it is a Windows-only solution. The Windows 3CX CTI client allows you to run a particular executable when an inbound call is received, i.e. when the phone rings. I wanted to pause the manic euro beats so that I could answer my phone and here the caller and uphold professionalism.\nThe AutoIT Code ; ***************************************************************************** ; Auto Pause Spotify ; Pauses spotify when called ; Author: Tom Cocking - www.tomcocking.com ; ***************************************************************************** #include \u0026lt;MsgBoxConstants.au3\u0026gt; SpotifyPause() Func SpotifyPause() ; Retrieve the handle of the Spotify window using the classname of SpotifyMainWindow. Local $hWnd: WinGetHandle(\u0026#34;[CLASS:SpotifyMainWindow]\u0026#34;) WinActivate($hWnd) Local $sText: WinGetTitle(\u0026#34;[ACTIVE]\u0026#34;) If $sText: \u0026#34;Spotify\u0026#34; Then Exit Else Send(\u0026#34;{SPACE}\u0026#34;) Exit EndIf If @error Then ConsoleWrite(\u0026#34;An error occurred when trying to retrieve the window handle of Spotify\u0026#34;) Exit EndIf EndFunc As you can see, the code is very straight forward and to the point; we are asking 3CX to run the compiled exe file every time there is an inbound call, fast and reliable code is essential. It is also really important that you conclude you executable with an exit statement to prevent stale processes lingering on your system.\nOne subtle thing that this script does is check if the Spotify instance has a song/artist data in the window title (which would signal that music was playing) before trying to activate the window and pause playback. One thing to mention – it would be a lot easier if there was a global hotkey to play/pause music, particularly one that Spotify adhered to. This would make this script 3 lines long (not counting comments).\nEditors Note: I fully intend on uploading this to a public Github repository so that you can clone the repo, rather than just copying/pasting.\nConfigure the 3CX softphone client Once you have compiled your script into an executable file, you can find the setting in the 3CX client to run an exe upon an incoming call. So when you have a call from the boss, you can rest easy knowing that your secret adoration for Taylor Swift remains undisclosed.\nplease recognise my sarcasm\nYou can find more useful AutoIT snippets over on my Code Samples Page.\n","permalink":"https://exitcode0.net/posts/automatically-pause-spotify-with-the-3cx-client/","summary":"This is a really old script that I just found in my archive, but it still works all the same. The goal was to write a small script that would automatically pause Spotify with the 3CX client when there was an inbound call.\nThe script is written in AutoIT, meaning that it is a Windows-only solution. The Windows 3CX CTI client allows you to run a particular executable when an inbound call is received, i.","title":"Automatically pause Spotify with the 3CX client"},{"content":"I have a number of projects which I am currently working on which usually involve a raspberry pi and so other TCP enabled object. One project, in particular, is to control a KVM server with buttons connected to a raspberry pi (follow on social media or RSS for that future post). I have been using Python 3 SSH with Paramiko to send commands KVM server from the pi. Here is how I do that…\nWhat is Paramiko? Paramiko is a Python (2.7, 3.4+) implementation of the SSHv2 protocol, providing both client and server functionality. While it leverages a Python C extension for low level cryptography, Paramiko itself is a pure Python interface around SSH networking concepts. – http://www.paramiko.org/\nTLDR – Paramiko is a python library which lets you do SSH things.\nThe how-to: Python 3 SSH with Paramiko For the purpose of this blog, I will be focusing on using paramiko to make client connections. Let’s include this library and dig into sending some dangerous commands to Linux boxes.\nimport paramiko I have been using a function to complete my SSH command and passing my SSH variables when calling that function. Lets start with that funciton:\ndef func_do_ssh_Stuff(address, usr, pwd, command): try: print(\u0026#34;ssh \u0026#34; + usr + \u0026#34;@\u0026#34; + address + \u0026#34;, running : \u0026#34; + command) client: paramiko.SSHClient() client.load_system_host_keys() # this loads any local ssh keys client.set_missing_host_key_policy(paramiko.AutoAddPolicy()) client.connect(address, username=usr, password=pwd) _, ss_stdout, ss_stderr: client.exec_command(command) r_out, r_err: ss_stdout.readlines(), ss_stderr.read() print(r_err) if len(r_err) \u0026gt; 5: print(r_err) else: print(r_out) client.close() except IOError: print(\u0026#34;.. host \u0026#34; + address + \u0026#34; is not up\u0026#34;) return \u0026#34;host not up\u0026#34;, \u0026#34;host not up\u0026#34; What’s going on here? The function is expecting to be handed a username, password (or none if you plan to use key based authentication (recommended)), host address and the shell command to run once the connection is established.\nParamiko checks for local keys on the system which it can use when connecting to the host, then attempts a connection. Once the connection is established the command we passed to the function will be executed.\nFinally, we print the output or error returned by the execution of that command. This is all encased in a ‘try, catch’ which will inform us if the SSH target is offline.\nCalling the Paramiko SSH function Now that we have our SSH function declared, we can call it:\nfunc_do_ssh_Stuff(\u0026#34;192.168.0.33\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;echo Hello World!\u0026#34;) Note: you will need to change the IP to suit and add a password (“”) if required.\nI would highly recommend using key-based authentication on this one, otherwise, it will require you to hard code SSH passwords into your scripts… not a great idea.\nAnd now that you can SSH from a Pi (or anything else which runs python), the world is your oyster!\nMore wonderful Python things: How To: Python infinite loops with while true – https://exitcode0.net/posts/how-to-python-infinite-loops-with-while-true/ Kasa home automation with IFTTT and python webooks – https://exitcode0.net/kasa-home-automation-with-ifttt-and-python-webooks/ Getting started with Python 3 – a beginner’s cheat sheet – https://exitcode0.net/getting-started-with-python-3-a-beginners-cheat-sheet/ ","permalink":"https://exitcode0.net/posts/python-3-ssh-with-paramiko/","summary":"I have a number of projects which I am currently working on which usually involve a raspberry pi and so other TCP enabled object. One project, in particular, is to control a KVM server with buttons connected to a raspberry pi (follow on social media or RSS for that future post). I have been using Python 3 SSH with Paramiko to send commands KVM server from the pi. Here is how I do that…","title":"Python 3 SSH with Paramiko"},{"content":"Here is a quick guide on how to create an infinite loop in python using a ‘while true’ statement. There are number of reason that you might want to implement this; a great use case would be outputting a fluctuating variable to the terminal such as a temperature reading from a sensor.\nloops that make your brain hurt A ‘while true’ statement allows us to run a sequence of code until a particular condition is met. In order to make that sequence of code run in an infinite loop, we can set the condition to be one that is impossible to reach. Better still, we can simply omit the condition altogether to ensure that the while true loop never ends.\nwhile True: print(\u0026#34;The current time is: %s\u0026#34; % strTimeNow) time.sleep(5) In cases where it would be useful to exit that loop if a given condition is met or exception is reached, we can encase our ‘while true’ statement with a ‘try except’ statement.\ntry: while True: print(\u0026#34;The current time is: %s\u0026#34; % strTimeNow) time.sleep(5) except KeyboardInterrupt: print(\u0026#34;Hard Exit Initiated. Goodbye!\u0026#34;) There are a number of exception that can be handled by a try statement, I have chose to use the KeyboardInterupt (Ctrl+C) clause in this example.\nYou can learn more about exception handling here: https://docs.python.org/3/tutorial/errors.html#handling-exceptions\nMore Python Tips and tricks: More Python code samples and examples – https://exitcode0.net/code-samples/#python Kasa home automation with IFTTT and python webooks – https://exitcode0.net/kasa-home-automation-with-ifttt-and-python-webooks/ Getting started with Python 3 – a beginner’s cheat sheet – https://exitcode0.net/getting-started-with-python-3-a-beginners-cheat-sheet/ ","permalink":"https://exitcode0.net/posts/how-to-python-infinite-loops-with-while-true/","summary":"Here is a quick guide on how to create an infinite loop in python using a ‘while true’ statement. There are number of reason that you might want to implement this; a great use case would be outputting a fluctuating variable to the terminal such as a temperature reading from a sensor.\nloops that make your brain hurt A ‘while true’ statement allows us to run a sequence of code until a particular condition is met.","title":"How To: Python infinite loops with while true"},{"content":" note to self: never use LVM again… This title has far too many acronyms so I will start by clarifying the problem that this post aims to solve. I have a Ubuntu 18.04 server running LVM (Logical Volume Manager). My primary LV (Logical Volume) was ‘full’ but was only using 4GB of a 29GB PV (Physical Volume). So if you are suffering the same issue, here are the commands you need to fix this problem.\ndf -h output: Filesystem 1K-blocks Used Available Use% Mounted on /dev/mapper/i-hate-lvm 5166504 5150120 0 100% / df -h shows the space availble in mounted partitions This is fictitious data, but this is reflective of the fact that the root filesystem is full. This caused me a LOT of headaches and I needed to free up some free space on that partition before I could run any of the upcoming commands to expand the LV. Ways to free up space include:\nsudo apt clean sudo apt autoremove -y I was running snap and use the following command to remove older versions of a snap application:\nsnap remove \u0026#34;$snapname\u0026#34; --revision=\u0026#34;$revision\u0026#34; Finding the available PV space Now that we have a tiny bit of space on ‘/’ we cab start to work on expanding the available space in the LV. First, we need to find out the available space in the PV which is where the LV ‘lives’.\nvgdisplay the value we are looking for specifically is:\nFree PE / Size 2319 / 9.06 GiB As per the example above, we have just over 2GB available. I want to go head and give all of that space to my one and only LV.\nExtending LV with lvextend so finally, the command you were looking for:\nsudo lvextend -l +2319 /dev/i-hate-lvm Adjust the ‘2319’ value and the ‘/dev/’ path to suit your required level of expansion and the LVM volume.\nExtending an LV filesystem with resize2fs Finally, now that the LV is larger, we need to expand the filesystem on the volume to make the newly acquired space available for use. Fortunately this another really simple command:\nresize2fs /dev/mapper/i-hate-lvm Pay attention to the fact that I am running this command against ‘/dev/mapper/‘.\nRun the ‘df -h’ command and again and you should now have ALL OF THE FREE SPACE!\nThis was a massive PITA for me, an inexperienced LVM user. Consider giving this post a bookmark or a share to help out a fellow LVM sufferer in a moment of need.\nMore useful Ubuntu rangling posts: How to upgrade Ubuntu from 18.04 to 20.04 – https://exitcode0.net/how-to-upgrade-ubuntu-from-18-04-to-20-04/ Setup the Nextcloud Snap Package with HTTPS and a separate data path – https://exitcode0.net/setup-the-nextcloud-snap-package-with-https-and-a-separate-data-path/ ","permalink":"https://exitcode0.net/posts/how-to-expand-an-lvm-lv-to-use-all-space-on-the-pv/","summary":"note to self: never use LVM again… This title has far too many acronyms so I will start by clarifying the problem that this post aims to solve. I have a Ubuntu 18.04 server running LVM (Logical Volume Manager). My primary LV (Logical Volume) was ‘full’ but was only using 4GB of a 29GB PV (Physical Volume). So if you are suffering the same issue, here are the commands you need to fix this problem.","title":"How to expand an LVM LV to use all space on the PV"},{"content":"Ubuntu 20.04 LTS full release is almost upon, with a scheduled release date of April 23rd 2020 (2 days as of writing this). It is packed with lots of great new features and upgrades and the Linux community is very excited to get a hold of it. If you are looking to upgrade ubuntu, there is a quick guide down below.\nIf like me you are already running Ubuntu 18.04 LTS then you might be interested to learn how to upgrade your current version of the distro to 20.04 LTS.\nFourtnately it is relatively simple and requires a few terminal commands.\nPrerequisites It is really important to take a backup of any important data and only consider the upgrade in the first place if you are happy that your current software suite is supported in the new version of the OS. If you are not sure about this then I would advise building 20.04 in a VM for testing before committing to the upgrade.\nStarting the upgrade It would be a good idea to make sure that all your current packages in Ubuntu 18.04 are up to date before jumping to 20.04:\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade \u0026amp;\u0026amp; apt-get dist-upgrade follow that up with a reboot:\nsudo reboot now Now we can install the Ubuntu release upgrade tool:\nsudo apt install update-manager-core Now it it time to upgrade to the latest LTS:\nsudo do-release-upgrade The uprade will take quite a bit of time and require some minor intervention for confirmation during the upgrade. Once it is complete, reboot your system and move onto the final stage.\nIt is important to note that if you are trying to upgrade to 20.04 before the final release date, you will get the following message and tip:\nChecking for a new Ubuntu release There is no development version of an LTS available. To upgrade to the latest non-LTS develoment release set Prompt=normal in /etc/update-manager/release-upgrades Changing ‘Prompt’ to normal from LTS will allow your system to jump to developer builds when running the ‘do-release-upgrade’. Once again, if this is a mission-critical system, consider your decision to upgrade to development versions of your OS carefully.\nIf you play fast and loose, you can always ugrade to a developer version:\nsudo do-release-upgrade -d Final step Now it is time to verify the version of Ubuntu you are running:\nlsb_release -a If you want to share your version with the world, consider using neofetch as a way to make your terminal flex that bit more exciting:\nsudo apt install neofetch neofetch Feel free to post your neofect flex in the comments below!\nMore useful Ubuntu tips and tricks Ubuntu 19.10 – How to upgrade python 2.7 to python 3.7 – [http://Ubuntu 19.10 – How to upgrade python 2.7 to python 3.7](http://Ubuntu 19.10 – How to upgrade python 2.7 to python 3.7 \u0026ldquo;http://Ubuntu 19.10 – How to upgrade python 2.7 to python 3.7\u0026rdquo;)\n(this will probably work on 20.04 too!) ","permalink":"https://exitcode0.net/posts/how-to-upgrade-ubuntu-from-18-04-to-20-04/","summary":"Ubuntu 20.04 LTS full release is almost upon, with a scheduled release date of April 23rd 2020 (2 days as of writing this). It is packed with lots of great new features and upgrades and the Linux community is very excited to get a hold of it. If you are looking to upgrade ubuntu, there is a quick guide down below.\nIf like me you are already running Ubuntu 18.04 LTS then you might be interested to learn how to upgrade your current version of the distro to 20.","title":"How to upgrade Ubuntu from 18.04 to 20.04"},{"content":" https://snapcraft.io/nextcloud I have been using the Nextcloud snap package for a long time, but some recent system changes in my colocation facility mean that it is time for a big upgrade. Rather than migrate my existing Nextcloud instance, I have decided to build a new install and here are my goals for the new installation:\nUse lets encrypt to have automatic and valid HTTPS certificates Have the data path for Nextcloud on a HDD and the rest of the host OS on an SSD. Host OS Details For this setup, I am using Ubuntu 18.04 installed on a 30GB SSD backed partition. I then have a 1TB disk attached and mounted to /media.\nStep 1 – How to install Nextcloud Snap Package Snap is simple and effective, this one command gives you a basic install of Nextcloud (assuming that you have your DNS records and firewall rules in place at this point):\nsudo snap install nextcloud Do not visit the new site in your browser yet!\nStep 2 – Change the data directory to use another disk partition You must make sure that your secondary drive is mounted in /media or /mnt – as described in the Nextcloud snap wiki: https://github.com/nextcloud/nextcloud-snap/blob/master/README.md\nWhen you have you drive mounted, you will need to allow the snap instance to access ‘removable media’:\nsudo snap connect nextcloud:removable-media sudo chown -R root:root /media/\u0026lt;my mount point\u0026gt; Now we need to edit the Nextcloud config to match this data directory:\nsudo nano /var/snap/nextcloud/current/nextcloud/config/autoconfig.php Here we must change the following line to reflect our data path:\n// ... \u0026#39;directory\u0026#39; =\u0026gt; \u0026#39;/media/\u0026lt;my mount point\u0026gt;\u0026#39;, // ... Once you have exited and saves your changes to the config file, it is time to restart the service:\nsudo snap restart nextcloud.php-fpm Now it is safe to visit your Nextcloud URL!\nAt this point, you will be visiting a HTTP page and will be asked to create your default admin Nextcloud user. Feel free to do this.\nStep 3 – Enabling Lets Encrypt HTTPS for the Nextcloud Snap Package We now have a function Nextcloud server, but it is not secure. We need to enable HTTPS and generate a certificate with Lets Encrypt. Again, assuming that your DNS records and firewall rules are in order:\nsudo nextcloud.enable-https lets-encrypt Complete the steps by entering your recovery email address and the domain of the Nextcloud server and the tool will generate and apply your certificate. You only need to this once; Lets Encrypt handles auto-renewal.\nAt this point, you can refresh your browser and you will be auto-forward to the HTTPS version of your new Nextcloud sever.\nOther useful Nextcloud tutorials: Backup Nextcloud with RCLONE – https://exitcode0.net/posts/backup-nextcloud-with-rclone/ ","permalink":"https://exitcode0.net/posts/setup-the-nextcloud-snap-package-with-https-and-a-separate-data-path/","summary":"https://snapcraft.io/nextcloud I have been using the Nextcloud snap package for a long time, but some recent system changes in my colocation facility mean that it is time for a big upgrade. Rather than migrate my existing Nextcloud instance, I have decided to build a new install and here are my goals for the new installation:\nUse lets encrypt to have automatic and valid HTTPS certificates Have the data path for Nextcloud on a HDD and the rest of the host OS on an SSD.","title":"Setup the Nextcloud Snap Package with HTTPS and a separate data path"},{"content":"There might be a number of reasons to want to prevent your windows screensavers and lock-screens from engaging and in some cases (no doubt yours if you have hit this article from a search engine) local policy on the machine preventing you from changing these settings.\nPowershell lets us work around this problem and prevent the machine from locking or activating a screensaver.\nPrevent Screensavers and Lock-screens The bulk of the code below has been lifted from this great write up: https://dmitrysotnikov.wordpress.com/2009/06/29/prevent-desktop-lock-or-screensaver-with-powershell/ I just took things one step further by adding an infinite loop. My version of this code will run indefinitely.\nAffectionately named: PScaffeine The following code sends a relatively harmless (for most cases – change where appropriate) key to the system emulating a keypress on a human interface device:\n#powershell caffeine param($minutes: 29) $myshell: New-Object -com \u0026#34;WScript.Shell\u0026#34; Write-Host \u0026#34;Pouring out fresh coffee...\u0026#34; while($true){ for ($i: 0; $i -lt $minutes; $i++) { Start-Sleep -s 60 $myshell.sendkeys(\u0026#39;{NUMLOCK}\u0026#39;) $myshell.sendkeys(\u0026#39;{NUMLOCK}\u0026#39;) } Write-Host \u0026#34;*Takes another sip*\u0026#34; } Write-Host \u0026#34;Caffeine has worn off...\u0026#34; Change the $minutes parameter to 1 minute less than your systems lock/screensaver start initiation time.\nCombine this with a scheduled task if you want this to run on startup or system login.\nMore useful code snippets ‘Copy, Paste’ With Powershell Sendkeys – https://exitcode0.net/posts/copy-paste-with-powershell-sendkeys/ Powershell – Checking the Language Mode – https://exitcode0.net/posts/powershell-checking-the-language-mode/ Modifying Windows shortcuts is Powershell – https://exitcode0.net/modifying-windows-shortcuts-is-powershell/ All other codes samples – https://exitcode0.net/code-samples/ ","permalink":"https://exitcode0.net/posts/prevent-screensavers-and-lock-screens-with-powershell/","summary":"There might be a number of reasons to want to prevent your windows screensavers and lock-screens from engaging and in some cases (no doubt yours if you have hit this article from a search engine) local policy on the machine preventing you from changing these settings.\nPowershell lets us work around this problem and prevent the machine from locking or activating a screensaver.\nPrevent Screensavers and Lock-screens The bulk of the code below has been lifted from this great write up: https://dmitrysotnikov.","title":"Prevent Screensavers and Lock-screens with Powershell"},{"content":"I’ve made previous posts about backing cloud services with RCLONE – Backup Google Photos with Rclone – and this post will detail how to backup a self-hosted cloud service, Nextcloud.\nGet started with Nextcloud There are a lot of benefits to running your own private ‘cloud’ storage system, but for me, my top concern is privacy and Nextcloud gives me the ability to take accountability for my own data. With this in mind, I am conscious of my Nextcloud server going offline or suffering data loss, so I want a way to create 1 or more backups of my datacentre hosted, Nextcloud data on my home NAS.\nMy home NAS is running Linux, UNRAID to be precise. It would be possible to run the Nextcloud client on a windows or linux desktop to sync all of the files to a machine. However, RCLONE allows you to copy the data from Nextcloud over a webdav connection.\nHow to configure RCLONE Nextcloud backups Let’s start by running the RCLONE configuration tool:\nrclone config Then use the following options:\nn) New remote -- name: MyNextcloudBackup -- type: webdav -- url: https://\u0026lt;your nextcloud server url\u0026gt;/remote.php/webdav/ -- vendor: nextcloud -- user: \u0026lt;your nextcloud username\u0026gt; -- pass: \u0026lt;your nextcloud user password\u0026gt; -- bearer_token: Remote config This will encrypt the password so that it is not stored in plaintext.\nNow we can run the copy to the UNRAID server:\nrclone copy -P MyNextcloudBackup:\u0026lt;folder name\u0026gt; /mnt/user/\u0026lt;share and path on your UNRAID server\u0026gt; If you leave blank then it will download all folder in your nextcloud account.\n-P Will show you the progress of the copy job.\nYou can take this one step further and run this on a schedule using the user scripts plugin: https://forums.unraid.net/topic/48286-plugin-ca-user-scripts/\nOther useful posts: Backup Google Photos with Rclone – https://exitcode0.net/posts/backup-google-photos-with-rclone/ Kali Linux – How to upgrade python 2.7 to python 3.7 – https://exitcode0.net/posts/kali-linux-how-to-upgrade-python-2-7-to-python-3-7/ Switching to a Linux laptop – https://exitcode0.net/switching-to-a-linux-laptop/ I’m working hard to bring as much useful content to this blog as I can. You can support this site by sharing pages or posts and I will be forever indebted to those who do!\n","permalink":"https://exitcode0.net/posts/backup-nextcloud-with-rclone/","summary":"I’ve made previous posts about backing cloud services with RCLONE – Backup Google Photos with Rclone – and this post will detail how to backup a self-hosted cloud service, Nextcloud.\nGet started with Nextcloud There are a lot of benefits to running your own private ‘cloud’ storage system, but for me, my top concern is privacy and Nextcloud gives me the ability to take accountability for my own data. With this in mind, I am conscious of my Nextcloud server going offline or suffering data loss, so I want a way to create 1 or more backups of my datacentre hosted, Nextcloud data on my home NAS.","title":"Backup Nextcloud with RCLONE"},{"content":" Here’s a quick guide on how to convert ESXi VMs to and UNRAID KVM virtual machine – it’s a simple process but a minefield to navigate on searches and forums.\nI currently have a windows 10 VM running on an ESXi server (vmdk) which I want to migrate to my UNRAID server which uses KVM to host VMs. The vmdk file is unfortunately thick provisioned, so I have had to do some disk juggling, due to my limited amount of available of space on my array.\nStep 1 – Copy the VMDK files to the UNRAID server There are a number of ways to do this, but I chose to mount an UNRAID share as an NFS datastore in ESXi and copy the VM folder across (note: copy not move). Remember to power off the VM before doing this. If you need help mounting NFS shares, just leave a comment below.\nStep 2 – Convert ESXi vmdk file to an img file using ‘qemu-img convert’ Now that we have a copy of the vmdk files on our UNRAID server, we can use the qemu-img tool to convert the files to a format that UNRAID can use to boot the VM – a .img file. Get console access to your UNRAID server and use the following command format:\nqemu-img convert -p -f vmdk -O raw /mnt/user/\u0026lt;the location of your vmdk file\u0026gt; /mnt/user/\u0026lt;the location of your new file\u0026gt;.img -p shows you the progress of the conversion. The larger the vmdk, the longer it will take -f the input file type -O the output file type /mnt/user/ is where UNRAID stores its shares. My VM was thick provisioned, so my output file was the same size as the vmdk. If your vmdk is thin provisioned, this process will swell the .img file – effectively making it thick provisioned – so make sure that you have enough available disk space for the conversion. I didn’t have this space available so I have to use the unassigned devices plugin and mount a USB hard drive to complete the conversion. Unassigned drives are mounted in /mnt/disks/.\nStep 3 – Create a KVM VM with the converted .img file Now we can create a VM using our newly created .img file; here are the settings I used for my converted Windows VM:\nThe CPU, memory, graphics card and other passthrough devices have no impact on the machine booting up. The VM will likely need to reconfigure itself to use the new hardware setup and you might need to install the virtIO network drivers.\nIf you are having problems, then it is best to keep the VM config simple and add devices as you solve problems.\nPhysical machine to UNRAID VM? Much of the same principals of this guide can be applied to moving a physical windows machine over to an UNRAID, KVM virtual machine. I will be covering this in future posts – using Clonezilla to build a .img file of a physical Windows machine.\n","permalink":"https://exitcode0.net/posts/convert-an-esxi-vm-to-unraid-kvm-with-qume-img-convert/","summary":"Here’s a quick guide on how to convert ESXi VMs to and UNRAID KVM virtual machine – it’s a simple process but a minefield to navigate on searches and forums.\nI currently have a windows 10 VM running on an ESXi server (vmdk) which I want to migrate to my UNRAID server which uses KVM to host VMs. The vmdk file is unfortunately thick provisioned, so I have had to do some disk juggling, due to my limited amount of available of space on my array.","title":"Convert an ESXi VM to UNRAID KVM with qume-img convert"},{"content":"Google photos auto backup is a nice way to make sure that all your photos on your mobile phone are backed up. It takes the pain out of backups by allowing you to automatically upload your photos and videos to Google servers. For those who would also like an offline copy of those photos, it is now easy to automate this. Here is how to backup your Google Photos library to your local computer or NAS with Rclone.\nhttps://rclone.org/ Rclone is a command line program to sync files and directories to and from:\nGoogle Drive / Photos Dropbox Onedrive and many more services Rclone can actually copy data to these cloud services as well, but this particular article focuses on how to take your data back from the cloud – because I like to own my data.\nPrerequisites This is based on the assumption that:\nyou are running Linux, mac OS or have a docker environment have Rclone installed – Install guide: https://rclone.org/install/ Backup Google Photos with Rclone First, we must make a remote endpoint – Google Photos. So jump into Rclone config to do so:\nrclone config No remotes found - make a new one n) New remote s) Set configuration password q) Quit config n/s/q\u0026gt; n name\u0026gt; remote Type of storage to configure. Enter a string value. Press Enter for the default (\u0026#34;\u0026#34;). Choose a number from below, or type in your own value [snip] XX / Google Photos \\ \u0026#34;google photos\u0026#34; [snip] Storage\u0026gt; google photos ** See help for google photos backend at: https://rclone.org/googlephotos/ ** Google Application Client Id Leave blank normally. Enter a string value. Press Enter for the default (\u0026#34;\u0026#34;). client_id\u0026gt; Google Application Client Secret Leave blank normally. Enter a string value. Press Enter for the default (\u0026#34;\u0026#34;). client_secret\u0026gt; Set to make the Google Photos backend read only. If you choose read only then rclone will only request read only access to your photos, otherwise rclone will request full access. Enter a boolean value (true or false). Press Enter for the default (\u0026#34;false\u0026#34;). read_only\u0026gt; Edit advanced config? (y/n) y) Yes n) No y/n\u0026gt; n Remote config Use auto config? * Say Y if not sure * Say N if you are working on a remote or headless machine y) Yes n) No y/n\u0026gt; y If your browser doesn\u0026#39;t open automatically go to the following link: http://127.0.0.1:53682/auth Log in and authorize rclone for access Waiting for code... Got code *** IMPORTANT: All media items uploaded to Google Photos with rclone *** are stored in full resolution at original quality. These uploads *** will count towards storage in your Google Account. -------------------- [remote] type: google photos token: {\u0026#34;access_token\u0026#34;:\u0026#34;XXX\u0026#34;,\u0026#34;token_type\u0026#34;:\u0026#34;Bearer\u0026#34;,\u0026#34;refresh_token\u0026#34;:\u0026#34;XXX\u0026#34;,\u0026#34;expiry\u0026#34;:\u0026#34;2019-06-28T17:38:04.644930156+01:00\u0026#34;} -------------------- y) Yes this is OK e) Edit this remote d) Delete this remote y/e/d\u0026gt; y I was running this config script over a remote SSH session so actually chose not to use auto config. The only difference is the URL you are given sends you to a Google auth page to manually retrieve your session auth code.\nNow that you have a remote endpoint setup, you can reference it in Rclone commands. This command is a simple way to take photos from my Google account and put them into Folders sorted by Year \u0026gt; Month:\nrclone copy GooglePhotos:media/by-month /mnt/user/PhotosBackup adding –progress to the end of that will give you a visual reference to the progress of the copy action.\nNow what? From here you can set this command to run on a schedule using cron if you are using Linux:\ncrontab -e A great resource for building cron tab commands: http://www.cronmaker.com/\n","permalink":"https://exitcode0.net/posts/backup-google-photos-with-rclone/","summary":"Google photos auto backup is a nice way to make sure that all your photos on your mobile phone are backed up. It takes the pain out of backups by allowing you to automatically upload your photos and videos to Google servers. For those who would also like an offline copy of those photos, it is now easy to automate this. Here is how to backup your Google Photos library to your local computer or NAS with Rclone.","title":"Backup Google Photos with Rclone"},{"content":" Rejoice! Kali Linux 2020.1 is here! The first release of 2020 has arrived and this post will contain some updates and my initial thoughts on the fresh release.\nKali Linux 2020.1 features and changes Non-Root by default Kali single installer image Kali NetHunter Rootless Improvements to theme \u0026amp; kali-undercover New tools Full Kali release notes as per their release blog post: https://www.kali.org/releases/kali-linux-2020-1-release/\nGot root? Kali has installed with root as the default user since the dawn of its existence, Backtrack, WHAX and Whoppix, but finally, the devs have decided that it time to bring Kali in line with Debian on this one and have a default user which is no longer root.\nMany might see this as an unnecessary move as the OS is not intended or condoned for use as a daily driver (even 2020.1) – although some people ignore this anyway so not your browser as root is always a good thing.\nWhile we don’t encourage people to run Kali as their day to day operating system, over the last few years more and more users have started to do so (even if they are not using it to do penetration testing full time), including some members of the Kali development team. When people do so, they obviously don’t run as default root user. With this usage over time, there is the obvious conclusion that default root user is no longer necessary and Kali will be better off moving to a more traditional security model. –\nhttps://www.kali.org/news/kali-default-non-root-user/\nThe maintainers have discovered that a large number of tools no longer require root (with the exception of NMAP as an example) to run, so it made sense to align with a more traditional security model.\nFirst Impressions I grabbed the 64bit ISO and spun up a VM – the host is running KVM, but this makes no difference.\nShock horror no more root by default – text-based install method. Pro-tip for first time Kali users… don’t give a name by which you could be identified.\nI like that you can now select the metapackages which you install and the Desktop environment (unless you use the automated installer method). So you can make your installation as fat or lightweight as you need it.\nOnce you’ve completed your install, everything else feels more or less the same. The maintainers are committed to binning as many python2 dependant tools as they can as technically they are no longer supported. We have some new tools, which I excited to test at a given opportunity:\ncloud-enum emailharvester phpggc sherlock splinter. Upgrade from Kali 1904.1 or earlier If you already have an installation and you don’t want to nuke and pave, then it is also possible to upgrade.\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list deb http://http.kali.org/kali kali-rolling main non-free contrib EOF apt update -y \u0026amp;\u0026amp; apt full-upgrade -y Once that is complete, go for a quick reboot then check that you are now running the latest and greatest:\nlsb_release -a Other useful Kali posts: Kali Linux – How to upgrade python 2.7 to python 3.7 https://exitcode0.net/posts/kali-linux-how-to-upgrade-python-2-7-to-python-3-7/ How to make a VirtualBox VM the same resolution as host – https://exitcode0.net/how-to-make-a-virtualbox-vm-the-same-resolution-as-host/ Clipboard and Shared Folders on Kali Linux with VirtualBox – https://exitcode0.net/clipboard-and-shared-folders-on-kali-linux-with-virtualbox/ AWUS036AC (rt8812au) driver setup in Kali Linux – https://exitcode0.net/posts/awus036ac-rt8812au-driver-setup-in-kali-linux/ (pending validation with 2020.1) ","permalink":"https://exitcode0.net/posts/kali-linux-2020-1-no-more-root-by-default/","summary":"Rejoice! Kali Linux 2020.1 is here! The first release of 2020 has arrived and this post will contain some updates and my initial thoughts on the fresh release.\nKali Linux 2020.1 features and changes Non-Root by default Kali single installer image Kali NetHunter Rootless Improvements to theme \u0026amp; kali-undercover New tools Full Kali release notes as per their release blog post: https://www.kali.org/releases/kali-linux-2020-1-release/\nGot root? Kali has installed with root as the default user since the dawn of its existence, Backtrack, WHAX and Whoppix, but finally, the devs have decided that it time to bring Kali in line with Debian on this one and have a default user which is no longer root.","title":"Kali Linux 2020.1 - No more root by default"},{"content":" https://ddclient.net A quote short post on how to secure your DDNS updates with Namecheap, SSL and DDClient. For those of us who use dynamic DNS to work around roaming IP addresses, it is important to make sure that you are updating your DNS records securely with SSL.\nThe default DDclient config has an example configuration file for using Namecheap’s DDNS service, however, it does not use SSL to check for your IP. In theory that connection could be manipulated and a false IP result could be returned – updating your DNS records to a wrong, malicious IP could cause a number of problems.\nNamecheap SSL DDClient Config Example ssl=yes use=web, web=dynamicdns.park-your-domain.com/getip protocol=namecheap server=dynamicdns.park-your-domain.com login=\u0026lt;your domain goes here\u0026gt; password=\u0026lt;your ddns password goes here\u0026gt; \u0026lt;your sub domain goes here\u0026gt; Something worth noting, Namecheap also have not made the effort to put this option to use SSL in their example config:\nhttps://www.namecheap.com/support/knowledgebase/article.aspx/583/11/how-do-i-configure-ddclient\n","permalink":"https://exitcode0.net/posts/secure-ddns-namecheap-ssl-with-ddclient/","summary":"https://ddclient.net A quote short post on how to secure your DDNS updates with Namecheap, SSL and DDClient. For those of us who use dynamic DNS to work around roaming IP addresses, it is important to make sure that you are updating your DNS records securely with SSL.\nThe default DDclient config has an example configuration file for using Namecheap’s DDNS service, however, it does not use SSL to check for your IP.","title":"Secure DDNS - Namecheap SSL With DDClient"},{"content":"Yet another simple problem/resolution. If you are looking to make your VirtualBox VM’s resolution match that of your host making full-screen mode, truly full screen, look no further, here is the answer.\nOnce again we are looking at an issue caused by a default setting. You can set your VM to full-screen mode, but it not likely to rescale to the native resolution of your display.\nThe setting which you need to change requires you to have the VM powered off. Enter the settings for the VM \u0026gt; Display \u0026gt; Screen Tab \u0026gt; Graphics Controller: VBoxSVGA.\nOnce you have changed it to VBoxSVGA, you can boot your VM and it will automatically fill the available resolution within the bounds of the virtual box window. Subsequently, if you switch to full screen mode, the VM will change the resolution to match the host’s display.\nChange the graphics controller to VBoxSVGA\nOther VirtualBox tips and How-To posts Clipboard and Shared Folders on Kali Linux with VirtualBox – https://exitcode0.net/clipboard-and-shared-folders-on-kali-linux-with-virtualbox/\n","permalink":"https://exitcode0.net/posts/how-to-make-a-virtualbox-vm-the-same-resolution-as-host/","summary":"Yet another simple problem/resolution. If you are looking to make your VirtualBox VM’s resolution match that of your host making full-screen mode, truly full screen, look no further, here is the answer.\nOnce again we are looking at an issue caused by a default setting. You can set your VM to full-screen mode, but it not likely to rescale to the native resolution of your display.\nThe setting which you need to change requires you to have the VM powered off.","title":"How to make a VirtualBox VM the same resolution as host"},{"content":" A short and sweet problem/resolution. If you are looking to enable subnet overlapping on a Fortigate so that you can give multiple interfaces an IP in the same subnet, this is the post for you.\nNOTE: This feature can only be enabled in the Fortigate’s CLI. To enable the overlapping feature, enter the following commands:\nconfig system settings set allow-subnet-overlap [enable/disable] end What is subnet overlapping? Subnet overlapping is disabled by default in fortiOS and for good reason; if you misuse subnet overlapping it can cause massive routing issues for your clients and their traffic. Subnet overlapping lets you apply IPs from the same subnet (e.g 192.160.1.X/24) to multiple interfaces that are not in the same virtual/physical switch.\nWhen trying to set an overlapping IP to an interface without enabling overlapping, the FortiGate will give the following error messages, CLI or GUI respectively:\n\u0026#39;Subnets overlap between \u0026#39;port2\u0026#39; and the primary IP of \u0026#39;port2\u0026#39; object set operator error, -54 discard the setting\u0026#39; or\n\u0026#39;IP address is in same subnet as the others.\u0026#39; When might I use subnet overlapping? One of my most common reasons for using subnet overlapping on a Fortigate is to give a HA interface a management IP on the same subnet as the shared (floating) management interface for your Fortigate. This makes it easy to access the web interface or SSH to the CLI of a HA slave if you need to do some troubleshooting.\nSupported FortiOS versions All of the following branches support subnet overlapping, but all have it disabled by default:\nFortiGate v4.0 MR2\nFortiGate v4.0 MR3\nFortiGate v5.0\nFortiGate v5.2\nFortiGate v5.4\nFortiGate v5.6\nFortiGate v6.0\nFortiGate v6.2\nOnline Resources Fortinet KB for Subnet Overlapping – https://kb.fortinet.com/kb/documentLink.do?externalID=FD30014\nOther Fortigate Tips and How-To posts Double NAT port forwarding with a Fortigate – https://exitcode0.net/posts/double-nat-port-forwarding-with-a-fortigate/\n","permalink":"https://exitcode0.net/posts/how-to-enable-subnet-overlapping-on-a-fortigate/","summary":"A short and sweet problem/resolution. If you are looking to enable subnet overlapping on a Fortigate so that you can give multiple interfaces an IP in the same subnet, this is the post for you.\nNOTE: This feature can only be enabled in the Fortigate’s CLI. To enable the overlapping feature, enter the following commands:\nconfig system settings set allow-subnet-overlap [enable/disable] end What is subnet overlapping? Subnet overlapping is disabled by default in fortiOS and for good reason; if you misuse subnet overlapping it can cause massive routing issues for your clients and their traffic.","title":"How to enable subnet overlapping on a Fortigate"},{"content":"1st January 2020 marked the official end of python 2.7 development, including feature support and security fixes.\nPython 2.7 was over 9 years old in development years, making it the longest supported version to date. The code freeze is no in place, with the final release – 2.7.18 – scheduled for an April 2020 release. So yes there will be one more version to come down the tubes but it’s probably best that the new python project you were thinking of starting is written in 3.7 or above.\nMigrating away from Python 2.7 One useful resource for tracking the impending demise of a Python version: https://python-release-cycle.glitch.me/\nYou might be inclined to believe that any version of 3.X is good enough and better than running or developing in 2.7, however, there are a number of versions of 3 which have already reached the end of support – 3.2, 3.3, 3.4 – and others with only a few months on the clock – 3.5 and 3.6.\nFor those working on a Python project written in 2.7, here is the official porting guide: https://docs.python.org/3/howto/pyporting.html\nFor those who just wish to update the operating system’s primary python version used for running scripts and commands, I have composed a number of simple guides to make 3.7 your default version of python, including subsequent fixes for pip:\nChanging the default python version in Debian – https://exitcode0.net/posts/changing-the-default-python-version-in-debian/ Ubuntu 19.10 – How to upgrade python 2.7 to python 3.7 – https://exitcode0.net/ubuntu-19-10-how-to-upgrade-python-2-7-to-python-3-7/ Kali Linux – How to upgrade python 2.7 to python 3.7 – https://exitcode0.net/posts/kali-linux-how-to-upgrade-python-2-7-to-python-3-7/ and if you are looking to move from 3.5 to 3.7:\nDebian 9 – How to upgrade python 3.5 to python 3.7 – https://exitcode0.net/posts/debian-9-how-to-upgrade-python-3-5-to-python-3-7/ The final goodbye for Python 2.7? In conclusion, whilst the new year marks the official sunset for Python 2.7, the road to migration to newer versions has been famously messy and in some sense a failure. Don’t be surprised to encounter 2.7 projects for years to come; consider 2.7 an annoying requirement for so time to come.\n","permalink":"https://exitcode0.net/posts/python-2-7-end-of-life-the-time-to-upgrade-is-upon-us/","summary":"1st January 2020 marked the official end of python 2.7 development, including feature support and security fixes.\nPython 2.7 was over 9 years old in development years, making it the longest supported version to date. The code freeze is no in place, with the final release – 2.7.18 – scheduled for an April 2020 release. So yes there will be one more version to come down the tubes but it’s probably best that the new python project you were thinking of starting is written in 3.","title":"Python 2.7 end-of-life - The time to upgrade is upon us"},{"content":"They say that the long tail page titles are best for SEO, well this one certainly should be up there with the best of them. In this post, I am going to explain how easy it is to control Kasa a HS100 / HS110 with IFTTT and python webhooks. Using this basic code, you expand into building a very powerful home automation system which fits your needs and take your Kasa home automation to the next level.\nKasa HS100 / HS110 Introduction HS100 (UK Version) Kasa is the home automation/ smart device brand offered by TP-Link. Their products office a great deal of functionality out of the box, allowing you to control a device smart plug/bulb from your phone without the need for a controller – such as the Philips hue systems. This is achieved using a cloud-hosted API model; every device communicates to/from the Kasa cloud-hosted backend. Every Kasa device is registered to your account.\nIFTTT Webhooks Introduction If This Then That (IFTTT) – is a free web-based service to create chains of simple conditional statements, called applets. An applet is triggered by changes that occur within other web services such as Gmail, Facebook, Telegram, Instagram, or Pinterest.\nYou must first activate the webhook service, then you can acquire your webhook URL.\nFind you webhook URL https://ifttt.com/maker_webhooks/settings\nBuild a webhook trigger Next, you need to build a trigger. In the case of a HS100/HS110, it would be sensible to build two triggers – on and off triggers. As you will see later in the code, the Event Name value is passed to IFTTT in our webhook URL; make this unique to each webhook trigger.\nKasa home automation with python Finally, it is time to write to some code – at last. This part is arguably less fiddly than the prerequisite setup for this project. As previously mentioned, this code contains my unique IFTTT trigger URL so I will be obfuscating some sensitive data to prevent everyone from turning my Kasa plug on and off.\n#!/usr/bin/python import requests def funcHSTon (): requests.post(\u0026#34;https://maker.ifttt.com/trigger/temp_low/with/key/\u0026lt;your webhook key\u0026gt;\u0026#34;) def funcHSToff (): requests.post(\u0026#34;https://maker.ifttt.com/trigger/temp_high/with/key/\u0026lt;your webhook key\u0026gt;\u0026#34;) #funcHSToff() funcHSTon() I have split both webhook triggers into two functions – funcHSTon \u0026amp; funcHSToff – and I have commented out the function that I don’t want to run during testing.\nRemember to replace ‘’ with your own key!\nBest of luck with the never ending story of home automation with python!\nOther useful posts: Ubuntu 19.10 – How to upgrade python 2.7 to python 3.7 – https://exitcode0.net/ubuntu-19-10-how-to-upgrade-python-2-7-to-python-3-7/ Debian 9 – How to upgrade python 3.5 to python 3.7 – https://exitcode0.net/posts/debian-9-how-to-upgrade-python-3-5-to-python-3-7/ ","permalink":"https://exitcode0.net/posts/kasa-home-automation-with-ifttt-and-python-webhooks/","summary":"They say that the long tail page titles are best for SEO, well this one certainly should be up there with the best of them. In this post, I am going to explain how easy it is to control Kasa a HS100 / HS110 with IFTTT and python webhooks. Using this basic code, you expand into building a very powerful home automation system which fits your needs and take your Kasa home automation to the next level.","title":"Kasa home automation with IFTTT and python webhooks"},{"content":"I am planning a number of articles which focus on using aircrack-ng and hashcast to recover WPA wireless security passwords. However to get into that you need to have a specific wireless device which supports monitor mode and packet injection. I decided on the Alfa AWUS036AC, but some work was required to get the drivers installed.\nThis guide is based on Kali Linux 2019.4 – but the drivers are certified for earlier versions of Kali and the kernel that 2019.4 uses. See updates below for getting this working on Kali 2020.4.\nHardware – AWUS036AC My choice of hardware for WPA password recovery was the Alfa Networks AWUS036AC – https://amzn.to/34LlqXY\nFull manufacturer specifications – https://www.alfa.com.tw/products_detail/3.htm\nIncreased Wireless Signal Penetration With unmatched Wi-Fi signal strength and coverage. AWUS036AC not only has maximum WiFi range, it helps to penetrate walls, and eliminate Wi-Fi dead spots in your living space easily.\nIf you are conductin a wireless assessment, this is great news because you can conduct your testing from the comfort of your desk chair.\nChipset Realtek RTL8812AU WiFi Standards IEEE 802.11ac/a/b/g/n WiFi Frequency Dual Band 2.4GHz or 5GHz Antenna Connector RP-SMA female x 2 Antenna Type 2.4G/5GHz Dual-Band 5dBi dipole antenna Wireless Performance 802.11a: up to 54Mbps 802.11b: up to 11Mbps 802.11g: up to 54Mbps 802.11n: up to 300Mbps 802.11ac: up to 867Mbps Wireless Security 64/128 bit WEP,WPA/WPA2,WPA-PSK/WPA2-PSK,WPS Interface USB 3.0 OS Requirement Windows XP, Vista, 7, 8/8.1 and Windows 10 32/64bit, macOS 10.5 to 10.14 or later Linux AWUS036AC (rtl8812au) Driver Installation Assuming that you are loged into a terminal session on your kali linux machine as root, the following commands are required to download and install the drivers from source:\nClone the aircrack-ng git repository:\ngit clone https://github.com/aircrack-ng/rtl8812au Enter the newly downloaded directory:\ncd rtl8812au Build and install the source files:\nmake make install Reboot the Kali instance to complete:\nreboot now There are a number of other git repositories where you can obtain drivers for this usb wireless adaptor. My findings were that the aircrack-ng repo was the only place which supplied drivers supported in the current kernel version for Kali 2019.4 – kernel 5.3.9\nVerify your device To ensure that your device is available and ready to be used in Kali, you can run the following command to confirm that the OS can recognise the adapter:\niwconfig Update: 11/01/2021 As you can see from the Github issue reports, https://github.com/aircrack-ng/rtl8812au/issues, there are a lot of active issues with these drivers.\nAs of 11/01/2021, I was able to get this chipset working on Kali with the following driver install method:\napt-get update apt install realtek-rtl88xxau-dkms And just to verify, here is my curent Kali version:\nroot@kali:~# lsb_release -a No LSB modules are available. Distributor ID: Kali Description: Kali GNU/Linux Rolling Release: 2020.4 Codename: kali-rolling …and here is proof of function with airodump-ng:\nSome yummy WEP networks in there for my troubles! It is worth noting that iwconfig shows the interface wlan0 in monitor mode; a mon0 interface is not created like most online tutorials demonstrate: https://www.computerweekly.com/tip/Step-by-step-aircrack-tutorial-for-Wi-Fi-penetration-testing.\nOther useful articles: https://exitcode0.net/posts/kali-linux-how-to-upgrade-python-2-7-to-python-3-7/ https://exitcode0.net/clipboard-and-shared-folders-on-kali-linux-with-virtualbox/ ","permalink":"https://exitcode0.net/posts/awus036ac-rt8812au-driver-setup-in-kali-linux/","summary":"I am planning a number of articles which focus on using aircrack-ng and hashcast to recover WPA wireless security passwords. However to get into that you need to have a specific wireless device which supports monitor mode and packet injection. I decided on the Alfa AWUS036AC, but some work was required to get the drivers installed.\nThis guide is based on Kali Linux 2019.4 – but the drivers are certified for earlier versions of Kali and the kernel that 2019.","title":"AWUS036AC (rtl8812au) driver setup in Kali Linux"},{"content":"I have covered changing the default version of python in Debian, however for those looking to Google for a quick fix on Kali, I hope that this reaches you well.\nThis was tested on a completely fresh install of Kali Linux with no other alterations made prior.\nThe basic premise is to configure Kali to use python 3.7 at a higher priority to python 2.7 or any other version installed on the system.\nCheck your python version Step 1 is to check your current python version:\npython -V or\npython --version Kali default output:\nPython 2.7.17\nSet your Python Default Now it is time configure the priority for the versions of python that we have installed, 2.7 and 3.5/7. You can list all of the available alternatives installed by running:\nls /usr/bin/python* To set your version priorities, with 3.7 being the high priority:\nupdate-alternatives --install /usr/bin/python python /usr/bin/python2.7 1 update-alternatives --install /usr/bin/python python /usr/bin/python3.7 2 We have just set 3.7 (2) to have a priority great than 2.7 (1). Now when we list the python priorities we see see 3.7 is higher that 2.7:\nupdate-alternatives --config python This is also a great way to easily switch those priorities around once they have been set.\nCheck you default version, again… python -V Now this command should return the default which you configured above.\nOther Useful Python tips: https://exitcode0.net/posts/debian-9-how-to-upgrade-python-3-5-to-python-3-7/ – Debian 9 – How to upgrade python 3.5 to python 3.7 https://exitcode0.net/posts/changing-the-default-python-version-in-debian/ – Changing the default python version in Debian If you have found this guide useful or it has solved a burning issue for you, please consider throw a coin in the tip jar to help this site stay active:\nhttps://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick\u0026amp;hosted_button_id=BTQD4GN8TTWJN\u0026amp;source=url\n","permalink":"https://exitcode0.net/posts/kali-linux-how-to-upgrade-python-2-7-to-python-3-7/","summary":"I have covered changing the default version of python in Debian, however for those looking to Google for a quick fix on Kali, I hope that this reaches you well.\nThis was tested on a completely fresh install of Kali Linux with no other alterations made prior.\nThe basic premise is to configure Kali to use python 3.7 at a higher priority to python 2.7 or any other version installed on the system.","title":"Kali Linux - How to upgrade python 2.7 to python 3.7"},{"content":"Much to my surprise, I found that a fresh install of Ubuntu 19.10 was set to use python 2.7 as the default python instance.\nI have covered changing the default version of python in Debian, however for those looking to Google for a quick fix on Ubuntu 19.10, I hope that this reaches you well.\nThis was tested on a completely fresh install of Ubuntu 19.10 with no other alterations made prior.\nThe basic premise is to configure Ubuntu to use python 3.7 at a higher priority to python 2.7 or any other version installed on the system.\nCheck your python version Step 1 is to check your current python version:\npython -V or\npython --version Ubuntu 19.10 default output:\nPython 2.7.17rc1\nSet your Python Default Now it is time configure the priority for the versions of python that we have installed, 2.7 and 3.5/7. You can list all of the available alternatives installed by running:\nls /usr/bin/python* Ubuntu 19.10 default output:\nv2.7 and 3.7 are available To set your version priorities, with 3.7 being the high priority:\nupdate-alternatives --install /usr/bin/python python /usr/bin/python2.7 1 update-alternatives --install /usr/bin/python python /usr/bin/python3.7 2 We have just set 3.7 (2) to have a priority great than 2.7 (1). Now when we list the python priorities we see see 3.7 is higher that 2.7:\nupdate-alternatives --config python This is also a great way to easily switch those priorities around once they have been set.\nCheck you default version, again… python -V Now this command should return the default which you configured above.\nOther Useful Python tips: https://exitcode0.net/posts/debian-9-how-to-upgrade-python-3-5-to-python-3-7/ – Debian 9 – How to upgrade python 3.5 to python 3.7 https://exitcode0.net/posts/changing-the-default-python-version-in-debian/ – Changing the default python version in Debian If you have found this guide useful or it has solved a burning issue for you, please consider throw a coin in the tip jar to help this site stay active:\nhttps://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick\u0026amp;hosted_button_id=BTQD4GN8TTWJN\u0026amp;source=url\n","permalink":"https://exitcode0.net/posts/ubuntu-19-10-how-to-upgrade-python-2-7-to-python-3-7/","summary":"Much to my surprise, I found that a fresh install of Ubuntu 19.10 was set to use python 2.7 as the default python instance.\nI have covered changing the default version of python in Debian, however for those looking to Google for a quick fix on Ubuntu 19.10, I hope that this reaches you well.\nThis was tested on a completely fresh install of Ubuntu 19.10 with no other alterations made prior.","title":"Ubuntu 19.10 - How to upgrade python 2.7 to python 3.7"},{"content":" Python.Org I have been getting started with python 3 – I want to make this my primary scripting language. One way I like to assist myself whilst I learn the rope is to maintain a crib sheet filled with all the trivial things I would otherwise forget.\nPython 3 Cheat Sheet #Define Variables programming_languages: \u0026#34;Python\u0026#34;, \u0026#34;VB\u0026#34;, \u0026#34;C++\u0026#34;, \u0026#34;C#\u0026#34; #Print Variables print(programming_languages) print(\u0026#39;--------------------\u0026#39;) #Basic for loop + variables for language in programming_languages: print(language) print(\u0026#39;--------------------\u0026#39;) #Basic Function def FuncExample(): i: 1 for language in programming_languages: #Concatinate Strings and Integers in print statements print(\u0026#34;Language \u0026#34; + str(i) + \u0026#34;:\u0026#34; + language) #Increment Integer i += 1 FuncExample() print(\u0026#39;--------------------\u0026#39;) #Functions with Variables #1 - Strings def FuncVarExample1(fname, lname): #Print with CRLF print(\u0026#34;First Name: \u0026#34; + fname + \u0026#34;\\r\\n\u0026#34; + \u0026#34;Last Name: \u0026#34; + lname) FuncVarExample1(\u0026#34;Joe\u0026#34;,\u0026#34;Bloggs\u0026#34;) print(\u0026#39;--------------------\u0026#39;) #Functions with Variables #2 - Integers + Returning Values def FuncVarExample2(x, y): #Basic integer maths return x+y #Concatenating Strings and Integers print(\u0026#34;33 + 42: \u0026#34; + str(FuncVarExample2(33,42))) print(\u0026#39;--------------------\u0026#39;) You can also find more code snippets here: https://exitcode0.net/code-samples/\n","permalink":"https://exitcode0.net/posts/getting-started-with-python-3-a-beginners-cheat-sheet/","summary":"Python.Org I have been getting started with python 3 – I want to make this my primary scripting language. One way I like to assist myself whilst I learn the rope is to maintain a crib sheet filled with all the trivial things I would otherwise forget.\nPython 3 Cheat Sheet #Define Variables programming_languages: \u0026#34;Python\u0026#34;, \u0026#34;VB\u0026#34;, \u0026#34;C++\u0026#34;, \u0026#34;C#\u0026#34; #Print Variables print(programming_languages) print(\u0026#39;--------------------\u0026#39;) #Basic for loop + variables for language in programming_languages: print(language) print(\u0026#39;--------------------\u0026#39;) #Basic Function def FuncExample(): i: 1 for language in programming_languages: #Concatinate Strings and Integers in print statements print(\u0026#34;Language \u0026#34; + str(i) + \u0026#34;:\u0026#34; + language) #Increment Integer i += 1 FuncExample() print(\u0026#39;--------------------\u0026#39;) #Functions with Variables #1 - Strings def FuncVarExample1(fname, lname): #Print with CRLF print(\u0026#34;First Name: \u0026#34; + fname + \u0026#34;\\r\\n\u0026#34; + \u0026#34;Last Name: \u0026#34; + lname) FuncVarExample1(\u0026#34;Joe\u0026#34;,\u0026#34;Bloggs\u0026#34;) print(\u0026#39;--------------------\u0026#39;) #Functions with Variables #2 - Integers + Returning Values def FuncVarExample2(x, y): #Basic integer maths return x+y #Concatenating Strings and Integers print(\u0026#34;33 + 42: \u0026#34; + str(FuncVarExample2(33,42))) print(\u0026#39;--------------------\u0026#39;) You can also find more code snippets here: https://exitcode0.","title":"Getting started with Python 3 - a beginner's cheat sheet"},{"content":"I spent more time that care to admit trying to setup a shared folder between my windows host and Kali VirtualBox VM. So hopefully the Google algos pick this one up and save you the time trying to find the right packages to fix this a clipboard sync.\napt-get update apt-get install -y virtualbox-guest-x11 Go for a quick reboot once the above commands are complete and you should have clipboard sync (text) and shared folders should mount successfully.\nMore Linux Tidbits: Changing the default python version in Debian https://exitcode0.net/posts/changing-the-default-python-version-in-debian/ Debian 9 – Running a python script at boot https://exitcode0.net/posts/debian-9-how-to-upgrade-python-3-5-to-python-3-7/ Changing the default python version in Debian https://exitcode0.net/posts/changing-the-default-python-version-in-debian/ ","permalink":"https://exitcode0.net/posts/clipboard-and-shared-folders-on-kali-linux-with-virtualbox/","summary":"I spent more time that care to admit trying to setup a shared folder between my windows host and Kali VirtualBox VM. So hopefully the Google algos pick this one up and save you the time trying to find the right packages to fix this a clipboard sync.\napt-get update apt-get install -y virtualbox-guest-x11 Go for a quick reboot once the above commands are complete and you should have clipboard sync (text) and shared folders should mount successfully.","title":"Clipboard and Shared Folders on Kali Linux with VirtualBox"},{"content":"The Raspberry Pi 4 has now been released offering up to 4GB of RAM! All of the horsepower required for an excellent lower power, docker host.\nHowever, there are currently issues undergoing work which prevent docker from running on the only Rasbian image currently available for the Pi 4 – ‘Rasbian Buster‘. Details of these issues can been found here on the GitHub thread – https://github.com/docker/for-linux/issues/709\nCurrent Working Solution Fear not, for there is a simple way to fool your docker installation and successfully getting it to run on the Pi 4.\nFirst, let make sure that your raspbian install is up to date:\nsudo apt-get update sudo apt-get upgrade Now let’s install docker:\ncurl -SL get.docker.com | sed \u0026#39;s/9)/10)/\u0026#39; | sh Once complete the installer will advise that you should add the Pi user to the docker group if you wish to run docker commands from that account:\nusermod -aG docker pi And you’re done! Docker will now be running on your Raspberry Pi 4.\nIf this posted helped you, consider throwing a penny in the tip jar to support this site.\n","permalink":"https://exitcode0.net/posts/solved-running-docker-on-a-raspberry-pi-4/","summary":"The Raspberry Pi 4 has now been released offering up to 4GB of RAM! All of the horsepower required for an excellent lower power, docker host.\nHowever, there are currently issues undergoing work which prevent docker from running on the only Rasbian image currently available for the Pi 4 – ‘Rasbian Buster‘. Details of these issues can been found here on the GitHub thread – https://github.com/docker/for-linux/issues/709\nCurrent Working Solution Fear not, for there is a simple way to fool your docker installation and successfully getting it to run on the Pi 4.","title":"[SOLVED] - Running Docker on a Raspberry Pi 4"},{"content":"In a previous post, I explained how to upgrade from Python3.5 to Python3.7, however is is still the case in most fresh Debian installs that the default python version is 2.7. This post is going to show you the simple steps you need to take when changing the default python version in Debian.\nThis guide is based on a Debian 9 installation, but work for multiple releases.\nCheck you default version First we need to check our current default version of python. This is the version of python that you Debian OS will try to execute python scripts with unless otherwise specified.\npython -V or\npython --version My fresh install of Debian 9 told me that my default was: 2.7.17. Old and unwanted.\nInstall Python 3 This is optional and only applies for those who don’t already have python 3 installed:\napt install python3 Set your Python Default Now it is time configure the priority for the versions of python that we have installed, 2.7 and 3.5/7. You can list all of the available alternatives installed by running:\nls /usr/bin/python* To set your version priorities, with 3.5 being the high priority:\nupdate-alternatives --install /usr/bin/python python /usr/bin/python2.7 1 update-alternatives --install /usr/bin/python python /usr/bin/python3.5 2 We have just set 3.5 to have a priority great than 2.7. Now when we list the python priorities we see see 3.5 is higher that 2.7:\nupdate-alternatives --config python This is also a great way to easily switch those priorities around once they have been set.\nCheck you default version, again… python -V Now this command should return the default which you configured above.\nIf you have found this guide useful or it has solved a burning issue for you, please consider throw a coin in the tip jar to help this site stay active:\nhttps://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick\u0026amp;hosted_button_id=BTQD4GN8TTWJN\u0026amp;source=url\n","permalink":"https://exitcode0.net/posts/changing-the-default-python-version-in-debian/","summary":"In a previous post, I explained how to upgrade from Python3.5 to Python3.7, however is is still the case in most fresh Debian installs that the default python version is 2.7. This post is going to show you the simple steps you need to take when changing the default python version in Debian.\nThis guide is based on a Debian 9 installation, but work for multiple releases.\nCheck you default version First we need to check our current default version of python.","title":"Changing the default python version in Debian"},{"content":"This post is going to share how to use DHT11 sensors with a the raspberry pi , using the GPIO pins. Furthermore the particular sensor that I am going to be discussing requires zero soldering and is purely plug and play.\nThe Sensor:\nThe sensor that we will be using is the DHT11 – the exact variant is the 3 pin version. You can find the exact version that I used here\nhttps://amzn.to/2XkclSq\nThis pack of 5 sensors also comes with the cables you will need to hook up the DHT11 to the GPIO pinout of the raspberry pi.\nThe PIN Layout\nThe sensor pin are a positive power pin, a neutral pin and a data pin. Depending on the version of your raspberry pi, the GPIO layout can very. The following diagram shows the GPIO layout for the raspberry pi 3 – the board I used:\nThe pins to connect the DHT11 to in this example would be 2, 6, and 7. That being said, it would be possible to use any of the other available data pins (green), if this was the only device attached via the GPIO pins.\n","permalink":"https://exitcode0.net/posts/using-a-dht11-sensor-with-a-raspberry-pi/","summary":"This post is going to share how to use DHT11 sensors with a the raspberry pi , using the GPIO pins. Furthermore the particular sensor that I am going to be discussing requires zero soldering and is purely plug and play.\nThe Sensor:\nThe sensor that we will be using is the DHT11 – the exact variant is the 3 pin version. You can find the exact version that I used here","title":"Using a DHT11 sensor with a raspberry pi"},{"content":"If you are unfortunate enough to have to deal with double NAT on your gateway then you might know the troubles surrounding portforwarding or VIPs. Here is a quick how to guide for setting up a port forward on a Forgate where double NAT is inplace.\nCase Study – Plex port forward\nPlex is a great tool for managing your personal media collection and it gets even better when you enable a port forward to let you access this collection from anywhere in the world. Whilst Plex ahve made a number of changes to allow you to reach your contect via a relay server, the best way to access your content from outside your LAN is by using a port forward.\nDouble NAT means that there is a device runing NAT service in front of your NAT enabled default gateway – this can make portforwards difficult.\nI started by setting the ‘WAN IP’ of my Fortigate to a DMZ IP on the border NAT device – this will prevent any port foltering or firewall restrictions on traffic destined for the Fortigate.\nNext when creating your VIP, use the following config:\nNOTE: I am currently using WAN2 as the primary WAN conenction on my Fortigate.\n","permalink":"https://exitcode0.net/posts/double-nat-port-forwarding-with-a-fortigate/","summary":"If you are unfortunate enough to have to deal with double NAT on your gateway then you might know the troubles surrounding portforwarding or VIPs. Here is a quick how to guide for setting up a port forward on a Forgate where double NAT is inplace.\nCase Study – Plex port forward\nPlex is a great tool for managing your personal media collection and it gets even better when you enable a port forward to let you access this collection from anywhere in the world.","title":"Double NAT port forwarding with a Fortigate"},{"content":"There are a number of ways that you can run a python or bash script at system start but I am about to show you possibly the easiest way, using crontab.\nFirst you need to make that your script is executable and can run unattended. Then you need to login as the user your wish to execute the script as and edit the crontab file:\ncrontab -e If this is your first time editing the crontab file, you might be asked which editor you wish to use – I chose nano.\nOnce in the crontab file add a new line at the bottom of the file; it should look something like this\n@reboot /the/path/to/your/script/./your_script.py [any other parameters] \u0026gt; /path/of/your/log.txt Alternatively, if you don’t wish to record any output from your script you can send the output to /dev/null. Now it’s time to reboot your Debian instance and your script will be live.\n","permalink":"https://exitcode0.net/posts/debian-9-running-a-python-script-at-startup/","summary":"There are a number of ways that you can run a python or bash script at system start but I am about to show you possibly the easiest way, using crontab.\nFirst you need to make that your script is executable and can run unattended. Then you need to login as the user your wish to execute the script as and edit the crontab file:\ncrontab -e If this is your first time editing the crontab file, you might be asked which editor you wish to use – I chose nano.","title":"Debian 9 - Running a python script at boot"},{"content":" I recently spent 30 minutes figuring out how to upgrade to python 3.7 and subsequently pip version on a fresh install of Debian 9. I ran into a number of issues doing this so I though that I would put this quick post together to make this a little bit less complicated for anyone else trying to this.\nThis post relates specifically to python version 3.7.3. Although the generic commands should also apply to earlier versions, but your milage may vary.\nThe basic premise is, upgrade your version of python 3 to your desired version – 3.7.3 in this instance – then configure Debian to use python 3.7 at a higher priority to python 3.5.\nCheck your version\nStep 1 is to check your current python version:\npython3 -V or\npython3 --version Download the latest or desired version of python 3\nNext we need to download the latest version or desired version of python 3 from the python website. In my case I selected 3.7.3. Once downloaded we need to extract the tar file.\nwget https://www.python.org/ftp/python/3.7.3/Python-3.7.3.tar.xz tar xf Python-3.7.3.tar.xz cd ./Python-3.7.3 Make and Install\nNow that we have the files downloaded and extracted, it is time to compile them.\n./configure make make install Switch to the new version\nAfter compile the new version of python from source, we can now configure Debian to make it our default version of python3.\nupdate-alternatives --install /usr/bin/python python /usr/local/bin/python3.7 10 The integer at the end of this command (10) sets the priority for the python version; the greater the integer, the higher the priority. At this point we can rerun the previously used version commands and we should see that we now have 3.7.3 active.\nFixing and Updating Pip\nIt was at this point that I attempted to install some required addons using pip and discovered that the upgrade to python 3.7.3 had broken a few things. These were the commands I used to resolved issues with lsb_release and pip:\nln -s /usr/share/pyshared/lsb_release.py /usr/local/lib/python3.7/site-packages/lsb_release.py pip3 install --upgrade pip If you have found this guide useful or it has solved a burning issue for you, please consider throw a coin in the tip jar to help this site stay active:\nhttps://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick\u0026amp;hosted_button_id=BTQD4GN8TTWJN\u0026amp;source=url\nSome useful guides I found along the way: https://jcutrer.com/linux/upgrade-python37-ubuntu1810 – How to upgrade to python 3.7 on ubuntu 18.10. https://tecadmin.net/install-python-3-7-on-ubuntu-linuxmint/ – How to Install Python 3.7 on Ubuntu, Debian and LinuxMint. https://linuxconfig.org/how-to-change-default-python-version-on-debian-9-stretch-linux – How to change default python version on Debian 9 Stretch Linux Other Useful Debian tips: https://exitcode0.net/posts/debian-9-running-a-python-script-at-startup/ – Debian 9 – Running a python script at boot. ","permalink":"https://exitcode0.net/posts/debian-9-how-to-upgrade-python-3-5-to-python-3-7/","summary":"I recently spent 30 minutes figuring out how to upgrade to python 3.7 and subsequently pip version on a fresh install of Debian 9. I ran into a number of issues doing this so I though that I would put this quick post together to make this a little bit less complicated for anyone else trying to this.\nThis post relates specifically to python version 3.7.3. Although the generic commands should also apply to earlier versions, but your milage may vary.","title":"Debian 9 - How to upgrade python 3.5 to python 3.7"},{"content":"This is a local DNS server for local DNS requests.\nThis post is going to explain the why and how I created a local DNS server in my home network environment. I used the PiHole project to make a network wide advert blocking a reality.\nhttps://pi-hole.net/\nI have previously created a DNS server using Bind, running on CentOS 7. Thankfully digital ocean came to my rescue with some of the config in this home lab project, so be sure to check out their guide if you are looking for slightly higher level DNS setup:\nhttps://www.digitalocean.com/community/tutorials/how-to-configure-bind-as-a-private-network-dns-server-on-centos-7\nPiHole Setup\nStep 1 – Server Installation\nEvery great Linux project starts with a tedious operating system install. I started this project by setting up a Debian 9 VM on one of my local hypervisors. However as the name suggest, PiHole is very comfortable running on a Raspberry Pi single single board computer.\nMy VM specs: 1 vCPU, 512MB RAM, 15GB Storage\nStep 1 – Server Installation\nEvery great Linux project starts with a tedious operating system install. I started this project by setting up a Debian 9 VM on one of my local hypervisors. However as the name suggest, PiHole is very comfortable running on a Raspberry Pi single single board computer.\nMy VM specs: 1 vCPU, 512MB RAM, 15GB Storage\nStep 2 – Update all the things\nBe sure to update and upgrade the package repositories in your OS once the install is complete\nsudo apt-get update\nsudo apt-get upgrade\nYou might also want to take this opportunity to install the nano text editor as well as a couple of other handy utilities\nStep 3 – Set a static IP\nNow that our CentOS box is configured how you like it, its is time to make sure that it has a static IP. If you are going to be distributing this DNS server to your network clients, then it going to be important that this server does not get different DHCP lease addresses\nsudo nano /etc/network/interfaces\nChange the file to reflect the following config, changing the network setting to match your environment:\nauto lo\niface lo inet loopback\niface eth0 inet static\naddress 192.168.0.100\nnetmask 255.255.255.0\nnetwork 192.168.0.0\nbroadcast 192.168.0.255\ngateway 192.168.0.1\nStep 4 – Install PiHole\nThis final step pretty much rounds up the PiHole install process. The following command runs a guided installer which lets you chose your settings but offer a very functional system when the defaults are selected\ncurl -sSL https://install.pi-hole.net |bash\n","permalink":"https://exitcode0.net/posts/creating-a-local-dns-server-with-pi-hole/","summary":"This is a local DNS server for local DNS requests.\nThis post is going to explain the why and how I created a local DNS server in my home network environment. I used the PiHole project to make a network wide advert blocking a reality.\nhttps://pi-hole.net/\nI have previously created a DNS server using Bind, running on CentOS 7. Thankfully digital ocean came to my rescue with some of the config in this home lab project, so be sure to check out their guide if you are looking for slightly higher level DNS setup:","title":"Creating a local DNS server with Pi Hole"},{"content":" I once faced a rather tedious task that involved recursively modifying a number of shortcut paths stored across a convoluted folder structure. There was approximately 100 shortcuts which needed part of their path modifying.\nThe answer: Create a****PowerShell script.\nThe following code utilises regex to check for the existence of a string and modify with the define replacement. It is easily possible to use Read-Host to make this a little bit more interactive, but the purposes of my use-case it was just as simple to modify these variables before running the script.\n$oldPrefix: “\\OLDPLACE”\n#$oldPrefix: “\\NEWPLACE”\n$newPrefix: “\\NEWPLACE”\n#$newPrefix: “\\OLDPLACE”\n$searchPath: $PSScriptRoot\n$totalchanges: 0\n$dryRun: $TRUE\n$shell: new-object -com wscript.shell\nWrite-Host “Welcome to the script”\n$dryRuninput: Read-Host “Type True for dry run, type False for real run: ”\nif ($dryRuninput -match “[true]{4}”) {\n$dryRun: $TRUE\n} elseif ($dryRuninput -match “[false]{5}”) {\n$dryRun: $FALSE\n} else {\nWrite-Host “No Valid input detected”\n}\nif ( $dryRun ) {\nwrite-host “Executing dry run” -foregroundcolor green -backgroundcolor black\n} else {\nwrite-host “Executing real run” -foregroundcolor red -backgroundcolor black\n}\ndir $searchPath -filter *.lnk -recurse | foreach {\n$totalchanges ++\n$lnk: $shell.createShortcut( $_.fullname )\n$oldPath= $lnk.targetPath\n$lnkRegex: [regex]::escape( $oldPrefix )\nif ( $oldPath -match $lnkRegex ) {\n$newPath: $oldPath -replace $lnkRegex, $newPrefix\nwrite-host “Found: ” + $_.fullname -foregroundcolor yellow -backgroundcolor black\nwrite-host ” Replace: ” + $oldPath\nwrite-host ” With: ” + $newPath\nif ( !$dryRun ) {\n$lnk.targetPath: $newPath\n$lnk.Save()\n}\n}\n}\nwrite-host “Total Number of links found: ” $totalchanges\n","permalink":"https://exitcode0.net/posts/modifying-windows-shortcuts-is-powershell/","summary":"I once faced a rather tedious task that involved recursively modifying a number of shortcut paths stored across a convoluted folder structure. There was approximately 100 shortcuts which needed part of their path modifying.\nThe answer: Create a****PowerShell script.\nThe following code utilises regex to check for the existence of a string and modify with the define replacement. It is easily possible to use Read-Host to make this a little bit more interactive, but the purposes of my use-case it was just as simple to modify these variables before running the script.","title":"Modifying Windows shortcuts is Powershell"},{"content":"For security purposes, it is possible to control the language mode in a given Powershell session. These language modes can constrict which modules can be loaded during the life of a powershell session.\nLearn mode about Powershell langeuage modes: About Language Modes – Microsoft\nDetect the Current Language Mode $sLangMode: $ExecutionContext.SessionState.LanguageMode\nIf ($sLangMode -ne “FullLanguage”){\nWrite-Host ” !! Unable to run scrit – Powershell Using Wrong Language Mode !! ”\n}\nElse{\n#RUN THE MAIN FUNCTION\n}\nTry putting this simple statement at the start of your powershell scripts to avoid any unhandled exceptions caused by constrictive language modes.\n","permalink":"https://exitcode0.net/posts/powershell-checking-the-language-mode/","summary":"For security purposes, it is possible to control the language mode in a given Powershell session. These language modes can constrict which modules can be loaded during the life of a powershell session.\nLearn mode about Powershell langeuage modes: About Language Modes – Microsoft\nDetect the Current Language Mode $sLangMode: $ExecutionContext.SessionState.LanguageMode\nIf ($sLangMode -ne “FullLanguage”){\nWrite-Host ” !! Unable to run scrit – Powershell Using Wrong Language Mode !! ”","title":"Powershell - Checking the Language Mode"},{"content":"This is really quick nugget of Powershell for anyone who is struggling to copy and paste into a particular window or dialog box.\nPerhaps it is a case of a website which prevents text form being sent to a field from the clipboard, or in my case, a windows UAC prompt. If you are following password best practices, your passwords should be long, complex and contain zero dictionary words. Furthermore, you should have a different password for every site and service. So, when a user interface doesn’t allow you to paste a password, life gets that little bit more difficult.\nPowershell to the rescue!\nfunction FuncMain {\nadd-type -AssemblyName System.Windows.Forms\nWrite-Host “”\nWrite-Host ” — Welcome to the UAC Clipboard Utility — ”\nWrite-Host “”\n$sUsername: Read-Host “Enter Username”\n$sPassword: Read-Host “Enter Password”\nWrite-Host “User name and password ready…”\nWrite-Host “”\nWrite-Host “!!! PLACE THE CURSOR IN THE UAC USERNAME FIELD NOW !!!”\nWrite-Host “”\nStart-Sleep -Seconds 1\n$i: 5\nWrite-Host “Begin Countdown:”\nwhile( $i -gt 0){\nwrite-host $i\n$i —\nStart-Sleep -Seconds 1\n}\nTry {\n#Sends the entered username followed by tab and then the password.\nSystem.Windows.Forms.SendKeys::SendWait($sUsername + “{TAB}”)\nStart-Sleep -Seconds 1\n}\ncatch {\nWrite-Host “There was an error sending username / password!”\n}\nWrite-Host “”\nWrite-Host “…Paste of Credentials Complete”\nRead-Host “Press Enter to Clear Console and close the script”\nClear-Host\n}\n$sLangMode: $ExecutionContext.SessionState.LanguageMode\nIf ($sLangMode -ne “FullLanguage”){\nWrite-Host “”\nWrite-Host ” !! Unable to run scrit – Powershell Using Wrong Language Mode !! ”\nWrite-Host “”\n}\nElse{\n#RUN THE MAIN FUNCTION\nFuncMain\n}\nModifications: If this doesn’t quite fit your use-case, you can easily modify this code by commenting out the read-host line which asks for a username, as well as commenting out the line where the username keys are sent – This converts the script into a password only tool.\nChanging the value of $i changes the countdown timer.\nThanks again to Powershell scripts for making our lives a little bit easier.\n","permalink":"https://exitcode0.net/posts/copy-paste-with-powershell-sendkeys/","summary":"This is really quick nugget of Powershell for anyone who is struggling to copy and paste into a particular window or dialog box.\nPerhaps it is a case of a website which prevents text form being sent to a field from the clipboard, or in my case, a windows UAC prompt. If you are following password best practices, your passwords should be long, complex and contain zero dictionary words. Furthermore, you should have a different password for every site and service.","title":"'Copy, Paste' With Powershell Sendkeys"},{"content":"We collect usage data using google analytics for the purposes of visualizing the traffic on this website.\nAll data is anonymized and we do not sell or share any data with third parties.\nIf you choose to deny, no info will be collected whatsoever.\n","permalink":"https://exitcode0.net/privacy/","summary":"We collect usage data using google analytics for the purposes of visualizing the traffic on this website.\nAll data is anonymized and we do not sell or share any data with third parties.\nIf you choose to deny, no info will be collected whatsoever.","title":"Privacy"}]